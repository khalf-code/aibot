node_modules/@mariozechner/pi-ai/dist/models.generated.d.ts.map:{"version":3,"file":"models.generated.d.ts","sourceRoot":"","sources":["../src/models.generated.ts"],"names":[],"mappings":"AAKA,eAAO,MAAM,MAAM;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;CAkkVT,CAAC","sourcesContent":["// This file is auto-generated by scripts/generate-models.ts\n// Do not edit manually - run 'npm run generate-models' to update\n\nimport type { Model } from \"./types.js\";\n\nexport const MODELS = {\n\t\"amazon-bedrock\": {\n\t\t\"anthropic.claude-3-5-haiku-20241022-v1:0\": {\n\t\t\tid: \"anthropic.claude-3-5-haiku-20241022-v1:0\",\n\t\t\tname: \"Claude Haiku 3.5\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.8,\n\t\t\t\toutput: 4,\n\t\t\t\tcacheRead: 0.08,\n\t\t\t\tcacheWrite: 1,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"anthropic.claude-3-5-sonnet-20240620-v1:0\": {\n\t\t\tid: \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n\t\t\tname: \"Claude Sonnet 3.5\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"anthropic.claude-3-5-sonnet-20241022-v2:0\": {\n\t\t\tid: \"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n\t\t\tname: \"Claude Sonnet 3.5 v2\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"anthropic.claude-3-haiku-20240307-v1:0\": {\n\t\t\tid: \"anthropic.claude-3-haiku-20240307-v1:0\",\n\t\t\tname: \"Claude Haiku 3\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 1.25,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"anthropic.claude-3-opus-20240229-v1:0\": {\n\t\t\tid: \"anthropic.claude-3-opus-20240229-v1:0\",\n\t\t\tname: \"Claude Opus 3\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 75,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"anthropic.claude-3-sonnet-20240229-v1:0\": {\n\t\t\tid: \"anthropic.claude-3-sonnet-20240229-v1:0\",\n\t\t\tname: \"Claude Sonnet 3\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"cohere.command-r-plus-v1:0\": {\n\t\t\tid: \"cohere.command-r-plus-v1:0\",\n\t\t\tname: \"Command R+\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"cohere.command-r-v1:0\": {\n\t\t\tid: \"cohere.command-r-v1:0\",\n\t\t\tname: \"Command R\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 1.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"deepseek.v3-v1:0\": {\n\t\t\tid: \"deepseek.v3-v1:0\",\n\t\t\tname: \"DeepSeek-V3.1\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.58,\n\t\t\t\toutput: 1.68,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 163840,\n\t\t\tmaxTokens: 81920,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"eu.anthropic.claude-haiku-4-5-20251001-v1:0\": {\n\t\t\tid: \"eu.anthropic.claude-haiku-4-5-20251001-v1:0\",\n\t\t\tname: \"Claude Haiku 4.5 (EU)\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1,\n\t\t\t\toutput: 5,\n\t\t\t\tcacheRead: 0.1,\n\t\t\t\tcacheWrite: 1.25,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"eu.anthropic.claude-opus-4-5-20251101-v1:0\": {\n\t\t\tid: \"eu.anthropic.claude-opus-4-5-20251101-v1:0\",\n\t\t\tname: \"Claude Opus 4.5 (EU)\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 5,\n\t\t\t\toutput: 25,\n\t\t\t\tcacheRead: 0.5,\n\t\t\t\tcacheWrite: 6.25,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"eu.anthropic.claude-sonnet-4-5-20250929-v1:0\": {\n\t\t\tid: \"eu.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n\t\t\tname: \"Claude Sonnet 4.5 (EU)\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"global.amazon.nova-2-lite-v1:0\": {\n\t\t\tid: \"global.amazon.nova-2-lite-v1:0\",\n\t\t\tname: \"Nova 2 Lite\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.33,\n\t\t\t\toutput: 2.75,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"global.anthropic.claude-haiku-4-5-20251001-v1:0\": {\n\t\t\tid: \"global.anthropic.claude-haiku-4-5-20251001-v1:0\",\n\t\t\tname: \"Claude Haiku 4.5\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1,\n\t\t\t\toutput: 5,\n\t\t\t\tcacheRead: 0.1,\n\t\t\t\tcacheWrite: 1.25,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"global.anthropic.claude-opus-4-5-20251101-v1:0\": {\n\t\t\tid: \"global.anthropic.claude-opus-4-5-20251101-v1:0\",\n\t\t\tname: \"Claude Opus 4.5\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 5,\n\t\t\t\toutput: 25,\n\t\t\t\tcacheRead: 0.5,\n\t\t\t\tcacheWrite: 6.25,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"global.anthropic.claude-sonnet-4-20250514-v1:0\": {\n\t\t\tid: \"global.anthropic.claude-sonnet-4-20250514-v1:0\",\n\t\t\tname: \"Claude Sonnet 4\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"global.anthropic.claude-sonnet-4-5-20250929-v1:0\": {\n\t\t\tid: \"global.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n\t\t\tname: \"Claude Sonnet 4.5\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"google.gemma-3-27b-it\": {\n\t\t\tid: \"google.gemma-3-27b-it\",\n\t\t\tname: \"Google Gemma 3 27B Instruct\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.12,\n\t\t\t\toutput: 0.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 202752,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"google.gemma-3-4b-it\": {\n\t\t\tid: \"google.gemma-3-4b-it\",\n\t\t\tname: \"Gemma 3 4B IT\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.04,\n\t\t\t\toutput: 0.08,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"meta.llama3-1-70b-instruct-v1:0\": {\n\t\t\tid: \"meta.llama3-1-70b-instruct-v1:0\",\n\t\t\tname: \"Llama 3.1 70B Instruct\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.72,\n\t\t\t\toutput: 0.72,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"meta.llama3-1-8b-instruct-v1:0\": {\n\t\t\tid: \"meta.llama3-1-8b-instruct-v1:0\",\n\t\t\tname: \"Llama 3.1 8B Instruct\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.22,\n\t\t\t\toutput: 0.22,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"minimax.minimax-m2\": {\n\t\t\tid: \"minimax.minimax-m2\",\n\t\t\tname: \"MiniMax M2\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 1.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 204608,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"mistral.ministral-3-14b-instruct\": {\n\t\t\tid: \"mistral.ministral-3-14b-instruct\",\n\t\t\tname: \"Ministral 14B 3.0\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.2,\n\t\t\t\toutput: 0.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"mistral.ministral-3-8b-instruct\": {\n\t\t\tid: \"mistral.ministral-3-8b-instruct\",\n\t\t\tname: \"Ministral 3 8B\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"mistral.mistral-large-2402-v1:0\": {\n\t\t\tid: \"mistral.mistral-large-2402-v1:0\",\n\t\t\tname: \"Mistral Large (24.02)\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 1.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"mistral.voxtral-mini-3b-2507\": {\n\t\t\tid: \"mistral.voxtral-mini-3b-2507\",\n\t\t\tname: \"Voxtral Mini 3B 2507\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.04,\n\t\t\t\toutput: 0.04,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"mistral.voxtral-small-24b-2507\": {\n\t\t\tid: \"mistral.voxtral-small-24b-2507\",\n\t\t\tname: \"Voxtral Small 24B 2507\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.35,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"moonshot.kimi-k2-thinking\": {\n\t\t\tid: \"moonshot.kimi-k2-thinking\",\n\t\t\tname: \"Kimi K2 Thinking\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.6,\n\t\t\t\toutput: 2.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 256000,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"nvidia.nemotron-nano-12b-v2\": {\n\t\t\tid: \"nvidia.nemotron-nano-12b-v2\",\n\t\t\tname: \"NVIDIA Nemotron Nano 12B v2 VL BF16\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.2,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"nvidia.nemotron-nano-9b-v2\": {\n\t\t\tid: \"nvidia.nemotron-nano-9b-v2\",\n\t\t\tname: \"NVIDIA Nemotron Nano 9B v2\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.06,\n\t\t\t\toutput: 0.23,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"openai.gpt-oss-120b-1:0\": {\n\t\t\tid: \"openai.gpt-oss-120b-1:0\",\n\t\t\tname: \"gpt-oss-120b\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"openai.gpt-oss-20b-1:0\": {\n\t\t\tid: \"openai.gpt-oss-20b-1:0\",\n\t\t\tname: \"gpt-oss-20b\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.07,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"openai.gpt-oss-safeguard-120b\": {\n\t\t\tid: \"openai.gpt-oss-safeguard-120b\",\n\t\t\tname: \"GPT OSS Safeguard 120B\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"openai.gpt-oss-safeguard-20b\": {\n\t\t\tid: \"openai.gpt-oss-safeguard-20b\",\n\t\t\tname: \"GPT OSS Safeguard 20B\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.07,\n\t\t\t\toutput: 0.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"qwen.qwen3-235b-a22b-2507-v1:0\": {\n\t\t\tid: \"qwen.qwen3-235b-a22b-2507-v1:0\",\n\t\t\tname: \"Qwen3 235B A22B 2507\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.22,\n\t\t\t\toutput: 0.88,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"qwen.qwen3-32b-v1:0\": {\n\t\t\tid: \"qwen.qwen3-32b-v1:0\",\n\t\t\tname: \"Qwen3 32B (dense)\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 16384,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"qwen.qwen3-coder-30b-a3b-v1:0\": {\n\t\t\tid: \"qwen.qwen3-coder-30b-a3b-v1:0\",\n\t\t\tname: \"Qwen3 Coder 30B A3B Instruct\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"qwen.qwen3-coder-480b-a35b-v1:0\": {\n\t\t\tid: \"qwen.qwen3-coder-480b-a35b-v1:0\",\n\t\t\tname: \"Qwen3 Coder 480B A35B Instruct\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.22,\n\t\t\t\toutput: 1.8,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"qwen.qwen3-next-80b-a3b\": {\n\t\t\tid: \"qwen.qwen3-next-80b-a3b\",\n\t\t\tname: \"Qwen/Qwen3-Next-80B-A3B-Instruct\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.14,\n\t\t\t\toutput: 1.4,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262000,\n\t\t\tmaxTokens: 262000,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"qwen.qwen3-vl-235b-a22b\": {\n\t\t\tid: \"qwen.qwen3-vl-235b-a22b\",\n\t\t\tname: \"Qwen/Qwen3-VL-235B-A22B-Instruct\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 1.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262000,\n\t\t\tmaxTokens: 262000,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"us.amazon.nova-lite-v1:0\": {\n\t\t\tid: \"us.amazon.nova-lite-v1:0\",\n\t\t\tname: \"Nova Lite\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.06,\n\t\t\t\toutput: 0.24,\n\t\t\t\tcacheRead: 0.015,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 300000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"us.amazon.nova-micro-v1:0\": {\n\t\t\tid: \"us.amazon.nova-micro-v1:0\",\n\t\t\tname: \"Nova Micro\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.035,\n\t\t\t\toutput: 0.14,\n\t\t\t\tcacheRead: 0.00875,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"us.amazon.nova-premier-v1:0\": {\n\t\t\tid: \"us.amazon.nova-premier-v1:0\",\n\t\t\tname: \"Nova Premier\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2.5,\n\t\t\t\toutput: 12.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"us.amazon.nova-pro-v1:0\": {\n\t\t\tid: \"us.amazon.nova-pro-v1:0\",\n\t\t\tname: \"Nova Pro\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.8,\n\t\t\t\toutput: 3.2,\n\t\t\t\tcacheRead: 0.2,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 300000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\": {\n\t\t\tid: \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n\t\t\tname: \"Claude Sonnet 3.7\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"us.anthropic.claude-opus-4-1-20250805-v1:0\": {\n\t\t\tid: \"us.anthropic.claude-opus-4-1-20250805-v1:0\",\n\t\t\tname: \"Claude Opus 4.1\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 75,\n\t\t\t\tcacheRead: 1.5,\n\t\t\t\tcacheWrite: 18.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 32000,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"us.anthropic.claude-opus-4-20250514-v1:0\": {\n\t\t\tid: \"us.anthropic.claude-opus-4-20250514-v1:0\",\n\t\t\tname: \"Claude Opus 4\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 75,\n\t\t\t\tcacheRead: 1.5,\n\t\t\t\tcacheWrite: 18.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 32000,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"us.deepseek.r1-v1:0\": {\n\t\t\tid: \"us.deepseek.r1-v1:0\",\n\t\t\tname: \"DeepSeek-R1\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.35,\n\t\t\t\toutput: 5.4,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"us.meta.llama3-2-11b-instruct-v1:0\": {\n\t\t\tid: \"us.meta.llama3-2-11b-instruct-v1:0\",\n\t\t\tname: \"Llama 3.2 11B Instruct\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.16,\n\t\t\t\toutput: 0.16,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"us.meta.llama3-2-1b-instruct-v1:0\": {\n\t\t\tid: \"us.meta.llama3-2-1b-instruct-v1:0\",\n\t\t\tname: \"Llama 3.2 1B Instruct\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.1,\n\t\t\t\toutput: 0.1,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"us.meta.llama3-2-3b-instruct-v1:0\": {\n\t\t\tid: \"us.meta.llama3-2-3b-instruct-v1:0\",\n\t\t\tname: \"Llama 3.2 3B Instruct\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"us.meta.llama3-2-90b-instruct-v1:0\": {\n\t\t\tid: \"us.meta.llama3-2-90b-instruct-v1:0\",\n\t\t\tname: \"Llama 3.2 90B Instruct\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.72,\n\t\t\t\toutput: 0.72,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"us.meta.llama3-3-70b-instruct-v1:0\": {\n\t\t\tid: \"us.meta.llama3-3-70b-instruct-v1:0\",\n\t\t\tname: \"Llama 3.3 70B Instruct\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.72,\n\t\t\t\toutput: 0.72,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"us.meta.llama4-maverick-17b-instruct-v1:0\": {\n\t\t\tid: \"us.meta.llama4-maverick-17b-instruct-v1:0\",\n\t\t\tname: \"Llama 4 Maverick 17B Instruct\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.24,\n\t\t\t\toutput: 0.97,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t\t\"us.meta.llama4-scout-17b-instruct-v1:0\": {\n\t\t\tid: \"us.meta.llama4-scout-17b-instruct-v1:0\",\n\t\t\tname: \"Llama 4 Scout 17B Instruct\",\n\t\t\tapi: \"bedrock-converse-stream\",\n\t\t\tprovider: \"amazon-bedrock\",\n\t\t\tbaseUrl: \"https://bedrock-runtime.us-east-1.amazonaws.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.17,\n\t\t\t\toutput: 0.66,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 3500000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"bedrock-converse-stream\">,\n\t},\n\t\"anthropic\": {\n\t\t\"claude-3-5-haiku-20241022\": {\n\t\t\tid: \"claude-3-5-haiku-20241022\",\n\t\t\tname: \"Claude Haiku 3.5\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.8,\n\t\t\t\toutput: 4,\n\t\t\t\tcacheRead: 0.08,\n\t\t\t\tcacheWrite: 1,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-3-5-haiku-latest\": {\n\t\t\tid: \"claude-3-5-haiku-latest\",\n\t\t\tname: \"Claude Haiku 3.5 (latest)\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.8,\n\t\t\t\toutput: 4,\n\t\t\t\tcacheRead: 0.08,\n\t\t\t\tcacheWrite: 1,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-3-5-sonnet-20240620\": {\n\t\t\tid: \"claude-3-5-sonnet-20240620\",\n\t\t\tname: \"Claude Sonnet 3.5\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-3-5-sonnet-20241022\": {\n\t\t\tid: \"claude-3-5-sonnet-20241022\",\n\t\t\tname: \"Claude Sonnet 3.5 v2\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-3-7-sonnet-20250219\": {\n\t\t\tid: \"claude-3-7-sonnet-20250219\",\n\t\t\tname: \"Claude Sonnet 3.7\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-3-7-sonnet-latest\": {\n\t\t\tid: \"claude-3-7-sonnet-latest\",\n\t\t\tname: \"Claude Sonnet 3.7 (latest)\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-3-haiku-20240307\": {\n\t\t\tid: \"claude-3-haiku-20240307\",\n\t\t\tname: \"Claude Haiku 3\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 1.25,\n\t\t\t\tcacheRead: 0.03,\n\t\t\t\tcacheWrite: 0.3,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-3-opus-20240229\": {\n\t\t\tid: \"claude-3-opus-20240229\",\n\t\t\tname: \"Claude Opus 3\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 75,\n\t\t\t\tcacheRead: 1.5,\n\t\t\t\tcacheWrite: 18.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-3-sonnet-20240229\": {\n\t\t\tid: \"claude-3-sonnet-20240229\",\n\t\t\tname: \"Claude Sonnet 3\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 0.3,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-haiku-4-5\": {\n\t\t\tid: \"claude-haiku-4-5\",\n\t\t\tname: \"Claude Haiku 4.5 (latest)\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1,\n\t\t\t\toutput: 5,\n\t\t\t\tcacheRead: 0.1,\n\t\t\t\tcacheWrite: 1.25,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-haiku-4-5-20251001\": {\n\t\t\tid: \"claude-haiku-4-5-20251001\",\n\t\t\tname: \"Claude Haiku 4.5\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1,\n\t\t\t\toutput: 5,\n\t\t\t\tcacheRead: 0.1,\n\t\t\t\tcacheWrite: 1.25,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-opus-4-0\": {\n\t\t\tid: \"claude-opus-4-0\",\n\t\t\tname: \"Claude Opus 4 (latest)\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 75,\n\t\t\t\tcacheRead: 1.5,\n\t\t\t\tcacheWrite: 18.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 32000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-opus-4-1\": {\n\t\t\tid: \"claude-opus-4-1\",\n\t\t\tname: \"Claude Opus 4.1 (latest)\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 75,\n\t\t\t\tcacheRead: 1.5,\n\t\t\t\tcacheWrite: 18.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 32000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-opus-4-1-20250805\": {\n\t\t\tid: \"claude-opus-4-1-20250805\",\n\t\t\tname: \"Claude Opus 4.1\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 75,\n\t\t\t\tcacheRead: 1.5,\n\t\t\t\tcacheWrite: 18.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 32000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-opus-4-20250514\": {\n\t\t\tid: \"claude-opus-4-20250514\",\n\t\t\tname: \"Claude Opus 4\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 75,\n\t\t\t\tcacheRead: 1.5,\n\t\t\t\tcacheWrite: 18.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 32000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-opus-4-5\": {\n\t\t\tid: \"claude-opus-4-5\",\n\t\t\tname: \"Claude Opus 4.5 (latest)\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 5,\n\t\t\t\toutput: 25,\n\t\t\t\tcacheRead: 0.5,\n\t\t\t\tcacheWrite: 6.25,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-opus-4-5-20251101\": {\n\t\t\tid: \"claude-opus-4-5-20251101\",\n\t\t\tname: \"Claude Opus 4.5\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 5,\n\t\t\t\toutput: 25,\n\t\t\t\tcacheRead: 0.5,\n\t\t\t\tcacheWrite: 6.25,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-sonnet-4-0\": {\n\t\t\tid: \"claude-sonnet-4-0\",\n\t\t\tname: \"Claude Sonnet 4 (latest)\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-sonnet-4-20250514\": {\n\t\t\tid: \"claude-sonnet-4-20250514\",\n\t\t\tname: \"Claude Sonnet 4\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-sonnet-4-5\": {\n\t\t\tid: \"claude-sonnet-4-5\",\n\t\t\tname: \"Claude Sonnet 4.5 (latest)\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-sonnet-4-5-20250929\": {\n\t\t\tid: \"claude-sonnet-4-5-20250929\",\n\t\t\tname: \"Claude Sonnet 4.5\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"anthropic\",\n\t\t\tbaseUrl: \"https://api.anthropic.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t},\n\t\"cerebras\": {\n\t\t\"gpt-oss-120b\": {\n\t\t\tid: \"gpt-oss-120b\",\n\t\t\tname: \"GPT OSS 120B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"cerebras\",\n\t\t\tbaseUrl: \"https://api.cerebras.ai/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 0.69,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen-3-235b-a22b-instruct-2507\": {\n\t\t\tid: \"qwen-3-235b-a22b-instruct-2507\",\n\t\t\tname: \"Qwen 3 235B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"cerebras\",\n\t\t\tbaseUrl: \"https://api.cerebras.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.6,\n\t\t\t\toutput: 1.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131000,\n\t\t\tmaxTokens: 32000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"zai-glm-4.7\": {\n\t\t\tid: \"zai-glm-4.7\",\n\t\t\tname: \"Z.AI GLM-4.7\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"cerebras\",\n\t\t\tbaseUrl: \"https://api.cerebras.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 40000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t},\n\t\"github-copilot\": {\n\t\t\"claude-haiku-4.5\": {\n\t\t\tid: \"claude-haiku-4.5\",\n\t\t\tname: \"Claude Haiku 4.5\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\tcompat: {\"supportsStore\":false,\"supportsDeveloperRole\":false,\"supportsReasoningEffort\":false},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"claude-opus-4.5\": {\n\t\t\tid: \"claude-opus-4.5\",\n\t\t\tname: \"Claude Opus 4.5\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\tcompat: {\"supportsStore\":false,\"supportsDeveloperRole\":false,\"supportsReasoningEffort\":false},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"claude-sonnet-4\": {\n\t\t\tid: \"claude-sonnet-4\",\n\t\t\tname: \"Claude Sonnet 4\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\tcompat: {\"supportsStore\":false,\"supportsDeveloperRole\":false,\"supportsReasoningEffort\":false},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"claude-sonnet-4.5\": {\n\t\t\tid: \"claude-sonnet-4.5\",\n\t\t\tname: \"Claude Sonnet 4.5\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\tcompat: {\"supportsStore\":false,\"supportsDeveloperRole\":false,\"supportsReasoningEffort\":false},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"gemini-2.5-pro\": {\n\t\t\tid: \"gemini-2.5-pro\",\n\t\t\tname: \"Gemini 2.5 Pro\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\tcompat: {\"supportsStore\":false,\"supportsDeveloperRole\":false,\"supportsReasoningEffort\":false},\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"gemini-3-flash-preview\": {\n\t\t\tid: \"gemini-3-flash-preview\",\n\t\t\tname: \"Gemini 3 Flash\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\tcompat: {\"supportsStore\":false,\"supportsDeveloperRole\":false,\"supportsReasoningEffort\":false},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"gemini-3-pro-preview\": {\n\t\t\tid: \"gemini-3-pro-preview\",\n\t\t\tname: \"Gemini 3 Pro Preview\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\tcompat: {\"supportsStore\":false,\"supportsDeveloperRole\":false,\"supportsReasoningEffort\":false},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"gpt-4.1\": {\n\t\t\tid: \"gpt-4.1\",\n\t\t\tname: \"GPT-4.1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\tcompat: {\"supportsStore\":false,\"supportsDeveloperRole\":false,\"supportsReasoningEffort\":false},\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"gpt-4o\": {\n\t\t\tid: \"gpt-4o\",\n\t\t\tname: \"GPT-4o\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\tcompat: {\"supportsStore\":false,\"supportsDeveloperRole\":false,\"supportsReasoningEffort\":false},\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 64000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"gpt-5\": {\n\t\t\tid: \"gpt-5\",\n\t\t\tname: \"GPT-5\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5-mini\": {\n\t\t\tid: \"gpt-5-mini\",\n\t\t\tname: \"GPT-5-mini\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.1\": {\n\t\t\tid: \"gpt-5.1\",\n\t\t\tname: \"GPT-5.1\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.1-codex\": {\n\t\t\tid: \"gpt-5.1-codex\",\n\t\t\tname: \"GPT-5.1-Codex\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.1-codex-max\": {\n\t\t\tid: \"gpt-5.1-codex-max\",\n\t\t\tname: \"GPT-5.1-Codex-max\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.1-codex-mini\": {\n\t\t\tid: \"gpt-5.1-codex-mini\",\n\t\t\tname: \"GPT-5.1-Codex-mini\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.2\": {\n\t\t\tid: \"gpt-5.2\",\n\t\t\tname: \"GPT-5.2\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.2-codex\": {\n\t\t\tid: \"gpt-5.2-codex\",\n\t\t\tname: \"GPT-5.2-Codex\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 272000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"grok-code-fast-1\": {\n\t\t\tid: \"grok-code-fast-1\",\n\t\t\tname: \"Grok Code Fast 1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"github-copilot\",\n\t\t\tbaseUrl: \"https://api.individual.githubcopilot.com\",\n\t\t\theaders: {\"User-Agent\":\"GitHubCopilotChat/0.35.0\",\"Editor-Version\":\"vscode/1.107.0\",\"Editor-Plugin-Version\":\"copilot-chat/0.35.0\",\"Copilot-Integration-Id\":\"vscode-chat\"},\n\t\t\tcompat: {\"supportsStore\":false,\"supportsDeveloperRole\":false,\"supportsReasoningEffort\":false},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t},\n\t\"google\": {\n\t\t\"gemini-1.5-flash\": {\n\t\t\tid: \"gemini-1.5-flash\",\n\t\t\tname: \"Gemini 1.5 Flash\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.075,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0.01875,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-1.5-flash-8b\": {\n\t\t\tid: \"gemini-1.5-flash-8b\",\n\t\t\tname: \"Gemini 1.5 Flash-8B\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.0375,\n\t\t\t\toutput: 0.15,\n\t\t\t\tcacheRead: 0.01,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-1.5-pro\": {\n\t\t\tid: \"gemini-1.5-pro\",\n\t\t\tname: \"Gemini 1.5 Pro\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 5,\n\t\t\t\tcacheRead: 0.3125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-2.0-flash\": {\n\t\t\tid: \"gemini-2.0-flash\",\n\t\t\tname: \"Gemini 2.0 Flash\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.1,\n\t\t\t\toutput: 0.4,\n\t\t\t\tcacheRead: 0.025,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-2.0-flash-lite\": {\n\t\t\tid: \"gemini-2.0-flash-lite\",\n\t\t\tname: \"Gemini 2.0 Flash Lite\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.075,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-2.5-flash\": {\n\t\t\tid: \"gemini-2.5-flash\",\n\t\t\tname: \"Gemini 2.5 Flash\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 2.5,\n\t\t\t\tcacheRead: 0.075,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-2.5-flash-lite\": {\n\t\t\tid: \"gemini-2.5-flash-lite\",\n\t\t\tname: \"Gemini 2.5 Flash Lite\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.1,\n\t\t\t\toutput: 0.4,\n\t\t\t\tcacheRead: 0.025,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-2.5-flash-lite-preview-06-17\": {\n\t\t\tid: \"gemini-2.5-flash-lite-preview-06-17\",\n\t\t\tname: \"Gemini 2.5 Flash Lite Preview 06-17\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.1,\n\t\t\t\toutput: 0.4,\n\t\t\t\tcacheRead: 0.025,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-2.5-flash-lite-preview-09-2025\": {\n\t\t\tid: \"gemini-2.5-flash-lite-preview-09-2025\",\n\t\t\tname: \"Gemini 2.5 Flash Lite Preview 09-25\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.1,\n\t\t\t\toutput: 0.4,\n\t\t\t\tcacheRead: 0.025,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-2.5-flash-preview-04-17\": {\n\t\t\tid: \"gemini-2.5-flash-preview-04-17\",\n\t\t\tname: \"Gemini 2.5 Flash Preview 04-17\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0.0375,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-2.5-flash-preview-05-20\": {\n\t\t\tid: \"gemini-2.5-flash-preview-05-20\",\n\t\t\tname: \"Gemini 2.5 Flash Preview 05-20\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0.0375,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-2.5-flash-preview-09-2025\": {\n\t\t\tid: \"gemini-2.5-flash-preview-09-2025\",\n\t\t\tname: \"Gemini 2.5 Flash Preview 09-25\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 2.5,\n\t\t\t\tcacheRead: 0.075,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-2.5-pro\": {\n\t\t\tid: \"gemini-2.5-pro\",\n\t\t\tname: \"Gemini 2.5 Pro\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.31,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-2.5-pro-preview-05-06\": {\n\t\t\tid: \"gemini-2.5-pro-preview-05-06\",\n\t\t\tname: \"Gemini 2.5 Pro Preview 05-06\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.31,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-2.5-pro-preview-06-05\": {\n\t\t\tid: \"gemini-2.5-pro-preview-06-05\",\n\t\t\tname: \"Gemini 2.5 Pro Preview 06-05\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.31,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-3-flash-preview\": {\n\t\t\tid: \"gemini-3-flash-preview\",\n\t\t\tname: \"Gemini 3 Flash Preview\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 3,\n\t\t\t\tcacheRead: 0.05,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-3-pro-preview\": {\n\t\t\tid: \"gemini-3-pro-preview\",\n\t\t\tname: \"Gemini 3 Pro Preview\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 12,\n\t\t\t\tcacheRead: 0.2,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-flash-latest\": {\n\t\t\tid: \"gemini-flash-latest\",\n\t\t\tname: \"Gemini Flash Latest\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 2.5,\n\t\t\t\tcacheRead: 0.075,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-flash-lite-latest\": {\n\t\t\tid: \"gemini-flash-lite-latest\",\n\t\t\tname: \"Gemini Flash-Lite Latest\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.1,\n\t\t\t\toutput: 0.4,\n\t\t\t\tcacheRead: 0.025,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-live-2.5-flash\": {\n\t\t\tid: \"gemini-live-2.5-flash\",\n\t\t\tname: \"Gemini Live 2.5 Flash\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 8000,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-live-2.5-flash-preview-native-audio\": {\n\t\t\tid: \"gemini-live-2.5-flash-preview-native-audio\",\n\t\t\tname: \"Gemini Live 2.5 Flash Preview Native Audio\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"google\",\n\t\t\tbaseUrl: \"https://generativelanguage.googleapis.com/v1beta\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t},\n\t\"google-antigravity\": {\n\t\t\"claude-opus-4-5-thinking\": {\n\t\t\tid: \"claude-opus-4-5-thinking\",\n\t\t\tname: \"Claude Opus 4.5 Thinking (Antigravity)\",\n\t\t\tapi: \"google-gemini-cli\",\n\t\t\tprovider: \"google-antigravity\",\n\t\t\tbaseUrl: \"https://daily-cloudcode-pa.sandbox.googleapis.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 5,\n\t\t\t\toutput: 25,\n\t\t\t\tcacheRead: 0.5,\n\t\t\t\tcacheWrite: 6.25,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"google-gemini-cli\">,\n\t\t\"claude-sonnet-4-5\": {\n\t\t\tid: \"claude-sonnet-4-5\",\n\t\t\tname: \"Claude Sonnet 4.5 (Antigravity)\",\n\t\t\tapi: \"google-gemini-cli\",\n\t\t\tprovider: \"google-antigravity\",\n\t\t\tbaseUrl: \"https://daily-cloudcode-pa.sandbox.googleapis.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"google-gemini-cli\">,\n\t\t\"claude-sonnet-4-5-thinking\": {\n\t\t\tid: \"claude-sonnet-4-5-thinking\",\n\t\t\tname: \"Claude Sonnet 4.5 Thinking (Antigravity)\",\n\t\t\tapi: \"google-gemini-cli\",\n\t\t\tprovider: \"google-antigravity\",\n\t\t\tbaseUrl: \"https://daily-cloudcode-pa.sandbox.googleapis.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"google-gemini-cli\">,\n\t\t\"gemini-3-flash\": {\n\t\t\tid: \"gemini-3-flash\",\n\t\t\tname: \"Gemini 3 Flash (Antigravity)\",\n\t\t\tapi: \"google-gemini-cli\",\n\t\t\tprovider: \"google-antigravity\",\n\t\t\tbaseUrl: \"https://daily-cloudcode-pa.sandbox.googleapis.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 3,\n\t\t\t\tcacheRead: 0.5,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65535,\n\t\t} satisfies Model<\"google-gemini-cli\">,\n\t\t\"gemini-3-pro-high\": {\n\t\t\tid: \"gemini-3-pro-high\",\n\t\t\tname: \"Gemini 3 Pro High (Antigravity)\",\n\t\t\tapi: \"google-gemini-cli\",\n\t\t\tprovider: \"google-antigravity\",\n\t\t\tbaseUrl: \"https://daily-cloudcode-pa.sandbox.googleapis.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 12,\n\t\t\t\tcacheRead: 0.2,\n\t\t\t\tcacheWrite: 2.375,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65535,\n\t\t} satisfies Model<\"google-gemini-cli\">,\n\t\t\"gemini-3-pro-low\": {\n\t\t\tid: \"gemini-3-pro-low\",\n\t\t\tname: \"Gemini 3 Pro Low (Antigravity)\",\n\t\t\tapi: \"google-gemini-cli\",\n\t\t\tprovider: \"google-antigravity\",\n\t\t\tbaseUrl: \"https://daily-cloudcode-pa.sandbox.googleapis.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 12,\n\t\t\t\tcacheRead: 0.2,\n\t\t\t\tcacheWrite: 2.375,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65535,\n\t\t} satisfies Model<\"google-gemini-cli\">,\n\t\t\"gpt-oss-120b-medium\": {\n\t\t\tid: \"gpt-oss-120b-medium\",\n\t\t\tname: \"GPT-OSS 120B Medium (Antigravity)\",\n\t\t\tapi: \"google-gemini-cli\",\n\t\t\tprovider: \"google-antigravity\",\n\t\t\tbaseUrl: \"https://daily-cloudcode-pa.sandbox.googleapis.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09,\n\t\t\t\toutput: 0.36,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"google-gemini-cli\">,\n\t},\n\t\"google-gemini-cli\": {\n\t\t\"gemini-2.0-flash\": {\n\t\t\tid: \"gemini-2.0-flash\",\n\t\t\tname: \"Gemini 2.0 Flash (Cloud Code Assist)\",\n\t\t\tapi: \"google-gemini-cli\",\n\t\t\tprovider: \"google-gemini-cli\",\n\t\t\tbaseUrl: \"https://cloudcode-pa.googleapis.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"google-gemini-cli\">,\n\t\t\"gemini-2.5-flash\": {\n\t\t\tid: \"gemini-2.5-flash\",\n\t\t\tname: \"Gemini 2.5 Flash (Cloud Code Assist)\",\n\t\t\tapi: \"google-gemini-cli\",\n\t\t\tprovider: \"google-gemini-cli\",\n\t\t\tbaseUrl: \"https://cloudcode-pa.googleapis.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65535,\n\t\t} satisfies Model<\"google-gemini-cli\">,\n\t\t\"gemini-2.5-pro\": {\n\t\t\tid: \"gemini-2.5-pro\",\n\t\t\tname: \"Gemini 2.5 Pro (Cloud Code Assist)\",\n\t\t\tapi: \"google-gemini-cli\",\n\t\t\tprovider: \"google-gemini-cli\",\n\t\t\tbaseUrl: \"https://cloudcode-pa.googleapis.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65535,\n\t\t} satisfies Model<\"google-gemini-cli\">,\n\t\t\"gemini-3-flash-preview\": {\n\t\t\tid: \"gemini-3-flash-preview\",\n\t\t\tname: \"Gemini 3 Flash Preview (Cloud Code Assist)\",\n\t\t\tapi: \"google-gemini-cli\",\n\t\t\tprovider: \"google-gemini-cli\",\n\t\t\tbaseUrl: \"https://cloudcode-pa.googleapis.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65535,\n\t\t} satisfies Model<\"google-gemini-cli\">,\n\t\t\"gemini-3-pro-preview\": {\n\t\t\tid: \"gemini-3-pro-preview\",\n\t\t\tname: \"Gemini 3 Pro Preview (Cloud Code Assist)\",\n\t\t\tapi: \"google-gemini-cli\",\n\t\t\tprovider: \"google-gemini-cli\",\n\t\t\tbaseUrl: \"https://cloudcode-pa.googleapis.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65535,\n\t\t} satisfies Model<\"google-gemini-cli\">,\n\t},\n\t\"google-vertex\": {\n\t\t\"gemini-1.5-flash\": {\n\t\t\tid: \"gemini-1.5-flash\",\n\t\t\tname: \"Gemini 1.5 Flash (Vertex)\",\n\t\t\tapi: \"google-vertex\",\n\t\t\tprovider: \"google-vertex\",\n\t\t\tbaseUrl: \"https://{location}-aiplatform.googleapis.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.075,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0.01875,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"google-vertex\">,\n\t\t\"gemini-1.5-flash-8b\": {\n\t\t\tid: \"gemini-1.5-flash-8b\",\n\t\t\tname: \"Gemini 1.5 Flash-8B (Vertex)\",\n\t\t\tapi: \"google-vertex\",\n\t\t\tprovider: \"google-vertex\",\n\t\t\tbaseUrl: \"https://{location}-aiplatform.googleapis.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.0375,\n\t\t\t\toutput: 0.15,\n\t\t\t\tcacheRead: 0.01,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"google-vertex\">,\n\t\t\"gemini-1.5-pro\": {\n\t\t\tid: \"gemini-1.5-pro\",\n\t\t\tname: \"Gemini 1.5 Pro (Vertex)\",\n\t\t\tapi: \"google-vertex\",\n\t\t\tprovider: \"google-vertex\",\n\t\t\tbaseUrl: \"https://{location}-aiplatform.googleapis.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 5,\n\t\t\t\tcacheRead: 0.3125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"google-vertex\">,\n\t\t\"gemini-2.0-flash\": {\n\t\t\tid: \"gemini-2.0-flash\",\n\t\t\tname: \"Gemini 2.0 Flash (Vertex)\",\n\t\t\tapi: \"google-vertex\",\n\t\t\tprovider: \"google-vertex\",\n\t\t\tbaseUrl: \"https://{location}-aiplatform.googleapis.com\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0.0375,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"google-vertex\">,\n\t\t\"gemini-2.0-flash-lite\": {\n\t\t\tid: \"gemini-2.0-flash-lite\",\n\t\t\tname: \"Gemini 2.0 Flash Lite (Vertex)\",\n\t\t\tapi: \"google-vertex\",\n\t\t\tprovider: \"google-vertex\",\n\t\t\tbaseUrl: \"https://{location}-aiplatform.googleapis.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.075,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0.01875,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-vertex\">,\n\t\t\"gemini-2.5-flash\": {\n\t\t\tid: \"gemini-2.5-flash\",\n\t\t\tname: \"Gemini 2.5 Flash (Vertex)\",\n\t\t\tapi: \"google-vertex\",\n\t\t\tprovider: \"google-vertex\",\n\t\t\tbaseUrl: \"https://{location}-aiplatform.googleapis.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 2.5,\n\t\t\t\tcacheRead: 0.03,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-vertex\">,\n\t\t\"gemini-2.5-flash-lite\": {\n\t\t\tid: \"gemini-2.5-flash-lite\",\n\t\t\tname: \"Gemini 2.5 Flash Lite (Vertex)\",\n\t\t\tapi: \"google-vertex\",\n\t\t\tprovider: \"google-vertex\",\n\t\t\tbaseUrl: \"https://{location}-aiplatform.googleapis.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.1,\n\t\t\t\toutput: 0.4,\n\t\t\t\tcacheRead: 0.01,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-vertex\">,\n\t\t\"gemini-2.5-flash-lite-preview-09-2025\": {\n\t\t\tid: \"gemini-2.5-flash-lite-preview-09-2025\",\n\t\t\tname: \"Gemini 2.5 Flash Lite Preview 09-25 (Vertex)\",\n\t\t\tapi: \"google-vertex\",\n\t\t\tprovider: \"google-vertex\",\n\t\t\tbaseUrl: \"https://{location}-aiplatform.googleapis.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.1,\n\t\t\t\toutput: 0.4,\n\t\t\t\tcacheRead: 0.01,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-vertex\">,\n\t\t\"gemini-2.5-pro\": {\n\t\t\tid: \"gemini-2.5-pro\",\n\t\t\tname: \"Gemini 2.5 Pro (Vertex)\",\n\t\t\tapi: \"google-vertex\",\n\t\t\tprovider: \"google-vertex\",\n\t\t\tbaseUrl: \"https://{location}-aiplatform.googleapis.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-vertex\">,\n\t\t\"gemini-3-flash-preview\": {\n\t\t\tid: \"gemini-3-flash-preview\",\n\t\t\tname: \"Gemini 3 Flash Preview (Vertex)\",\n\t\t\tapi: \"google-vertex\",\n\t\t\tprovider: \"google-vertex\",\n\t\t\tbaseUrl: \"https://{location}-aiplatform.googleapis.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 3,\n\t\t\t\tcacheRead: 0.05,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-vertex\">,\n\t\t\"gemini-3-pro-preview\": {\n\t\t\tid: \"gemini-3-pro-preview\",\n\t\t\tname: \"Gemini 3 Pro Preview (Vertex)\",\n\t\t\tapi: \"google-vertex\",\n\t\t\tprovider: \"google-vertex\",\n\t\t\tbaseUrl: \"https://{location}-aiplatform.googleapis.com\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 12,\n\t\t\t\tcacheRead: 0.2,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"google-vertex\">,\n\t},\n\t\"groq\": {\n\t\t\"deepseek-r1-distill-llama-70b\": {\n\t\t\tid: \"deepseek-r1-distill-llama-70b\",\n\t\t\tname: \"DeepSeek R1 Distill Llama 70B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"groq\",\n\t\t\tbaseUrl: \"https://api.groq.com/openai/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.75,\n\t\t\t\toutput: 0.99,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"gemma2-9b-it\": {\n\t\t\tid: \"gemma2-9b-it\",\n\t\t\tname: \"Gemma 2 9B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"groq\",\n\t\t\tbaseUrl: \"https://api.groq.com/openai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.2,\n\t\t\t\toutput: 0.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 8192,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"llama-3.1-8b-instant\": {\n\t\t\tid: \"llama-3.1-8b-instant\",\n\t\t\tname: \"Llama 3.1 8B Instant\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"groq\",\n\t\t\tbaseUrl: \"https://api.groq.com/openai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.05,\n\t\t\t\toutput: 0.08,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"llama-3.3-70b-versatile\": {\n\t\t\tid: \"llama-3.3-70b-versatile\",\n\t\t\tname: \"Llama 3.3 70B Versatile\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"groq\",\n\t\t\tbaseUrl: \"https://api.groq.com/openai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.59,\n\t\t\t\toutput: 0.79,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"llama3-70b-8192\": {\n\t\t\tid: \"llama3-70b-8192\",\n\t\t\tname: \"Llama 3 70B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"groq\",\n\t\t\tbaseUrl: \"https://api.groq.com/openai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.59,\n\t\t\t\toutput: 0.79,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 8192,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"llama3-8b-8192\": {\n\t\t\tid: \"llama3-8b-8192\",\n\t\t\tname: \"Llama 3 8B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"groq\",\n\t\t\tbaseUrl: \"https://api.groq.com/openai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.05,\n\t\t\t\toutput: 0.08,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 8192,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"meta-llama/llama-4-maverick-17b-128e-instruct\": {\n\t\t\tid: \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n\t\t\tname: \"Llama 4 Maverick 17B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"groq\",\n\t\t\tbaseUrl: \"https://api.groq.com/openai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.2,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"meta-llama/llama-4-scout-17b-16e-instruct\": {\n\t\t\tid: \"meta-llama/llama-4-scout-17b-16e-instruct\",\n\t\t\tname: \"Llama 4 Scout 17B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"groq\",\n\t\t\tbaseUrl: \"https://api.groq.com/openai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.11,\n\t\t\t\toutput: 0.34,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistral-saba-24b\": {\n\t\t\tid: \"mistral-saba-24b\",\n\t\t\tname: \"Mistral Saba 24B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"groq\",\n\t\t\tbaseUrl: \"https://api.groq.com/openai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.79,\n\t\t\t\toutput: 0.79,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"moonshotai/kimi-k2-instruct\": {\n\t\t\tid: \"moonshotai/kimi-k2-instruct\",\n\t\t\tname: \"Kimi K2 Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"groq\",\n\t\t\tbaseUrl: \"https://api.groq.com/openai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1,\n\t\t\t\toutput: 3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"moonshotai/kimi-k2-instruct-0905\": {\n\t\t\tid: \"moonshotai/kimi-k2-instruct-0905\",\n\t\t\tname: \"Kimi K2 Instruct 0905\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"groq\",\n\t\t\tbaseUrl: \"https://api.groq.com/openai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1,\n\t\t\t\toutput: 3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-oss-120b\": {\n\t\t\tid: \"openai/gpt-oss-120b\",\n\t\t\tname: \"GPT OSS 120B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"groq\",\n\t\t\tbaseUrl: \"https://api.groq.com/openai/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-oss-20b\": {\n\t\t\tid: \"openai/gpt-oss-20b\",\n\t\t\tname: \"GPT OSS 20B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"groq\",\n\t\t\tbaseUrl: \"https://api.groq.com/openai/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.075,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen-qwq-32b\": {\n\t\t\tid: \"qwen-qwq-32b\",\n\t\t\tname: \"Qwen QwQ 32B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"groq\",\n\t\t\tbaseUrl: \"https://api.groq.com/openai/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.29,\n\t\t\t\toutput: 0.39,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-32b\": {\n\t\t\tid: \"qwen/qwen3-32b\",\n\t\t\tname: \"Qwen3 32B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"groq\",\n\t\t\tbaseUrl: \"https://api.groq.com/openai/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.29,\n\t\t\t\toutput: 0.59,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t},\n\t\"minimax\": {\n\t\t\"MiniMax-M2\": {\n\t\t\tid: \"MiniMax-M2\",\n\t\t\tname: \"MiniMax-M2\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"minimax\",\n\t\t\tbaseUrl: \"https://api.minimax.io/anthropic\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 1.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 196608,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"MiniMax-M2.1\": {\n\t\t\tid: \"MiniMax-M2.1\",\n\t\t\tname: \"MiniMax-M2.1\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"minimax\",\n\t\t\tbaseUrl: \"https://api.minimax.io/anthropic\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 1.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 204800,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t},\n\t\"minimax-cn\": {\n\t\t\"MiniMax-M2\": {\n\t\t\tid: \"MiniMax-M2\",\n\t\t\tname: \"MiniMax-M2\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"minimax-cn\",\n\t\t\tbaseUrl: \"https://api.minimaxi.com/anthropic\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 1.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 196608,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"MiniMax-M2.1\": {\n\t\t\tid: \"MiniMax-M2.1\",\n\t\t\tname: \"MiniMax-M2.1\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"minimax-cn\",\n\t\t\tbaseUrl: \"https://api.minimaxi.com/anthropic\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 1.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 204800,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t},\n\t\"mistral\": {\n\t\t\"codestral-latest\": {\n\t\t\tid: \"codestral-latest\",\n\t\t\tname: \"Codestral\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 0.9,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"devstral-2512\": {\n\t\t\tid: \"devstral-2512\",\n\t\t\tname: \"Devstral 2\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 262144,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"devstral-medium-2507\": {\n\t\t\tid: \"devstral-medium-2507\",\n\t\t\tname: \"Devstral Medium\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.4,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"devstral-medium-latest\": {\n\t\t\tid: \"devstral-medium-latest\",\n\t\t\tname: \"Devstral 2\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.4,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 262144,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"devstral-small-2505\": {\n\t\t\tid: \"devstral-small-2505\",\n\t\t\tname: \"Devstral Small 2505\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.1,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"devstral-small-2507\": {\n\t\t\tid: \"devstral-small-2507\",\n\t\t\tname: \"Devstral Small\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.1,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"labs-devstral-small-2512\": {\n\t\t\tid: \"labs-devstral-small-2512\",\n\t\t\tname: \"Devstral Small 2\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 256000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"magistral-medium-latest\": {\n\t\t\tid: \"magistral-medium-latest\",\n\t\t\tname: \"Magistral Medium\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"magistral-small\": {\n\t\t\tid: \"magistral-small\",\n\t\t\tname: \"Magistral Small\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 1.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"ministral-3b-latest\": {\n\t\t\tid: \"ministral-3b-latest\",\n\t\t\tname: \"Ministral 3B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.04,\n\t\t\t\toutput: 0.04,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"ministral-8b-latest\": {\n\t\t\tid: \"ministral-8b-latest\",\n\t\t\tname: \"Ministral 8B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.1,\n\t\t\t\toutput: 0.1,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistral-large-2411\": {\n\t\t\tid: \"mistral-large-2411\",\n\t\t\tname: \"Mistral Large 2.1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistral-large-2512\": {\n\t\t\tid: \"mistral-large-2512\",\n\t\t\tname: \"Mistral Large 3\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 1.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 262144,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistral-large-latest\": {\n\t\t\tid: \"mistral-large-latest\",\n\t\t\tname: \"Mistral Large\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 1.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 262144,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistral-medium-2505\": {\n\t\t\tid: \"mistral-medium-2505\",\n\t\t\tname: \"Mistral Medium 3\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.4,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistral-medium-2508\": {\n\t\t\tid: \"mistral-medium-2508\",\n\t\t\tname: \"Mistral Medium 3.1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.4,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 262144,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistral-medium-latest\": {\n\t\t\tid: \"mistral-medium-latest\",\n\t\t\tname: \"Mistral Medium\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.4,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistral-nemo\": {\n\t\t\tid: \"mistral-nemo\",\n\t\t\tname: \"Mistral Nemo\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistral-small-2506\": {\n\t\t\tid: \"mistral-small-2506\",\n\t\t\tname: \"Mistral Small 3.2\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.1,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistral-small-latest\": {\n\t\t\tid: \"mistral-small-latest\",\n\t\t\tname: \"Mistral Small\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.1,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"open-mistral-7b\": {\n\t\t\tid: \"open-mistral-7b\",\n\t\t\tname: \"Mistral 7B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 0.25,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 8000,\n\t\t\tmaxTokens: 8000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"open-mixtral-8x22b\": {\n\t\t\tid: \"open-mixtral-8x22b\",\n\t\t\tname: \"Mixtral 8x22B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 64000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"open-mixtral-8x7b\": {\n\t\t\tid: \"open-mixtral-8x7b\",\n\t\t\tname: \"Mixtral 8x7B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.7,\n\t\t\t\toutput: 0.7,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32000,\n\t\t\tmaxTokens: 32000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"pixtral-12b\": {\n\t\t\tid: \"pixtral-12b\",\n\t\t\tname: \"Pixtral 12B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"pixtral-large-latest\": {\n\t\t\tid: \"pixtral-large-latest\",\n\t\t\tname: \"Pixtral Large\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"mistral\",\n\t\t\tbaseUrl: \"https://api.mistral.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t},\n\t\"openai\": {\n\t\t\"codex-mini-latest\": {\n\t\t\tid: \"codex-mini-latest\",\n\t\t\tname: \"Codex Mini\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.5,\n\t\t\t\toutput: 6,\n\t\t\t\tcacheRead: 0.375,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-4\": {\n\t\t\tid: \"gpt-4\",\n\t\t\tname: \"GPT-4\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 30,\n\t\t\t\toutput: 60,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 8192,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-4-turbo\": {\n\t\t\tid: \"gpt-4-turbo\",\n\t\t\tname: \"GPT-4 Turbo\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 10,\n\t\t\t\toutput: 30,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-4.1\": {\n\t\t\tid: \"gpt-4.1\",\n\t\t\tname: \"GPT-4.1\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 8,\n\t\t\t\tcacheRead: 0.5,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1047576,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-4.1-mini\": {\n\t\t\tid: \"gpt-4.1-mini\",\n\t\t\tname: \"GPT-4.1 mini\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.4,\n\t\t\t\toutput: 1.6,\n\t\t\t\tcacheRead: 0.1,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1047576,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-4.1-nano\": {\n\t\t\tid: \"gpt-4.1-nano\",\n\t\t\tname: \"GPT-4.1 nano\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.1,\n\t\t\t\toutput: 0.4,\n\t\t\t\tcacheRead: 0.03,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1047576,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-4o\": {\n\t\t\tid: \"gpt-4o\",\n\t\t\tname: \"GPT-4o\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2.5,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 1.25,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-4o-2024-05-13\": {\n\t\t\tid: \"gpt-4o-2024-05-13\",\n\t\t\tname: \"GPT-4o (2024-05-13)\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 5,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-4o-2024-08-06\": {\n\t\t\tid: \"gpt-4o-2024-08-06\",\n\t\t\tname: \"GPT-4o (2024-08-06)\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2.5,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 1.25,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-4o-2024-11-20\": {\n\t\t\tid: \"gpt-4o-2024-11-20\",\n\t\t\tname: \"GPT-4o (2024-11-20)\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2.5,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 1.25,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-4o-mini\": {\n\t\t\tid: \"gpt-4o-mini\",\n\t\t\tname: \"GPT-4o mini\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0.08,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5\": {\n\t\t\tid: \"gpt-5\",\n\t\t\tname: \"GPT-5\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5-chat-latest\": {\n\t\t\tid: \"gpt-5-chat-latest\",\n\t\t\tname: \"GPT-5 Chat Latest\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5-codex\": {\n\t\t\tid: \"gpt-5-codex\",\n\t\t\tname: \"GPT-5-Codex\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5-mini\": {\n\t\t\tid: \"gpt-5-mini\",\n\t\t\tname: \"GPT-5 Mini\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0.025,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5-nano\": {\n\t\t\tid: \"gpt-5-nano\",\n\t\t\tname: \"GPT-5 Nano\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.05,\n\t\t\t\toutput: 0.4,\n\t\t\t\tcacheRead: 0.005,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5-pro\": {\n\t\t\tid: \"gpt-5-pro\",\n\t\t\tname: \"GPT-5 Pro\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 120,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 272000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.1\": {\n\t\t\tid: \"gpt-5.1\",\n\t\t\tname: \"GPT-5.1\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.13,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.1-chat-latest\": {\n\t\t\tid: \"gpt-5.1-chat-latest\",\n\t\t\tname: \"GPT-5.1 Chat\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.1-codex\": {\n\t\t\tid: \"gpt-5.1-codex\",\n\t\t\tname: \"GPT-5.1 Codex\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.1-codex-max\": {\n\t\t\tid: \"gpt-5.1-codex-max\",\n\t\t\tname: \"GPT-5.1 Codex Max\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.1-codex-mini\": {\n\t\t\tid: \"gpt-5.1-codex-mini\",\n\t\t\tname: \"GPT-5.1 Codex mini\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0.025,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.2\": {\n\t\t\tid: \"gpt-5.2\",\n\t\t\tname: \"GPT-5.2\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.75,\n\t\t\t\toutput: 14,\n\t\t\t\tcacheRead: 0.175,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.2-chat-latest\": {\n\t\t\tid: \"gpt-5.2-chat-latest\",\n\t\t\tname: \"GPT-5.2 Chat\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.75,\n\t\t\t\toutput: 14,\n\t\t\t\tcacheRead: 0.175,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.2-codex\": {\n\t\t\tid: \"gpt-5.2-codex\",\n\t\t\tname: \"GPT-5.2 Codex\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.75,\n\t\t\t\toutput: 14,\n\t\t\t\tcacheRead: 0.175,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.2-pro\": {\n\t\t\tid: \"gpt-5.2-pro\",\n\t\t\tname: \"GPT-5.2 Pro\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 21,\n\t\t\t\toutput: 168,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"o1\": {\n\t\t\tid: \"o1\",\n\t\t\tname: \"o1\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 60,\n\t\t\t\tcacheRead: 7.5,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"o1-pro\": {\n\t\t\tid: \"o1-pro\",\n\t\t\tname: \"o1-pro\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 150,\n\t\t\t\toutput: 600,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"o3\": {\n\t\t\tid: \"o3\",\n\t\t\tname: \"o3\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 8,\n\t\t\t\tcacheRead: 0.5,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"o3-deep-research\": {\n\t\t\tid: \"o3-deep-research\",\n\t\t\tname: \"o3-deep-research\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 10,\n\t\t\t\toutput: 40,\n\t\t\t\tcacheRead: 2.5,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"o3-mini\": {\n\t\t\tid: \"o3-mini\",\n\t\t\tname: \"o3-mini\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.1,\n\t\t\t\toutput: 4.4,\n\t\t\t\tcacheRead: 0.55,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"o3-pro\": {\n\t\t\tid: \"o3-pro\",\n\t\t\tname: \"o3-pro\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 20,\n\t\t\t\toutput: 80,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"o4-mini\": {\n\t\t\tid: \"o4-mini\",\n\t\t\tname: \"o4-mini\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.1,\n\t\t\t\toutput: 4.4,\n\t\t\t\tcacheRead: 0.28,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"o4-mini-deep-research\": {\n\t\t\tid: \"o4-mini-deep-research\",\n\t\t\tname: \"o4-mini-deep-research\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"openai\",\n\t\t\tbaseUrl: \"https://api.openai.com/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 8,\n\t\t\t\tcacheRead: 0.5,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t},\n\t\"openai-codex\": {\n\t\t\"gpt-5.1\": {\n\t\t\tid: \"gpt-5.1\",\n\t\t\tname: \"GPT-5.1\",\n\t\t\tapi: \"openai-codex-responses\",\n\t\t\tprovider: \"openai-codex\",\n\t\t\tbaseUrl: \"https://chatgpt.com/backend-api\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 272000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-codex-responses\">,\n\t\t\"gpt-5.1-codex-max\": {\n\t\t\tid: \"gpt-5.1-codex-max\",\n\t\t\tname: \"GPT-5.1 Codex Max\",\n\t\t\tapi: \"openai-codex-responses\",\n\t\t\tprovider: \"openai-codex\",\n\t\t\tbaseUrl: \"https://chatgpt.com/backend-api\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 272000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-codex-responses\">,\n\t\t\"gpt-5.1-codex-mini\": {\n\t\t\tid: \"gpt-5.1-codex-mini\",\n\t\t\tname: \"GPT-5.1 Codex Mini\",\n\t\t\tapi: \"openai-codex-responses\",\n\t\t\tprovider: \"openai-codex\",\n\t\t\tbaseUrl: \"https://chatgpt.com/backend-api\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0.025,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 272000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-codex-responses\">,\n\t\t\"gpt-5.2\": {\n\t\t\tid: \"gpt-5.2\",\n\t\t\tname: \"GPT-5.2\",\n\t\t\tapi: \"openai-codex-responses\",\n\t\t\tprovider: \"openai-codex\",\n\t\t\tbaseUrl: \"https://chatgpt.com/backend-api\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.75,\n\t\t\t\toutput: 14,\n\t\t\t\tcacheRead: 0.175,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 272000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-codex-responses\">,\n\t\t\"gpt-5.2-codex\": {\n\t\t\tid: \"gpt-5.2-codex\",\n\t\t\tname: \"GPT-5.2 Codex\",\n\t\t\tapi: \"openai-codex-responses\",\n\t\t\tprovider: \"openai-codex\",\n\t\t\tbaseUrl: \"https://chatgpt.com/backend-api\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.75,\n\t\t\t\toutput: 14,\n\t\t\t\tcacheRead: 0.175,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 272000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-codex-responses\">,\n\t},\n\t\"opencode\": {\n\t\t\"alpha-gd4\": {\n\t\t\tid: \"alpha-gd4\",\n\t\t\tname: \"Alpha GD4\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0.15,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"alpha-glm-4.7\": {\n\t\t\tid: \"alpha-glm-4.7\",\n\t\t\tname: \"Alpha GLM-4.7\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.6,\n\t\t\t\toutput: 2.2,\n\t\t\t\tcacheRead: 0.6,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 204800,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"big-pickle\": {\n\t\t\tid: \"big-pickle\",\n\t\t\tname: \"Big Pickle\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"claude-3-5-haiku\": {\n\t\t\tid: \"claude-3-5-haiku\",\n\t\t\tname: \"Claude Haiku 3.5\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.8,\n\t\t\t\toutput: 4,\n\t\t\t\tcacheRead: 0.08,\n\t\t\t\tcacheWrite: 1,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-haiku-4-5\": {\n\t\t\tid: \"claude-haiku-4-5\",\n\t\t\tname: \"Claude Haiku 4.5\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1,\n\t\t\t\toutput: 5,\n\t\t\t\tcacheRead: 0.1,\n\t\t\t\tcacheWrite: 1.25,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-opus-4-1\": {\n\t\t\tid: \"claude-opus-4-1\",\n\t\t\tname: \"Claude Opus 4.1\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 75,\n\t\t\t\tcacheRead: 1.5,\n\t\t\t\tcacheWrite: 18.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 32000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-opus-4-5\": {\n\t\t\tid: \"claude-opus-4-5\",\n\t\t\tname: \"Claude Opus 4.5\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 5,\n\t\t\t\toutput: 25,\n\t\t\t\tcacheRead: 0.5,\n\t\t\t\tcacheWrite: 6.25,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-sonnet-4\": {\n\t\t\tid: \"claude-sonnet-4\",\n\t\t\tname: \"Claude Sonnet 4\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"claude-sonnet-4-5\": {\n\t\t\tid: \"claude-sonnet-4-5\",\n\t\t\tname: \"Claude Sonnet 4.5\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"gemini-3-flash\": {\n\t\t\tid: \"gemini-3-flash\",\n\t\t\tname: \"Gemini 3 Flash\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 3,\n\t\t\t\tcacheRead: 0.05,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"gemini-3-pro\": {\n\t\t\tid: \"gemini-3-pro\",\n\t\t\tname: \"Gemini 3 Pro\",\n\t\t\tapi: \"google-generative-ai\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 12,\n\t\t\t\tcacheRead: 0.2,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"google-generative-ai\">,\n\t\t\"glm-4.6\": {\n\t\t\tid: \"glm-4.6\",\n\t\t\tname: \"GLM-4.6\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.6,\n\t\t\t\toutput: 2.2,\n\t\t\t\tcacheRead: 0.1,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 204800,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"glm-4.7-free\": {\n\t\t\tid: \"glm-4.7-free\",\n\t\t\tname: \"GLM-4.7\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 204800,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"gpt-5\": {\n\t\t\tid: \"gpt-5\",\n\t\t\tname: \"GPT-5\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.07,\n\t\t\t\toutput: 8.5,\n\t\t\t\tcacheRead: 0.107,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5-codex\": {\n\t\t\tid: \"gpt-5-codex\",\n\t\t\tname: \"GPT-5 Codex\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.07,\n\t\t\t\toutput: 8.5,\n\t\t\t\tcacheRead: 0.107,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5-nano\": {\n\t\t\tid: \"gpt-5-nano\",\n\t\t\tname: \"GPT-5 Nano\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.1\": {\n\t\t\tid: \"gpt-5.1\",\n\t\t\tname: \"GPT-5.1\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.07,\n\t\t\t\toutput: 8.5,\n\t\t\t\tcacheRead: 0.107,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.1-codex\": {\n\t\t\tid: \"gpt-5.1-codex\",\n\t\t\tname: \"GPT-5.1 Codex\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.07,\n\t\t\t\toutput: 8.5,\n\t\t\t\tcacheRead: 0.107,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.1-codex-max\": {\n\t\t\tid: \"gpt-5.1-codex-max\",\n\t\t\tname: \"GPT-5.1 Codex Max\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.1-codex-mini\": {\n\t\t\tid: \"gpt-5.1-codex-mini\",\n\t\t\tname: \"GPT-5.1 Codex Mini\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0.025,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.2\": {\n\t\t\tid: \"gpt-5.2\",\n\t\t\tname: \"GPT-5.2\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.75,\n\t\t\t\toutput: 14,\n\t\t\t\tcacheRead: 0.175,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"gpt-5.2-codex\": {\n\t\t\tid: \"gpt-5.2-codex\",\n\t\t\tname: \"GPT-5.2 Codex\",\n\t\t\tapi: \"openai-responses\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.75,\n\t\t\t\toutput: 14,\n\t\t\t\tcacheRead: 0.175,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-responses\">,\n\t\t\"grok-code\": {\n\t\t\tid: \"grok-code\",\n\t\t\tname: \"Grok Code Fast 1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 256000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"kimi-k2\": {\n\t\t\tid: \"kimi-k2\",\n\t\t\tname: \"Kimi K2\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.4,\n\t\t\t\toutput: 2.5,\n\t\t\t\tcacheRead: 0.4,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 262144,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"kimi-k2-thinking\": {\n\t\t\tid: \"kimi-k2-thinking\",\n\t\t\tname: \"Kimi K2 Thinking\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.4,\n\t\t\t\toutput: 2.5,\n\t\t\t\tcacheRead: 0.4,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 262144,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"minimax-m2.1-free\": {\n\t\t\tid: \"minimax-m2.1-free\",\n\t\t\tname: \"MiniMax M2.1\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 204800,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"qwen3-coder\": {\n\t\t\tid: \"qwen3-coder\",\n\t\t\tname: \"Qwen3 Coder\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"opencode\",\n\t\t\tbaseUrl: \"https://opencode.ai/zen/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.45,\n\t\t\t\toutput: 1.8,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t},\n\t\"openrouter\": {\n\t\t\"ai21/jamba-large-1.7\": {\n\t\t\tid: \"ai21/jamba-large-1.7\",\n\t\t\tname: \"AI21: Jamba Large 1.7\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 8,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"ai21/jamba-mini-1.7\": {\n\t\t\tid: \"ai21/jamba-mini-1.7\",\n\t\t\tname: \"AI21: Jamba Mini 1.7\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"alibaba/tongyi-deepresearch-30b-a3b\": {\n\t\t\tid: \"alibaba/tongyi-deepresearch-30b-a3b\",\n\t\t\tname: \"Tongyi DeepResearch 30B A3B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"allenai/olmo-3.1-32b-instruct\": {\n\t\t\tid: \"allenai/olmo-3.1-32b-instruct\",\n\t\t\tname: \"AllenAI: Olmo 3.1 32B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 65536,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"amazon/nova-2-lite-v1\": {\n\t\t\tid: \"amazon/nova-2-lite-v1\",\n\t\t\tname: \"Amazon: Nova 2 Lite\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 2.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 65535,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"amazon/nova-lite-v1\": {\n\t\t\tid: \"amazon/nova-lite-v1\",\n\t\t\tname: \"Amazon: Nova Lite 1.0\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.06,\n\t\t\t\toutput: 0.24,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 300000,\n\t\t\tmaxTokens: 5120,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"amazon/nova-micro-v1\": {\n\t\t\tid: \"amazon/nova-micro-v1\",\n\t\t\tname: \"Amazon: Nova Micro 1.0\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.035,\n\t\t\t\toutput: 0.14,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 5120,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"amazon/nova-premier-v1\": {\n\t\t\tid: \"amazon/nova-premier-v1\",\n\t\t\tname: \"Amazon: Nova Premier 1.0\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2.5,\n\t\t\t\toutput: 12.5,\n\t\t\t\tcacheRead: 0.625,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 32000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"amazon/nova-pro-v1\": {\n\t\t\tid: \"amazon/nova-pro-v1\",\n\t\t\tname: \"Amazon: Nova Pro 1.0\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.7999999999999999,\n\t\t\t\toutput: 3.1999999999999997,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 300000,\n\t\t\tmaxTokens: 5120,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"anthropic/claude-3-haiku\": {\n\t\t\tid: \"anthropic/claude-3-haiku\",\n\t\t\tname: \"Anthropic: Claude 3 Haiku\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 1.25,\n\t\t\t\tcacheRead: 0.03,\n\t\t\t\tcacheWrite: 0.3,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"anthropic/claude-3.5-haiku\": {\n\t\t\tid: \"anthropic/claude-3.5-haiku\",\n\t\t\tname: \"Anthropic: Claude 3.5 Haiku\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.7999999999999999,\n\t\t\t\toutput: 4,\n\t\t\t\tcacheRead: 0.08,\n\t\t\t\tcacheWrite: 1,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"anthropic/claude-3.5-sonnet\": {\n\t\t\tid: \"anthropic/claude-3.5-sonnet\",\n\t\t\tname: \"Anthropic: Claude 3.5 Sonnet\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 6,\n\t\t\t\toutput: 30,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"anthropic/claude-3.7-sonnet\": {\n\t\t\tid: \"anthropic/claude-3.7-sonnet\",\n\t\t\tname: \"Anthropic: Claude 3.7 Sonnet\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"anthropic/claude-3.7-sonnet:thinking\": {\n\t\t\tid: \"anthropic/claude-3.7-sonnet:thinking\",\n\t\t\tname: \"Anthropic: Claude 3.7 Sonnet (thinking)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"anthropic/claude-haiku-4.5\": {\n\t\t\tid: \"anthropic/claude-haiku-4.5\",\n\t\t\tname: \"Anthropic: Claude Haiku 4.5\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1,\n\t\t\t\toutput: 5,\n\t\t\t\tcacheRead: 0.09999999999999999,\n\t\t\t\tcacheWrite: 1.25,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"anthropic/claude-opus-4\": {\n\t\t\tid: \"anthropic/claude-opus-4\",\n\t\t\tname: \"Anthropic: Claude Opus 4\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 75,\n\t\t\t\tcacheRead: 1.5,\n\t\t\t\tcacheWrite: 18.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 32000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"anthropic/claude-opus-4.1\": {\n\t\t\tid: \"anthropic/claude-opus-4.1\",\n\t\t\tname: \"Anthropic: Claude Opus 4.1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 75,\n\t\t\t\tcacheRead: 1.5,\n\t\t\t\tcacheWrite: 18.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 32000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"anthropic/claude-opus-4.5\": {\n\t\t\tid: \"anthropic/claude-opus-4.5\",\n\t\t\tname: \"Anthropic: Claude Opus 4.5\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 5,\n\t\t\t\toutput: 25,\n\t\t\t\tcacheRead: 0.5,\n\t\t\t\tcacheWrite: 6.25,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"anthropic/claude-sonnet-4\": {\n\t\t\tid: \"anthropic/claude-sonnet-4\",\n\t\t\tname: \"Anthropic: Claude Sonnet 4\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"anthropic/claude-sonnet-4.5\": {\n\t\t\tid: \"anthropic/claude-sonnet-4.5\",\n\t\t\tname: \"Anthropic: Claude Sonnet 4.5\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"arcee-ai/trinity-mini\": {\n\t\t\tid: \"arcee-ai/trinity-mini\",\n\t\t\tname: \"Arcee AI: Trinity Mini\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.045,\n\t\t\t\toutput: 0.15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"arcee-ai/trinity-mini:free\": {\n\t\t\tid: \"arcee-ai/trinity-mini:free\",\n\t\t\tname: \"Arcee AI: Trinity Mini (free)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"arcee-ai/virtuoso-large\": {\n\t\t\tid: \"arcee-ai/virtuoso-large\",\n\t\t\tname: \"Arcee AI: Virtuoso Large\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.75,\n\t\t\t\toutput: 1.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"baidu/ernie-4.5-21b-a3b\": {\n\t\t\tid: \"baidu/ernie-4.5-21b-a3b\",\n\t\t\tname: \"Baidu: ERNIE 4.5 21B A3B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.07,\n\t\t\t\toutput: 0.28,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 120000,\n\t\t\tmaxTokens: 8000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"baidu/ernie-4.5-vl-28b-a3b\": {\n\t\t\tid: \"baidu/ernie-4.5-vl-28b-a3b\",\n\t\t\tname: \"Baidu: ERNIE 4.5 VL 28B A3B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.14,\n\t\t\t\toutput: 0.56,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 30000,\n\t\t\tmaxTokens: 8000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"bytedance-seed/seed-1.6\": {\n\t\t\tid: \"bytedance-seed/seed-1.6\",\n\t\t\tname: \"ByteDance Seed: Seed 1.6\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"bytedance-seed/seed-1.6-flash\": {\n\t\t\tid: \"bytedance-seed/seed-1.6-flash\",\n\t\t\tname: \"ByteDance Seed: Seed 1.6 Flash\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.075,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"cohere/command-r-08-2024\": {\n\t\t\tid: \"cohere/command-r-08-2024\",\n\t\t\tname: \"Cohere: Command R (08-2024)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"cohere/command-r-plus-08-2024\": {\n\t\t\tid: \"cohere/command-r-plus-08-2024\",\n\t\t\tname: \"Cohere: Command R+ (08-2024)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2.5,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"deepcogito/cogito-v2-preview-llama-109b-moe\": {\n\t\t\tid: \"deepcogito/cogito-v2-preview-llama-109b-moe\",\n\t\t\tname: \"Cogito V2 Preview Llama 109B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.18,\n\t\t\t\toutput: 0.59,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32767,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"deepcogito/cogito-v2-preview-llama-405b\": {\n\t\t\tid: \"deepcogito/cogito-v2-preview-llama-405b\",\n\t\t\tname: \"Deep Cogito: Cogito V2 Preview Llama 405B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3.5,\n\t\t\t\toutput: 3.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"deepcogito/cogito-v2-preview-llama-70b\": {\n\t\t\tid: \"deepcogito/cogito-v2-preview-llama-70b\",\n\t\t\tname: \"Deep Cogito: Cogito V2 Preview Llama 70B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.88,\n\t\t\t\toutput: 0.88,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"deepseek/deepseek-chat\": {\n\t\t\tid: \"deepseek/deepseek-chat\",\n\t\t\tname: \"DeepSeek: DeepSeek V3\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 1.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 163840,\n\t\t\tmaxTokens: 163840,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"deepseek/deepseek-chat-v3-0324\": {\n\t\t\tid: \"deepseek/deepseek-chat-v3-0324\",\n\t\t\tname: \"DeepSeek: DeepSeek V3 0324\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19,\n\t\t\t\toutput: 0.87,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 163840,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"deepseek/deepseek-chat-v3.1\": {\n\t\t\tid: \"deepseek/deepseek-chat-v3.1\",\n\t\t\tname: \"DeepSeek: DeepSeek V3.1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.75,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 7168,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"deepseek/deepseek-r1\": {\n\t\t\tid: \"deepseek/deepseek-r1\",\n\t\t\tname: \"DeepSeek: R1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.7,\n\t\t\t\toutput: 2.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 64000,\n\t\t\tmaxTokens: 16000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"deepseek/deepseek-r1-0528\": {\n\t\t\tid: \"deepseek/deepseek-r1-0528\",\n\t\t\tname: \"DeepSeek: R1 0528\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.39999999999999997,\n\t\t\t\toutput: 1.75,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 163840,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"deepseek/deepseek-r1-distill-llama-70b\": {\n\t\t\tid: \"deepseek/deepseek-r1-distill-llama-70b\",\n\t\t\tname: \"DeepSeek: R1 Distill Llama 70B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.03,\n\t\t\t\toutput: 0.11,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"deepseek/deepseek-v3.1-terminus\": {\n\t\t\tid: \"deepseek/deepseek-v3.1-terminus\",\n\t\t\tname: \"DeepSeek: DeepSeek V3.1 Terminus\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.21,\n\t\t\t\toutput: 0.7899999999999999,\n\t\t\t\tcacheRead: 0.16799999999999998,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 163840,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"deepseek/deepseek-v3.1-terminus:exacto\": {\n\t\t\tid: \"deepseek/deepseek-v3.1-terminus:exacto\",\n\t\t\tname: \"DeepSeek: DeepSeek V3.1 Terminus (exacto)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.21,\n\t\t\t\toutput: 0.7899999999999999,\n\t\t\t\tcacheRead: 0.16799999999999998,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 163840,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"deepseek/deepseek-v3.2\": {\n\t\t\tid: \"deepseek/deepseek-v3.2\",\n\t\t\tname: \"DeepSeek: DeepSeek V3.2\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 0.38,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 163840,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"deepseek/deepseek-v3.2-exp\": {\n\t\t\tid: \"deepseek/deepseek-v3.2-exp\",\n\t\t\tname: \"DeepSeek: DeepSeek V3.2 Exp\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.21,\n\t\t\t\toutput: 0.32,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 163840,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"google/gemini-2.0-flash-001\": {\n\t\t\tid: \"google/gemini-2.0-flash-001\",\n\t\t\tname: \"Google: Gemini 2.0 Flash\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0.024999999999999998,\n\t\t\t\tcacheWrite: 0.0833,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"google/gemini-2.0-flash-exp:free\": {\n\t\t\tid: \"google/gemini-2.0-flash-exp:free\",\n\t\t\tname: \"Google: Gemini 2.0 Flash Experimental (free)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"google/gemini-2.0-flash-lite-001\": {\n\t\t\tid: \"google/gemini-2.0-flash-lite-001\",\n\t\t\tname: \"Google: Gemini 2.0 Flash Lite\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.075,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"google/gemini-2.5-flash\": {\n\t\t\tid: \"google/gemini-2.5-flash\",\n\t\t\tname: \"Google: Gemini 2.5 Flash\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 2.5,\n\t\t\t\tcacheRead: 0.03,\n\t\t\t\tcacheWrite: 0.08333333333333334,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65535,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"google/gemini-2.5-flash-lite\": {\n\t\t\tid: \"google/gemini-2.5-flash-lite\",\n\t\t\tname: \"Google: Gemini 2.5 Flash Lite\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0.01,\n\t\t\t\tcacheWrite: 0.0833,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65535,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"google/gemini-2.5-flash-lite-preview-09-2025\": {\n\t\t\tid: \"google/gemini-2.5-flash-lite-preview-09-2025\",\n\t\t\tname: \"Google: Gemini 2.5 Flash Lite Preview 09-2025\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0.01,\n\t\t\t\tcacheWrite: 0.0833,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65535,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"google/gemini-2.5-flash-preview-09-2025\": {\n\t\t\tid: \"google/gemini-2.5-flash-preview-09-2025\",\n\t\t\tname: \"Google: Gemini 2.5 Flash Preview 09-2025\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 2.5,\n\t\t\t\tcacheRead: 0.03,\n\t\t\t\tcacheWrite: 0.0833,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65535,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"google/gemini-2.5-pro\": {\n\t\t\tid: \"google/gemini-2.5-pro\",\n\t\t\tname: \"Google: Gemini 2.5 Pro\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0.375,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"google/gemini-2.5-pro-preview\": {\n\t\t\tid: \"google/gemini-2.5-pro-preview\",\n\t\t\tname: \"Google: Gemini 2.5 Pro Preview 06-05\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.31,\n\t\t\t\tcacheWrite: 0.375,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"google/gemini-2.5-pro-preview-05-06\": {\n\t\t\tid: \"google/gemini-2.5-pro-preview-05-06\",\n\t\t\tname: \"Google: Gemini 2.5 Pro Preview 05-06\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.31,\n\t\t\t\tcacheWrite: 0.375,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65535,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"google/gemini-3-flash-preview\": {\n\t\t\tid: \"google/gemini-3-flash-preview\",\n\t\t\tname: \"Google: Gemini 3 Flash Preview\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 3,\n\t\t\t\tcacheRead: 0.049999999999999996,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65535,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"google/gemini-3-pro-preview\": {\n\t\t\tid: \"google/gemini-3-pro-preview\",\n\t\t\tname: \"Google: Gemini 3 Pro Preview\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 12,\n\t\t\t\tcacheRead: 0.19999999999999998,\n\t\t\t\tcacheWrite: 0.375,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"google/gemma-3-27b-it\": {\n\t\t\tid: \"google/gemma-3-27b-it\",\n\t\t\tname: \"Google: Gemma 3 27B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.04,\n\t\t\t\toutput: 0.15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 96000,\n\t\t\tmaxTokens: 96000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"google/gemma-3-27b-it:free\": {\n\t\t\tid: \"google/gemma-3-27b-it:free\",\n\t\t\tname: \"Google: Gemma 3 27B (free)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"inception/mercury\": {\n\t\t\tid: \"inception/mercury\",\n\t\t\tname: \"Inception: Mercury\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 1,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"inception/mercury-coder\": {\n\t\t\tid: \"inception/mercury-coder\",\n\t\t\tname: \"Inception: Mercury Coder\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 1,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"kwaipilot/kat-coder-pro\": {\n\t\t\tid: \"kwaipilot/kat-coder-pro\",\n\t\t\tname: \"Kwaipilot: KAT-Coder-Pro V1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.207,\n\t\t\t\toutput: 0.828,\n\t\t\t\tcacheRead: 0.0414,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"meta-llama/llama-3-8b-instruct\": {\n\t\t\tid: \"meta-llama/llama-3-8b-instruct\",\n\t\t\tname: \"Meta: Llama 3 8B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.03,\n\t\t\t\toutput: 0.06,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 8192,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"meta-llama/llama-3.1-405b-instruct\": {\n\t\t\tid: \"meta-llama/llama-3.1-405b-instruct\",\n\t\t\tname: \"Meta: Llama 3.1 405B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3.5,\n\t\t\t\toutput: 3.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 10000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"meta-llama/llama-3.1-70b-instruct\": {\n\t\t\tid: \"meta-llama/llama-3.1-70b-instruct\",\n\t\t\tname: \"Meta: Llama 3.1 70B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.39999999999999997,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"meta-llama/llama-3.1-8b-instruct\": {\n\t\t\tid: \"meta-llama/llama-3.1-8b-instruct\",\n\t\t\tname: \"Meta: Llama 3.1 8B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.02,\n\t\t\t\toutput: 0.049999999999999996,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 16384,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"meta-llama/llama-3.3-70b-instruct\": {\n\t\t\tid: \"meta-llama/llama-3.3-70b-instruct\",\n\t\t\tname: \"Meta: Llama 3.3 70B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.32,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"meta-llama/llama-3.3-70b-instruct:free\": {\n\t\t\tid: \"meta-llama/llama-3.3-70b-instruct:free\",\n\t\t\tname: \"Meta: Llama 3.3 70B Instruct (free)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"meta-llama/llama-4-maverick\": {\n\t\t\tid: \"meta-llama/llama-4-maverick\",\n\t\t\tname: \"Meta: Llama 4 Maverick\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"meta-llama/llama-4-scout\": {\n\t\t\tid: \"meta-llama/llama-4-scout\",\n\t\t\tname: \"Meta: Llama 4 Scout\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.08,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 327680,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"minimax/minimax-m1\": {\n\t\t\tid: \"minimax/minimax-m1\",\n\t\t\tname: \"MiniMax: MiniMax M1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.39999999999999997,\n\t\t\t\toutput: 2.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 40000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"minimax/minimax-m2\": {\n\t\t\tid: \"minimax/minimax-m2\",\n\t\t\tname: \"MiniMax: MiniMax M2\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 1,\n\t\t\t\tcacheRead: 0.03,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 196608,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"minimax/minimax-m2.1\": {\n\t\t\tid: \"minimax/minimax-m2.1\",\n\t\t\tname: \"MiniMax: MiniMax M2.1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.27,\n\t\t\t\toutput: 1.12,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 196608,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/codestral-2508\": {\n\t\t\tid: \"mistralai/codestral-2508\",\n\t\t\tname: \"Mistral: Codestral 2508\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 0.8999999999999999,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/devstral-2512\": {\n\t\t\tid: \"mistralai/devstral-2512\",\n\t\t\tname: \"Mistral: Devstral 2 2512\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.049999999999999996,\n\t\t\t\toutput: 0.22,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/devstral-2512:free\": {\n\t\t\tid: \"mistralai/devstral-2512:free\",\n\t\t\tname: \"Mistral: Devstral 2 2512 (free)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/devstral-medium\": {\n\t\t\tid: \"mistralai/devstral-medium\",\n\t\t\tname: \"Mistral: Devstral Medium\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.39999999999999997,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/devstral-small\": {\n\t\t\tid: \"mistralai/devstral-small\",\n\t\t\tname: \"Mistral: Devstral Small 1.1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/ministral-14b-2512\": {\n\t\t\tid: \"mistralai/ministral-14b-2512\",\n\t\t\tname: \"Mistral: Ministral 3 14B 2512\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 0.19999999999999998,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/ministral-3b\": {\n\t\t\tid: \"mistralai/ministral-3b\",\n\t\t\tname: \"Mistral: Ministral 3B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.04,\n\t\t\t\toutput: 0.04,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/ministral-3b-2512\": {\n\t\t\tid: \"mistralai/ministral-3b-2512\",\n\t\t\tname: \"Mistral: Ministral 3 3B 2512\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.09999999999999999,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/ministral-8b\": {\n\t\t\tid: \"mistralai/ministral-8b\",\n\t\t\tname: \"Mistral: Ministral 8B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.09999999999999999,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/ministral-8b-2512\": {\n\t\t\tid: \"mistralai/ministral-8b-2512\",\n\t\t\tname: \"Mistral: Ministral 3 8B 2512\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/mistral-large\": {\n\t\t\tid: \"mistralai/mistral-large\",\n\t\t\tname: \"Mistral Large\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/mistral-large-2407\": {\n\t\t\tid: \"mistralai/mistral-large-2407\",\n\t\t\tname: \"Mistral Large 2407\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/mistral-large-2411\": {\n\t\t\tid: \"mistralai/mistral-large-2411\",\n\t\t\tname: \"Mistral Large 2411\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/mistral-large-2512\": {\n\t\t\tid: \"mistralai/mistral-large-2512\",\n\t\t\tname: \"Mistral: Mistral Large 3 2512\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 1.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/mistral-medium-3\": {\n\t\t\tid: \"mistralai/mistral-medium-3\",\n\t\t\tname: \"Mistral: Mistral Medium 3\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.39999999999999997,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/mistral-medium-3.1\": {\n\t\t\tid: \"mistralai/mistral-medium-3.1\",\n\t\t\tname: \"Mistral: Mistral Medium 3.1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.39999999999999997,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/mistral-nemo\": {\n\t\t\tid: \"mistralai/mistral-nemo\",\n\t\t\tname: \"Mistral: Mistral Nemo\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.02,\n\t\t\t\toutput: 0.04,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/mistral-saba\": {\n\t\t\tid: \"mistralai/mistral-saba\",\n\t\t\tname: \"Mistral: Saba\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/mistral-small-24b-instruct-2501\": {\n\t\t\tid: \"mistralai/mistral-small-24b-instruct-2501\",\n\t\t\tname: \"Mistral: Mistral Small 3\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.03,\n\t\t\t\toutput: 0.11,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/mistral-small-3.1-24b-instruct\": {\n\t\t\tid: \"mistralai/mistral-small-3.1-24b-instruct\",\n\t\t\tname: \"Mistral: Mistral Small 3.1 24B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.03,\n\t\t\t\toutput: 0.11,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/mistral-small-3.1-24b-instruct:free\": {\n\t\t\tid: \"mistralai/mistral-small-3.1-24b-instruct:free\",\n\t\t\tname: \"Mistral: Mistral Small 3.1 24B (free)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/mistral-small-3.2-24b-instruct\": {\n\t\t\tid: \"mistralai/mistral-small-3.2-24b-instruct\",\n\t\t\tname: \"Mistral: Mistral Small 3.2 24B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.06,\n\t\t\t\toutput: 0.18,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/mistral-small-creative\": {\n\t\t\tid: \"mistralai/mistral-small-creative\",\n\t\t\tname: \"Mistral: Mistral Small Creative\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/mistral-tiny\": {\n\t\t\tid: \"mistralai/mistral-tiny\",\n\t\t\tname: \"Mistral Tiny\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 0.25,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/mixtral-8x22b-instruct\": {\n\t\t\tid: \"mistralai/mixtral-8x22b-instruct\",\n\t\t\tname: \"Mistral: Mixtral 8x22B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 65536,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/mixtral-8x7b-instruct\": {\n\t\t\tid: \"mistralai/mixtral-8x7b-instruct\",\n\t\t\tname: \"Mistral: Mixtral 8x7B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.54,\n\t\t\t\toutput: 0.54,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/pixtral-12b\": {\n\t\t\tid: \"mistralai/pixtral-12b\",\n\t\t\tname: \"Mistral: Pixtral 12B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.09999999999999999,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/pixtral-large-2411\": {\n\t\t\tid: \"mistralai/pixtral-large-2411\",\n\t\t\tname: \"Mistral: Pixtral Large 2411\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"mistralai/voxtral-small-24b-2507\": {\n\t\t\tid: \"mistralai/voxtral-small-24b-2507\",\n\t\t\tname: \"Mistral: Voxtral Small 24B 2507\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"moonshotai/kimi-k2\": {\n\t\t\tid: \"moonshotai/kimi-k2\",\n\t\t\tname: \"MoonshotAI: Kimi K2 0711\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 2.4,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"moonshotai/kimi-k2-0905\": {\n\t\t\tid: \"moonshotai/kimi-k2-0905\",\n\t\t\tname: \"MoonshotAI: Kimi K2 0905\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.39,\n\t\t\t\toutput: 1.9,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 262144,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"moonshotai/kimi-k2-0905:exacto\": {\n\t\t\tid: \"moonshotai/kimi-k2-0905:exacto\",\n\t\t\tname: \"MoonshotAI: Kimi K2 0905 (exacto)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.6,\n\t\t\t\toutput: 2.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"moonshotai/kimi-k2-thinking\": {\n\t\t\tid: \"moonshotai/kimi-k2-thinking\",\n\t\t\tname: \"MoonshotAI: Kimi K2 Thinking\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.39999999999999997,\n\t\t\t\toutput: 1.75,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 65535,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"nex-agi/deepseek-v3.1-nex-n1\": {\n\t\t\tid: \"nex-agi/deepseek-v3.1-nex-n1\",\n\t\t\tname: \"Nex AGI: DeepSeek V3.1 Nex N1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.27,\n\t\t\t\toutput: 1,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 163840,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"nousresearch/deephermes-3-mistral-24b-preview\": {\n\t\t\tid: \"nousresearch/deephermes-3-mistral-24b-preview\",\n\t\t\tname: \"Nous: DeepHermes 3 Mistral 24B Preview\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.02,\n\t\t\t\toutput: 0.09999999999999999,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"nousresearch/hermes-4-70b\": {\n\t\t\tid: \"nousresearch/hermes-4-70b\",\n\t\t\tname: \"Nous: Hermes 4 70B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.11,\n\t\t\t\toutput: 0.38,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"nvidia/llama-3.1-nemotron-70b-instruct\": {\n\t\t\tid: \"nvidia/llama-3.1-nemotron-70b-instruct\",\n\t\t\tname: \"NVIDIA: Llama 3.1 Nemotron 70B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.2,\n\t\t\t\toutput: 1.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"nvidia/llama-3.3-nemotron-super-49b-v1.5\": {\n\t\t\tid: \"nvidia/llama-3.3-nemotron-super-49b-v1.5\",\n\t\t\tname: \"NVIDIA: Llama 3.3 Nemotron Super 49B V1.5\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"nvidia/nemotron-3-nano-30b-a3b\": {\n\t\t\tid: \"nvidia/nemotron-3-nano-30b-a3b\",\n\t\t\tname: \"NVIDIA: Nemotron 3 Nano 30B A3B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.06,\n\t\t\t\toutput: 0.24,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 262144,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"nvidia/nemotron-3-nano-30b-a3b:free\": {\n\t\t\tid: \"nvidia/nemotron-3-nano-30b-a3b:free\",\n\t\t\tname: \"NVIDIA: Nemotron 3 Nano 30B A3B (free)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"nvidia/nemotron-nano-12b-v2-vl:free\": {\n\t\t\tid: \"nvidia/nemotron-nano-12b-v2-vl:free\",\n\t\t\tname: \"NVIDIA: Nemotron Nano 12B 2 VL (free)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"nvidia/nemotron-nano-9b-v2\": {\n\t\t\tid: \"nvidia/nemotron-nano-9b-v2\",\n\t\t\tname: \"NVIDIA: Nemotron Nano 9B V2\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.04,\n\t\t\t\toutput: 0.16,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"nvidia/nemotron-nano-9b-v2:free\": {\n\t\t\tid: \"nvidia/nemotron-nano-9b-v2:free\",\n\t\t\tname: \"NVIDIA: Nemotron Nano 9B V2 (free)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-3.5-turbo\": {\n\t\t\tid: \"openai/gpt-3.5-turbo\",\n\t\t\tname: \"OpenAI: GPT-3.5 Turbo\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 1.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 16385,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-3.5-turbo-0613\": {\n\t\t\tid: \"openai/gpt-3.5-turbo-0613\",\n\t\t\tname: \"OpenAI: GPT-3.5 Turbo (older v0613)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 4095,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-3.5-turbo-16k\": {\n\t\t\tid: \"openai/gpt-3.5-turbo-16k\",\n\t\t\tname: \"OpenAI: GPT-3.5 Turbo 16k\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 4,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 16385,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-4\": {\n\t\t\tid: \"openai/gpt-4\",\n\t\t\tname: \"OpenAI: GPT-4\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 30,\n\t\t\t\toutput: 60,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 8191,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-4-0314\": {\n\t\t\tid: \"openai/gpt-4-0314\",\n\t\t\tname: \"OpenAI: GPT-4 (older v0314)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 30,\n\t\t\t\toutput: 60,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 8191,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-4-1106-preview\": {\n\t\t\tid: \"openai/gpt-4-1106-preview\",\n\t\t\tname: \"OpenAI: GPT-4 Turbo (older v1106)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 10,\n\t\t\t\toutput: 30,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-4-turbo\": {\n\t\t\tid: \"openai/gpt-4-turbo\",\n\t\t\tname: \"OpenAI: GPT-4 Turbo\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 10,\n\t\t\t\toutput: 30,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-4-turbo-preview\": {\n\t\t\tid: \"openai/gpt-4-turbo-preview\",\n\t\t\tname: \"OpenAI: GPT-4 Turbo Preview\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 10,\n\t\t\t\toutput: 30,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-4.1\": {\n\t\t\tid: \"openai/gpt-4.1\",\n\t\t\tname: \"OpenAI: GPT-4.1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 8,\n\t\t\t\tcacheRead: 0.5,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1047576,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-4.1-mini\": {\n\t\t\tid: \"openai/gpt-4.1-mini\",\n\t\t\tname: \"OpenAI: GPT-4.1 Mini\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.39999999999999997,\n\t\t\t\toutput: 1.5999999999999999,\n\t\t\t\tcacheRead: 0.09999999999999999,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1047576,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-4.1-nano\": {\n\t\t\tid: \"openai/gpt-4.1-nano\",\n\t\t\tname: \"OpenAI: GPT-4.1 Nano\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0.024999999999999998,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1047576,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-4o\": {\n\t\t\tid: \"openai/gpt-4o\",\n\t\t\tname: \"OpenAI: GPT-4o\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2.5,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 1.25,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-4o-2024-05-13\": {\n\t\t\tid: \"openai/gpt-4o-2024-05-13\",\n\t\t\tname: \"OpenAI: GPT-4o (2024-05-13)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 5,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-4o-2024-08-06\": {\n\t\t\tid: \"openai/gpt-4o-2024-08-06\",\n\t\t\tname: \"OpenAI: GPT-4o (2024-08-06)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2.5,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 1.25,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-4o-2024-11-20\": {\n\t\t\tid: \"openai/gpt-4o-2024-11-20\",\n\t\t\tname: \"OpenAI: GPT-4o (2024-11-20)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2.5,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 1.25,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-4o-audio-preview\": {\n\t\t\tid: \"openai/gpt-4o-audio-preview\",\n\t\t\tname: \"OpenAI: GPT-4o Audio\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2.5,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-4o-mini\": {\n\t\t\tid: \"openai/gpt-4o-mini\",\n\t\t\tname: \"OpenAI: GPT-4o-mini\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0.075,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-4o-mini-2024-07-18\": {\n\t\t\tid: \"openai/gpt-4o-mini-2024-07-18\",\n\t\t\tname: \"OpenAI: GPT-4o-mini (2024-07-18)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0.075,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-4o:extended\": {\n\t\t\tid: \"openai/gpt-4o:extended\",\n\t\t\tname: \"OpenAI: GPT-4o (extended)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 6,\n\t\t\t\toutput: 18,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-5\": {\n\t\t\tid: \"openai/gpt-5\",\n\t\t\tname: \"OpenAI: GPT-5\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-5-codex\": {\n\t\t\tid: \"openai/gpt-5-codex\",\n\t\t\tname: \"OpenAI: GPT-5 Codex\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-5-image\": {\n\t\t\tid: \"openai/gpt-5-image\",\n\t\t\tname: \"OpenAI: GPT-5 Image\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 10,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 1.25,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-5-image-mini\": {\n\t\t\tid: \"openai/gpt-5-image-mini\",\n\t\t\tname: \"OpenAI: GPT-5 Image Mini\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2.5,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0.25,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-5-mini\": {\n\t\t\tid: \"openai/gpt-5-mini\",\n\t\t\tname: \"OpenAI: GPT-5 Mini\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0.024999999999999998,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-5-nano\": {\n\t\t\tid: \"openai/gpt-5-nano\",\n\t\t\tname: \"OpenAI: GPT-5 Nano\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.049999999999999996,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0.005,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-5-pro\": {\n\t\t\tid: \"openai/gpt-5-pro\",\n\t\t\tname: \"OpenAI: GPT-5 Pro\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 120,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-5.1\": {\n\t\t\tid: \"openai/gpt-5.1\",\n\t\t\tname: \"OpenAI: GPT-5.1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-5.1-chat\": {\n\t\t\tid: \"openai/gpt-5.1-chat\",\n\t\t\tname: \"OpenAI: GPT-5.1 Chat\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-5.1-codex\": {\n\t\t\tid: \"openai/gpt-5.1-codex\",\n\t\t\tname: \"OpenAI: GPT-5.1-Codex\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-5.1-codex-max\": {\n\t\t\tid: \"openai/gpt-5.1-codex-max\",\n\t\t\tname: \"OpenAI: GPT-5.1-Codex-Max\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-5.1-codex-mini\": {\n\t\t\tid: \"openai/gpt-5.1-codex-mini\",\n\t\t\tname: \"OpenAI: GPT-5.1-Codex-Mini\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0.024999999999999998,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-5.2\": {\n\t\t\tid: \"openai/gpt-5.2\",\n\t\t\tname: \"OpenAI: GPT-5.2\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.75,\n\t\t\t\toutput: 14,\n\t\t\t\tcacheRead: 0.175,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-5.2-chat\": {\n\t\t\tid: \"openai/gpt-5.2-chat\",\n\t\t\tname: \"OpenAI: GPT-5.2 Chat\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.75,\n\t\t\t\toutput: 14,\n\t\t\t\tcacheRead: 0.175,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-5.2-codex\": {\n\t\t\tid: \"openai/gpt-5.2-codex\",\n\t\t\tname: \"OpenAI: GPT-5.2-Codex\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.75,\n\t\t\t\toutput: 14,\n\t\t\t\tcacheRead: 0.175,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-5.2-pro\": {\n\t\t\tid: \"openai/gpt-5.2-pro\",\n\t\t\tname: \"OpenAI: GPT-5.2 Pro\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 21,\n\t\t\t\toutput: 168,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-oss-120b\": {\n\t\t\tid: \"openai/gpt-oss-120b\",\n\t\t\tname: \"OpenAI: gpt-oss-120b\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.039,\n\t\t\t\toutput: 0.19,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-oss-120b:exacto\": {\n\t\t\tid: \"openai/gpt-oss-120b:exacto\",\n\t\t\tname: \"OpenAI: gpt-oss-120b (exacto)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.039,\n\t\t\t\toutput: 0.19,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-oss-120b:free\": {\n\t\t\tid: \"openai/gpt-oss-120b:free\",\n\t\t\tname: \"OpenAI: gpt-oss-120b (free)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-oss-20b\": {\n\t\t\tid: \"openai/gpt-oss-20b\",\n\t\t\tname: \"OpenAI: gpt-oss-20b\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.02,\n\t\t\t\toutput: 0.09999999999999999,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-oss-20b:free\": {\n\t\t\tid: \"openai/gpt-oss-20b:free\",\n\t\t\tname: \"OpenAI: gpt-oss-20b (free)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/gpt-oss-safeguard-20b\": {\n\t\t\tid: \"openai/gpt-oss-safeguard-20b\",\n\t\t\tname: \"OpenAI: gpt-oss-safeguard-20b\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.075,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0.037,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/o1\": {\n\t\t\tid: \"openai/o1\",\n\t\t\tname: \"OpenAI: o1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 60,\n\t\t\t\tcacheRead: 7.5,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/o3\": {\n\t\t\tid: \"openai/o3\",\n\t\t\tname: \"OpenAI: o3\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 8,\n\t\t\t\tcacheRead: 0.5,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/o3-deep-research\": {\n\t\t\tid: \"openai/o3-deep-research\",\n\t\t\tname: \"OpenAI: o3 Deep Research\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 10,\n\t\t\t\toutput: 40,\n\t\t\t\tcacheRead: 2.5,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/o3-mini\": {\n\t\t\tid: \"openai/o3-mini\",\n\t\t\tname: \"OpenAI: o3 Mini\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.1,\n\t\t\t\toutput: 4.4,\n\t\t\t\tcacheRead: 0.55,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/o3-mini-high\": {\n\t\t\tid: \"openai/o3-mini-high\",\n\t\t\tname: \"OpenAI: o3 Mini High\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.1,\n\t\t\t\toutput: 4.4,\n\t\t\t\tcacheRead: 0.55,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/o3-pro\": {\n\t\t\tid: \"openai/o3-pro\",\n\t\t\tname: \"OpenAI: o3 Pro\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 20,\n\t\t\t\toutput: 80,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/o4-mini\": {\n\t\t\tid: \"openai/o4-mini\",\n\t\t\tname: \"OpenAI: o4 Mini\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.1,\n\t\t\t\toutput: 4.4,\n\t\t\t\tcacheRead: 0.275,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/o4-mini-deep-research\": {\n\t\t\tid: \"openai/o4-mini-deep-research\",\n\t\t\tname: \"OpenAI: o4 Mini Deep Research\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 8,\n\t\t\t\tcacheRead: 0.5,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openai/o4-mini-high\": {\n\t\t\tid: \"openai/o4-mini-high\",\n\t\t\tname: \"OpenAI: o4 Mini High\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.1,\n\t\t\t\toutput: 4.4,\n\t\t\t\tcacheRead: 0.275,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"openrouter/auto\": {\n\t\t\tid: \"openrouter/auto\",\n\t\t\tname: \"OpenRouter: Auto Router\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 2000000,\n\t\t\tmaxTokens: 30000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"prime-intellect/intellect-3\": {\n\t\t\tid: \"prime-intellect/intellect-3\",\n\t\t\tname: \"Prime Intellect: INTELLECT-3\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 1.1,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen-2.5-72b-instruct\": {\n\t\t\tid: \"qwen/qwen-2.5-72b-instruct\",\n\t\t\tname: \"Qwen2.5 72B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.12,\n\t\t\t\toutput: 0.39,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen-2.5-7b-instruct\": {\n\t\t\tid: \"qwen/qwen-2.5-7b-instruct\",\n\t\t\tname: \"Qwen: Qwen2.5 7B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.04,\n\t\t\t\toutput: 0.09999999999999999,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen-max\": {\n\t\t\tid: \"qwen/qwen-max\",\n\t\t\tname: \"Qwen: Qwen-Max \",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.5999999999999999,\n\t\t\t\toutput: 6.3999999999999995,\n\t\t\t\tcacheRead: 0.64,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen-plus\": {\n\t\t\tid: \"qwen/qwen-plus\",\n\t\t\tname: \"Qwen: Qwen-Plus\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.39999999999999997,\n\t\t\t\toutput: 1.2,\n\t\t\t\tcacheRead: 0.16,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen-plus-2025-07-28\": {\n\t\t\tid: \"qwen/qwen-plus-2025-07-28\",\n\t\t\tname: \"Qwen: Qwen Plus 0728\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.39999999999999997,\n\t\t\t\toutput: 1.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen-plus-2025-07-28:thinking\": {\n\t\t\tid: \"qwen/qwen-plus-2025-07-28:thinking\",\n\t\t\tname: \"Qwen: Qwen Plus 0728 (thinking)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.39999999999999997,\n\t\t\t\toutput: 4,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen-turbo\": {\n\t\t\tid: \"qwen/qwen-turbo\",\n\t\t\tname: \"Qwen: Qwen-Turbo\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.049999999999999996,\n\t\t\t\toutput: 0.19999999999999998,\n\t\t\t\tcacheRead: 0.02,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen-vl-max\": {\n\t\t\tid: \"qwen/qwen-vl-max\",\n\t\t\tname: \"Qwen: Qwen VL Max\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.7999999999999999,\n\t\t\t\toutput: 3.1999999999999997,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen2.5-vl-72b-instruct\": {\n\t\t\tid: \"qwen/qwen2.5-vl-72b-instruct\",\n\t\t\tname: \"Qwen: Qwen2.5 VL 72B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-14b\": {\n\t\t\tid: \"qwen/qwen3-14b\",\n\t\t\tname: \"Qwen: Qwen3 14B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.049999999999999996,\n\t\t\t\toutput: 0.22,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 40960,\n\t\t\tmaxTokens: 40960,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-235b-a22b\": {\n\t\t\tid: \"qwen/qwen3-235b-a22b\",\n\t\t\tname: \"Qwen: Qwen3 235B A22B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 40960,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-235b-a22b-2507\": {\n\t\t\tid: \"qwen/qwen3-235b-a22b-2507\",\n\t\t\tname: \"Qwen: Qwen3 235B A22B Instruct 2507\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.071,\n\t\t\t\toutput: 0.463,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-235b-a22b-thinking-2507\": {\n\t\t\tid: \"qwen/qwen3-235b-a22b-thinking-2507\",\n\t\t\tname: \"Qwen: Qwen3 235B A22B Thinking 2507\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.11,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 262144,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-30b-a3b\": {\n\t\t\tid: \"qwen/qwen3-30b-a3b\",\n\t\t\tname: \"Qwen: Qwen3 30B A3B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.06,\n\t\t\t\toutput: 0.22,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 40960,\n\t\t\tmaxTokens: 40960,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-30b-a3b-instruct-2507\": {\n\t\t\tid: \"qwen/qwen3-30b-a3b-instruct-2507\",\n\t\t\tname: \"Qwen: Qwen3 30B A3B Instruct 2507\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.08,\n\t\t\t\toutput: 0.33,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 262144,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-30b-a3b-thinking-2507\": {\n\t\t\tid: \"qwen/qwen3-30b-a3b-thinking-2507\",\n\t\t\tname: \"Qwen: Qwen3 30B A3B Thinking 2507\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.051,\n\t\t\t\toutput: 0.33999999999999997,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-32b\": {\n\t\t\tid: \"qwen/qwen3-32b\",\n\t\t\tname: \"Qwen: Qwen3 32B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.08,\n\t\t\t\toutput: 0.24,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 40960,\n\t\t\tmaxTokens: 40960,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-4b:free\": {\n\t\t\tid: \"qwen/qwen3-4b:free\",\n\t\t\tname: \"Qwen: Qwen3 4B (free)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 40960,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-8b\": {\n\t\t\tid: \"qwen/qwen3-8b\",\n\t\t\tname: \"Qwen: Qwen3 8B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.049999999999999996,\n\t\t\t\toutput: 0.25,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-coder\": {\n\t\t\tid: \"qwen/qwen3-coder\",\n\t\t\tname: \"Qwen: Qwen3 Coder 480B A35B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.22,\n\t\t\t\toutput: 0.95,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 262144,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-coder-30b-a3b-instruct\": {\n\t\t\tid: \"qwen/qwen3-coder-30b-a3b-instruct\",\n\t\t\tname: \"Qwen: Qwen3 Coder 30B A3B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.07,\n\t\t\t\toutput: 0.27,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 160000,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-coder-flash\": {\n\t\t\tid: \"qwen/qwen3-coder-flash\",\n\t\t\tname: \"Qwen: Qwen3 Coder Flash\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 1.5,\n\t\t\t\tcacheRead: 0.08,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-coder-plus\": {\n\t\t\tid: \"qwen/qwen3-coder-plus\",\n\t\t\tname: \"Qwen: Qwen3 Coder Plus\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1,\n\t\t\t\toutput: 5,\n\t\t\t\tcacheRead: 0.09999999999999999,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-coder:exacto\": {\n\t\t\tid: \"qwen/qwen3-coder:exacto\",\n\t\t\tname: \"Qwen: Qwen3 Coder 480B A35B (exacto)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.22,\n\t\t\t\toutput: 1.7999999999999998,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-coder:free\": {\n\t\t\tid: \"qwen/qwen3-coder:free\",\n\t\t\tname: \"Qwen: Qwen3 Coder 480B A35B (free)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262000,\n\t\t\tmaxTokens: 262000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-max\": {\n\t\t\tid: \"qwen/qwen3-max\",\n\t\t\tname: \"Qwen: Qwen3 Max\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.2,\n\t\t\t\toutput: 6,\n\t\t\t\tcacheRead: 0.24,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-next-80b-a3b-instruct\": {\n\t\t\tid: \"qwen/qwen3-next-80b-a3b-instruct\",\n\t\t\tname: \"Qwen: Qwen3 Next 80B A3B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09,\n\t\t\t\toutput: 1.1,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-next-80b-a3b-instruct:free\": {\n\t\t\tid: \"qwen/qwen3-next-80b-a3b-instruct:free\",\n\t\t\tname: \"Qwen: Qwen3 Next 80B A3B Instruct (free)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-next-80b-a3b-thinking\": {\n\t\t\tid: \"qwen/qwen3-next-80b-a3b-thinking\",\n\t\t\tname: \"Qwen: Qwen3 Next 80B A3B Thinking\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 1.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-vl-235b-a22b-instruct\": {\n\t\t\tid: \"qwen/qwen3-vl-235b-a22b-instruct\",\n\t\t\tname: \"Qwen: Qwen3 VL 235B A22B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 1.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-vl-235b-a22b-thinking\": {\n\t\t\tid: \"qwen/qwen3-vl-235b-a22b-thinking\",\n\t\t\tname: \"Qwen: Qwen3 VL 235B A22B Thinking\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.44999999999999996,\n\t\t\t\toutput: 3.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 262144,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-vl-30b-a3b-instruct\": {\n\t\t\tid: \"qwen/qwen3-vl-30b-a3b-instruct\",\n\t\t\tname: \"Qwen: Qwen3 VL 30B A3B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-vl-30b-a3b-thinking\": {\n\t\t\tid: \"qwen/qwen3-vl-30b-a3b-thinking\",\n\t\t\tname: \"Qwen: Qwen3 VL 30B A3B Thinking\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 1,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-vl-8b-instruct\": {\n\t\t\tid: \"qwen/qwen3-vl-8b-instruct\",\n\t\t\tname: \"Qwen: Qwen3 VL 8B Instruct\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.08,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwen3-vl-8b-thinking\": {\n\t\t\tid: \"qwen/qwen3-vl-8b-thinking\",\n\t\t\tname: \"Qwen: Qwen3 VL 8B Thinking\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.18,\n\t\t\t\toutput: 2.0999999999999996,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"qwen/qwq-32b\": {\n\t\t\tid: \"qwen/qwq-32b\",\n\t\t\tname: \"Qwen: QwQ 32B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"relace/relace-search\": {\n\t\t\tid: \"relace/relace-search\",\n\t\t\tname: \"Relace: Relace Search\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1,\n\t\t\t\toutput: 3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"sao10k/l3-euryale-70b\": {\n\t\t\tid: \"sao10k/l3-euryale-70b\",\n\t\t\tname: \"Sao10k: Llama 3 Euryale 70B v2.1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.48,\n\t\t\t\toutput: 1.48,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 8192,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"sao10k/l3.1-euryale-70b\": {\n\t\t\tid: \"sao10k/l3.1-euryale-70b\",\n\t\t\tname: \"Sao10K: Llama 3.1 Euryale 70B v2.2\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.65,\n\t\t\t\toutput: 0.75,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"stepfun-ai/step3\": {\n\t\t\tid: \"stepfun-ai/step3\",\n\t\t\tname: \"StepFun: Step3\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5700000000000001,\n\t\t\t\toutput: 1.42,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 65536,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"thedrummer/rocinante-12b\": {\n\t\t\tid: \"thedrummer/rocinante-12b\",\n\t\t\tname: \"TheDrummer: Rocinante 12B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.16999999999999998,\n\t\t\t\toutput: 0.43,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"thedrummer/unslopnemo-12b\": {\n\t\t\tid: \"thedrummer/unslopnemo-12b\",\n\t\t\tname: \"TheDrummer: UnslopNemo 12B\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.39999999999999997,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"tngtech/deepseek-r1t2-chimera\": {\n\t\t\tid: \"tngtech/deepseek-r1t2-chimera\",\n\t\t\tname: \"TNG: DeepSeek R1T2 Chimera\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 0.85,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 163840,\n\t\t\tmaxTokens: 163840,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"tngtech/tng-r1t-chimera\": {\n\t\t\tid: \"tngtech/tng-r1t-chimera\",\n\t\t\tname: \"TNG: R1T Chimera\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 0.85,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 163840,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"tngtech/tng-r1t-chimera:free\": {\n\t\t\tid: \"tngtech/tng-r1t-chimera:free\",\n\t\t\tname: \"TNG: R1T Chimera (free)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 163840,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"x-ai/grok-3\": {\n\t\t\tid: \"x-ai/grok-3\",\n\t\t\tname: \"xAI: Grok 3\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.75,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"x-ai/grok-3-beta\": {\n\t\t\tid: \"x-ai/grok-3-beta\",\n\t\t\tname: \"xAI: Grok 3 Beta\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.75,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"x-ai/grok-3-mini\": {\n\t\t\tid: \"x-ai/grok-3-mini\",\n\t\t\tname: \"xAI: Grok 3 Mini\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0.075,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"x-ai/grok-3-mini-beta\": {\n\t\t\tid: \"x-ai/grok-3-mini-beta\",\n\t\t\tname: \"xAI: Grok 3 Mini Beta\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0.075,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"x-ai/grok-4\": {\n\t\t\tid: \"x-ai/grok-4\",\n\t\t\tname: \"xAI: Grok 4\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.75,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"x-ai/grok-4-fast\": {\n\t\t\tid: \"x-ai/grok-4-fast\",\n\t\t\tname: \"xAI: Grok 4 Fast\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0.049999999999999996,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 2000000,\n\t\t\tmaxTokens: 30000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"x-ai/grok-4.1-fast\": {\n\t\t\tid: \"x-ai/grok-4.1-fast\",\n\t\t\tname: \"xAI: Grok 4.1 Fast\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0.049999999999999996,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 2000000,\n\t\t\tmaxTokens: 30000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"x-ai/grok-code-fast-1\": {\n\t\t\tid: \"x-ai/grok-code-fast-1\",\n\t\t\tname: \"xAI: Grok Code Fast 1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 1.5,\n\t\t\t\tcacheRead: 0.02,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 10000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"xiaomi/mimo-v2-flash\": {\n\t\t\tid: \"xiaomi/mimo-v2-flash\",\n\t\t\tname: \"Xiaomi: MiMo-V2-Flash\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09,\n\t\t\t\toutput: 0.29,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"xiaomi/mimo-v2-flash:free\": {\n\t\t\tid: \"xiaomi/mimo-v2-flash:free\",\n\t\t\tname: \"Xiaomi: MiMo-V2-Flash (free)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"z-ai/glm-4-32b\": {\n\t\t\tid: \"z-ai/glm-4-32b\",\n\t\t\tname: \"Z.AI: GLM 4 32B \",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.09999999999999999,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"z-ai/glm-4.5\": {\n\t\t\tid: \"z-ai/glm-4.5\",\n\t\t\tname: \"Z.AI: GLM 4.5\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.35,\n\t\t\t\toutput: 1.55,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"z-ai/glm-4.5-air\": {\n\t\t\tid: \"z-ai/glm-4.5-air\",\n\t\t\tname: \"Z.AI: GLM 4.5 Air\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.049999999999999996,\n\t\t\t\toutput: 0.22,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"z-ai/glm-4.5-air:free\": {\n\t\t\tid: \"z-ai/glm-4.5-air:free\",\n\t\t\tname: \"Z.AI: GLM 4.5 Air (free)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 96000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"z-ai/glm-4.5v\": {\n\t\t\tid: \"z-ai/glm-4.5v\",\n\t\t\tname: \"Z.AI: GLM 4.5V\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.6,\n\t\t\t\toutput: 1.7999999999999998,\n\t\t\t\tcacheRead: 0.11,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 65536,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"z-ai/glm-4.6\": {\n\t\t\tid: \"z-ai/glm-4.6\",\n\t\t\tname: \"Z.AI: GLM 4.6\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.35,\n\t\t\t\toutput: 1.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 202752,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"z-ai/glm-4.6:exacto\": {\n\t\t\tid: \"z-ai/glm-4.6:exacto\",\n\t\t\tname: \"Z.AI: GLM 4.6 (exacto)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.44,\n\t\t\t\toutput: 1.76,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 204800,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"z-ai/glm-4.6v\": {\n\t\t\tid: \"z-ai/glm-4.6v\",\n\t\t\tname: \"Z.AI: GLM 4.6V\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 0.8999999999999999,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"z-ai/glm-4.7\": {\n\t\t\tid: \"z-ai/glm-4.7\",\n\t\t\tname: \"Z.AI: GLM 4.7\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.39999999999999997,\n\t\t\t\toutput: 1.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 202752,\n\t\t\tmaxTokens: 65535,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"z-ai/glm-4.7-flash\": {\n\t\t\tid: \"z-ai/glm-4.7-flash\",\n\t\t\tname: \"Z.AI: GLM 4.7 Flash\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"openrouter\",\n\t\t\tbaseUrl: \"https://openrouter.ai/api/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.07,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0.01,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t},\n\t\"vercel-ai-gateway\": {\n\t\t\"alibaba/qwen-3-14b\": {\n\t\t\tid: \"alibaba/qwen-3-14b\",\n\t\t\tname: \"Qwen3-14B\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.06,\n\t\t\t\toutput: 0.24,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 40960,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"alibaba/qwen-3-235b\": {\n\t\t\tid: \"alibaba/qwen-3-235b\",\n\t\t\tname: \"Qwen3 235B A22b Instruct 2507\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.071,\n\t\t\t\toutput: 0.463,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 40960,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"alibaba/qwen-3-30b\": {\n\t\t\tid: \"alibaba/qwen-3-30b\",\n\t\t\tname: \"Qwen3-30B-A3B\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.08,\n\t\t\t\toutput: 0.29,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 40960,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"alibaba/qwen-3-32b\": {\n\t\t\tid: \"alibaba/qwen-3-32b\",\n\t\t\tname: \"Qwen 3.32B\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 40960,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"alibaba/qwen3-235b-a22b-thinking\": {\n\t\t\tid: \"alibaba/qwen3-235b-a22b-thinking\",\n\t\t\tname: \"Qwen3 235B A22B Thinking 2507\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 2.9000000000000004,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262114,\n\t\t\tmaxTokens: 262114,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"alibaba/qwen3-coder\": {\n\t\t\tid: \"alibaba/qwen3-coder\",\n\t\t\tname: \"Qwen3 Coder 480B A35B Instruct\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.38,\n\t\t\t\toutput: 1.53,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 66536,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"alibaba/qwen3-coder-30b-a3b\": {\n\t\t\tid: \"alibaba/qwen3-coder-30b-a3b\",\n\t\t\tname: \"Qwen 3 Coder 30B A3B Instruct\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.07,\n\t\t\t\toutput: 0.27,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 160000,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"alibaba/qwen3-coder-plus\": {\n\t\t\tid: \"alibaba/qwen3-coder-plus\",\n\t\t\tname: \"Qwen3 Coder Plus\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1,\n\t\t\t\toutput: 5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"alibaba/qwen3-max\": {\n\t\t\tid: \"alibaba/qwen3-max\",\n\t\t\tname: \"Qwen3 Max\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.2,\n\t\t\t\toutput: 6,\n\t\t\t\tcacheRead: 0.24,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"alibaba/qwen3-max-preview\": {\n\t\t\tid: \"alibaba/qwen3-max-preview\",\n\t\t\tname: \"Qwen3 Max Preview\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.2,\n\t\t\t\toutput: 6,\n\t\t\t\tcacheRead: 0.24,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"anthropic/claude-3-haiku\": {\n\t\t\tid: \"anthropic/claude-3-haiku\",\n\t\t\tname: \"Claude 3 Haiku\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 1.25,\n\t\t\t\tcacheRead: 0.03,\n\t\t\t\tcacheWrite: 0.3,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"anthropic/claude-3.5-haiku\": {\n\t\t\tid: \"anthropic/claude-3.5-haiku\",\n\t\t\tname: \"Claude 3.5 Haiku\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.7999999999999999,\n\t\t\t\toutput: 4,\n\t\t\t\tcacheRead: 0.08,\n\t\t\t\tcacheWrite: 1,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"anthropic/claude-3.5-sonnet\": {\n\t\t\tid: \"anthropic/claude-3.5-sonnet\",\n\t\t\tname: \"Claude 3.5 Sonnet\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"anthropic/claude-3.5-sonnet-20240620\": {\n\t\t\tid: \"anthropic/claude-3.5-sonnet-20240620\",\n\t\t\tname: \"Claude 3.5 Sonnet (2024-06-20)\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"anthropic/claude-3.7-sonnet\": {\n\t\t\tid: \"anthropic/claude-3.7-sonnet\",\n\t\t\tname: \"Claude 3.7 Sonnet\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"anthropic/claude-haiku-4.5\": {\n\t\t\tid: \"anthropic/claude-haiku-4.5\",\n\t\t\tname: \"Claude Haiku 4.5\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1,\n\t\t\t\toutput: 5,\n\t\t\t\tcacheRead: 0.09999999999999999,\n\t\t\t\tcacheWrite: 1.25,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"anthropic/claude-opus-4\": {\n\t\t\tid: \"anthropic/claude-opus-4\",\n\t\t\tname: \"Claude Opus 4\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 75,\n\t\t\t\tcacheRead: 1.5,\n\t\t\t\tcacheWrite: 18.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 32000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"anthropic/claude-opus-4.1\": {\n\t\t\tid: \"anthropic/claude-opus-4.1\",\n\t\t\tname: \"Claude Opus 4.1\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 75,\n\t\t\t\tcacheRead: 1.5,\n\t\t\t\tcacheWrite: 18.75,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 32000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"anthropic/claude-opus-4.5\": {\n\t\t\tid: \"anthropic/claude-opus-4.5\",\n\t\t\tname: \"Claude Opus 4.5\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 5,\n\t\t\t\toutput: 25,\n\t\t\t\tcacheRead: 0.5,\n\t\t\t\tcacheWrite: 6.25,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"anthropic/claude-sonnet-4\": {\n\t\t\tid: \"anthropic/claude-sonnet-4\",\n\t\t\tname: \"Claude Sonnet 4\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"anthropic/claude-sonnet-4.5\": {\n\t\t\tid: \"anthropic/claude-sonnet-4.5\",\n\t\t\tname: \"Claude Sonnet 4.5\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.3,\n\t\t\t\tcacheWrite: 3.75,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"bytedance/seed-1.6\": {\n\t\t\tid: \"bytedance/seed-1.6\",\n\t\t\tname: \"Seed 1.6\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0.049999999999999996,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 32000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"cohere/command-a\": {\n\t\t\tid: \"cohere/command-a\",\n\t\t\tname: \"Command A\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2.5,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 8000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"deepseek/deepseek-v3\": {\n\t\t\tid: \"deepseek/deepseek-v3\",\n\t\t\tname: \"DeepSeek V3 0324\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.77,\n\t\t\t\toutput: 0.77,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 163840,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"deepseek/deepseek-v3.1\": {\n\t\t\tid: \"deepseek/deepseek-v3.1\",\n\t\t\tname: \"DeepSeek-V3.1\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 1,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 163840,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"deepseek/deepseek-v3.1-terminus\": {\n\t\t\tid: \"deepseek/deepseek-v3.1-terminus\",\n\t\t\tname: \"DeepSeek V3.1 Terminus\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.27,\n\t\t\t\toutput: 1,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"deepseek/deepseek-v3.2-exp\": {\n\t\t\tid: \"deepseek/deepseek-v3.2-exp\",\n\t\t\tname: \"DeepSeek V3.2 Exp\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.27,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 163840,\n\t\t\tmaxTokens: 163840,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"deepseek/deepseek-v3.2-thinking\": {\n\t\t\tid: \"deepseek/deepseek-v3.2-thinking\",\n\t\t\tname: \"DeepSeek V3.2 Thinking\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.28,\n\t\t\t\toutput: 0.42,\n\t\t\t\tcacheRead: 0.028,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"google/gemini-2.5-flash\": {\n\t\t\tid: \"google/gemini-2.5-flash\",\n\t\t\tname: \"Gemini 2.5 Flash\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 2.5,\n\t\t\t\tcacheRead: 0.03,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"google/gemini-2.5-flash-lite\": {\n\t\t\tid: \"google/gemini-2.5-flash-lite\",\n\t\t\tname: \"Gemini 2.5 Flash Lite\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0.01,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"google/gemini-2.5-flash-lite-preview-09-2025\": {\n\t\t\tid: \"google/gemini-2.5-flash-lite-preview-09-2025\",\n\t\t\tname: \"Gemini 2.5 Flash Lite Preview 09-2025\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0.01,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"google/gemini-2.5-flash-preview-09-2025\": {\n\t\t\tid: \"google/gemini-2.5-flash-preview-09-2025\",\n\t\t\tname: \"Gemini 2.5 Flash Preview 09-2025\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 2.5,\n\t\t\t\tcacheRead: 0.03,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"google/gemini-2.5-pro\": {\n\t\t\tid: \"google/gemini-2.5-pro\",\n\t\t\tname: \"Gemini 2.5 Pro\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1048576,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"google/gemini-3-flash\": {\n\t\t\tid: \"google/gemini-3-flash\",\n\t\t\tname: \"Gemini 3 Flash\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 3,\n\t\t\t\tcacheRead: 0.049999999999999996,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"google/gemini-3-pro-preview\": {\n\t\t\tid: \"google/gemini-3-pro-preview\",\n\t\t\tname: \"Gemini 3 Pro Preview\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 12,\n\t\t\t\tcacheRead: 0.19999999999999998,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1000000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"inception/mercury-coder-small\": {\n\t\t\tid: \"inception/mercury-coder-small\",\n\t\t\tname: \"Mercury Coder Small Beta\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 1,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"meituan/longcat-flash-chat\": {\n\t\t\tid: \"meituan/longcat-flash-chat\",\n\t\t\tname: \"LongCat Flash Chat\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"meituan/longcat-flash-thinking\": {\n\t\t\tid: \"meituan/longcat-flash-thinking\",\n\t\t\tname: \"LongCat Flash Thinking\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 1.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"meta/llama-3.1-70b\": {\n\t\t\tid: \"meta/llama-3.1-70b\",\n\t\t\tname: \"Llama 3.1 70B Instruct\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.39999999999999997,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"meta/llama-3.1-8b\": {\n\t\t\tid: \"meta/llama-3.1-8b\",\n\t\t\tname: \"Llama 3.1 8B Instruct\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.03,\n\t\t\t\toutput: 0.049999999999999996,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"meta/llama-3.2-11b\": {\n\t\t\tid: \"meta/llama-3.2-11b\",\n\t\t\tname: \"Llama 3.2 11B Vision Instruct\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.16,\n\t\t\t\toutput: 0.16,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"meta/llama-3.2-90b\": {\n\t\t\tid: \"meta/llama-3.2-90b\",\n\t\t\tname: \"Llama 3.2 90B Vision Instruct\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.72,\n\t\t\t\toutput: 0.72,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"meta/llama-3.3-70b\": {\n\t\t\tid: \"meta/llama-3.3-70b\",\n\t\t\tname: \"Llama 3.3 70B Instruct\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.72,\n\t\t\t\toutput: 0.72,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"meta/llama-4-maverick\": {\n\t\t\tid: \"meta/llama-4-maverick\",\n\t\t\tname: \"Llama 4 Maverick 17B Instruct\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"meta/llama-4-scout\": {\n\t\t\tid: \"meta/llama-4-scout\",\n\t\t\tname: \"Llama 4 Scout 17B Instruct\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.08,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"minimax/minimax-m2\": {\n\t\t\tid: \"minimax/minimax-m2\",\n\t\t\tname: \"MiniMax M2\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.27,\n\t\t\t\toutput: 1.15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262114,\n\t\t\tmaxTokens: 262114,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"minimax/minimax-m2.1\": {\n\t\t\tid: \"minimax/minimax-m2.1\",\n\t\t\tname: \"MiniMax M2.1\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.28,\n\t\t\t\toutput: 1.2,\n\t\t\t\tcacheRead: 0.14,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 196608,\n\t\t\tmaxTokens: 196608,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"minimax/minimax-m2.1-lightning\": {\n\t\t\tid: \"minimax/minimax-m2.1-lightning\",\n\t\t\tname: \"MiniMax M2.1 Lightning\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 2.4,\n\t\t\t\tcacheRead: 0.03,\n\t\t\t\tcacheWrite: 0.375,\n\t\t\t},\n\t\t\tcontextWindow: 204800,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"mistral/codestral\": {\n\t\t\tid: \"mistral/codestral\",\n\t\t\tname: \"Mistral Codestral\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 0.8999999999999999,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"mistral/devstral-2\": {\n\t\t\tid: \"mistral/devstral-2\",\n\t\t\tname: \"Devstral 2\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 256000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"mistral/devstral-small\": {\n\t\t\tid: \"mistral/devstral-small\",\n\t\t\tname: \"Devstral Small 1.1\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"mistral/devstral-small-2\": {\n\t\t\tid: \"mistral/devstral-small-2\",\n\t\t\tname: \"Devstral Small 2\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 256000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"mistral/ministral-3b\": {\n\t\t\tid: \"mistral/ministral-3b\",\n\t\t\tname: \"Ministral 3B\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.04,\n\t\t\t\toutput: 0.04,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"mistral/ministral-8b\": {\n\t\t\tid: \"mistral/ministral-8b\",\n\t\t\tname: \"Ministral 8B\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.09999999999999999,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"mistral/mistral-medium\": {\n\t\t\tid: \"mistral/mistral-medium\",\n\t\t\tname: \"Mistral Medium 3.1\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.39999999999999997,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"mistral/mistral-small\": {\n\t\t\tid: \"mistral/mistral-small\",\n\t\t\tname: \"Mistral Small\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32000,\n\t\t\tmaxTokens: 4000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"mistral/pixtral-12b\": {\n\t\t\tid: \"mistral/pixtral-12b\",\n\t\t\tname: \"Pixtral 12B 2409\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"mistral/pixtral-large\": {\n\t\t\tid: \"mistral/pixtral-large\",\n\t\t\tname: \"Pixtral Large\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"moonshotai/kimi-k2\": {\n\t\t\tid: \"moonshotai/kimi-k2\",\n\t\t\tname: \"Kimi K2\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.5,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"moonshotai/kimi-k2-thinking\": {\n\t\t\tid: \"moonshotai/kimi-k2-thinking\",\n\t\t\tname: \"Kimi K2 Thinking\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.47,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0.14100000000000001,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 216144,\n\t\t\tmaxTokens: 216144,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"moonshotai/kimi-k2-thinking-turbo\": {\n\t\t\tid: \"moonshotai/kimi-k2-thinking-turbo\",\n\t\t\tname: \"Kimi K2 Thinking Turbo\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.15,\n\t\t\t\toutput: 8,\n\t\t\t\tcacheRead: 0.15,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262114,\n\t\t\tmaxTokens: 262114,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"moonshotai/kimi-k2-turbo\": {\n\t\t\tid: \"moonshotai/kimi-k2-turbo\",\n\t\t\tname: \"Kimi K2 Turbo\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2.4,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"nvidia/nemotron-nano-12b-v2-vl\": {\n\t\t\tid: \"nvidia/nemotron-nano-12b-v2-vl\",\n\t\t\tname: \"Nvidia Nemotron Nano 12B V2 VL\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"nvidia/nemotron-nano-9b-v2\": {\n\t\t\tid: \"nvidia/nemotron-nano-9b-v2\",\n\t\t\tname: \"Nvidia Nemotron Nano 9B V2\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.04,\n\t\t\t\toutput: 0.16,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/codex-mini\": {\n\t\t\tid: \"openai/codex-mini\",\n\t\t\tname: \"Codex Mini\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.5,\n\t\t\t\toutput: 6,\n\t\t\t\tcacheRead: 0.375,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-4-turbo\": {\n\t\t\tid: \"openai/gpt-4-turbo\",\n\t\t\tname: \"GPT-4 Turbo\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 10,\n\t\t\t\toutput: 30,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-4.1\": {\n\t\t\tid: \"openai/gpt-4.1\",\n\t\t\tname: \"GPT-4.1\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 8,\n\t\t\t\tcacheRead: 0.5,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1047576,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-4.1-mini\": {\n\t\t\tid: \"openai/gpt-4.1-mini\",\n\t\t\tname: \"GPT-4.1 mini\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.39999999999999997,\n\t\t\t\toutput: 1.5999999999999999,\n\t\t\t\tcacheRead: 0.09999999999999999,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1047576,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-4.1-nano\": {\n\t\t\tid: \"openai/gpt-4.1-nano\",\n\t\t\tname: \"GPT-4.1 nano\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0.024999999999999998,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 1047576,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-4o\": {\n\t\t\tid: \"openai/gpt-4o\",\n\t\t\tname: \"GPT-4o\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2.5,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 1.25,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-4o-mini\": {\n\t\t\tid: \"openai/gpt-4o-mini\",\n\t\t\tname: \"GPT-4o mini\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.15,\n\t\t\t\toutput: 0.6,\n\t\t\t\tcacheRead: 0.075,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-5\": {\n\t\t\tid: \"openai/gpt-5\",\n\t\t\tname: \"GPT-5\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.13,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-5-chat\": {\n\t\t\tid: \"openai/gpt-5-chat\",\n\t\t\tname: \"GPT-5 Chat\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-5-codex\": {\n\t\t\tid: \"openai/gpt-5-codex\",\n\t\t\tname: \"GPT-5-Codex\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.13,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-5-mini\": {\n\t\t\tid: \"openai/gpt-5-mini\",\n\t\t\tname: \"GPT-5 mini\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0.03,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-5-nano\": {\n\t\t\tid: \"openai/gpt-5-nano\",\n\t\t\tname: \"GPT-5 nano\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.049999999999999996,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0.01,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-5-pro\": {\n\t\t\tid: \"openai/gpt-5-pro\",\n\t\t\tname: \"GPT-5 pro\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 120,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 272000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-5.1-codex\": {\n\t\t\tid: \"openai/gpt-5.1-codex\",\n\t\t\tname: \"GPT-5.1-Codex\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-5.1-codex-max\": {\n\t\t\tid: \"openai/gpt-5.1-codex-max\",\n\t\t\tname: \"GPT 5.1 Codex Max\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-5.1-codex-mini\": {\n\t\t\tid: \"openai/gpt-5.1-codex-mini\",\n\t\t\tname: \"GPT-5.1 Codex mini\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.25,\n\t\t\t\toutput: 2,\n\t\t\t\tcacheRead: 0.024999999999999998,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-5.1-instant\": {\n\t\t\tid: \"openai/gpt-5.1-instant\",\n\t\t\tname: \"GPT-5.1 Instant\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-5.1-thinking\": {\n\t\t\tid: \"openai/gpt-5.1-thinking\",\n\t\t\tname: \"GPT 5.1 Thinking\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.25,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0.125,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-5.2\": {\n\t\t\tid: \"openai/gpt-5.2\",\n\t\t\tname: \"GPT-5.2\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.75,\n\t\t\t\toutput: 14,\n\t\t\t\tcacheRead: 0.175,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-5.2-chat\": {\n\t\t\tid: \"openai/gpt-5.2-chat\",\n\t\t\tname: \"GPT-5.2 Chat\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.75,\n\t\t\t\toutput: 14,\n\t\t\t\tcacheRead: 0.175,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-5.2-codex\": {\n\t\t\tid: \"openai/gpt-5.2-codex\",\n\t\t\tname: \"GPT-5.2-Codex\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.75,\n\t\t\t\toutput: 14,\n\t\t\t\tcacheRead: 0.175,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-5.2-pro\": {\n\t\t\tid: \"openai/gpt-5.2-pro\",\n\t\t\tname: \"GPT 5.2 \",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 21,\n\t\t\t\toutput: 168,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 400000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-oss-120b\": {\n\t\t\tid: \"openai/gpt-oss-120b\",\n\t\t\tname: \"gpt-oss-120b\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09999999999999999,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-oss-20b\": {\n\t\t\tid: \"openai/gpt-oss-20b\",\n\t\t\tname: \"gpt-oss-20b\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.07,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/gpt-oss-safeguard-20b\": {\n\t\t\tid: \"openai/gpt-oss-safeguard-20b\",\n\t\t\tname: \"gpt-oss-safeguard-20b\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.075,\n\t\t\t\toutput: 0.3,\n\t\t\t\tcacheRead: 0.037,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 65536,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/o1\": {\n\t\t\tid: \"openai/o1\",\n\t\t\tname: \"o1\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 15,\n\t\t\t\toutput: 60,\n\t\t\t\tcacheRead: 7.5,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/o3\": {\n\t\t\tid: \"openai/o3\",\n\t\t\tname: \"o3\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 8,\n\t\t\t\tcacheRead: 0.5,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/o3-deep-research\": {\n\t\t\tid: \"openai/o3-deep-research\",\n\t\t\tname: \"o3-deep-research\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 10,\n\t\t\t\toutput: 40,\n\t\t\t\tcacheRead: 2.5,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/o3-mini\": {\n\t\t\tid: \"openai/o3-mini\",\n\t\t\tname: \"o3-mini\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.1,\n\t\t\t\toutput: 4.4,\n\t\t\t\tcacheRead: 0.55,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/o3-pro\": {\n\t\t\tid: \"openai/o3-pro\",\n\t\t\tname: \"o3 Pro\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 20,\n\t\t\t\toutput: 80,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"openai/o4-mini\": {\n\t\t\tid: \"openai/o4-mini\",\n\t\t\tname: \"o4-mini\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1.1,\n\t\t\t\toutput: 4.4,\n\t\t\t\tcacheRead: 0.275,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 100000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"perplexity/sonar\": {\n\t\t\tid: \"perplexity/sonar\",\n\t\t\tname: \"Sonar\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 1,\n\t\t\t\toutput: 1,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 127000,\n\t\t\tmaxTokens: 8000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"perplexity/sonar-pro\": {\n\t\t\tid: \"perplexity/sonar-pro\",\n\t\t\tname: \"Sonar Pro\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 8000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"prime-intellect/intellect-3\": {\n\t\t\tid: \"prime-intellect/intellect-3\",\n\t\t\tname: \"INTELLECT 3\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 1.1,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"stealth/sonoma-dusk-alpha\": {\n\t\t\tid: \"stealth/sonoma-dusk-alpha\",\n\t\t\tname: \"Sonoma Dusk Alpha\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0.049999999999999996,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 2000000,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"stealth/sonoma-sky-alpha\": {\n\t\t\tid: \"stealth/sonoma-sky-alpha\",\n\t\t\tname: \"Sonoma Sky Alpha\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0.049999999999999996,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 2000000,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"vercel/v0-1.0-md\": {\n\t\t\tid: \"vercel/v0-1.0-md\",\n\t\t\tname: \"v0-1.0-md\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 32000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"vercel/v0-1.5-md\": {\n\t\t\tid: \"vercel/v0-1.5-md\",\n\t\t\tname: \"v0-1.5-md\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"xai/grok-2-vision\": {\n\t\t\tid: \"xai/grok-2-vision\",\n\t\t\tname: \"Grok 2 Vision\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 32768,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"xai/grok-3\": {\n\t\t\tid: \"xai/grok-3\",\n\t\t\tname: \"Grok 3 Beta\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"xai/grok-3-fast\": {\n\t\t\tid: \"xai/grok-3-fast\",\n\t\t\tname: \"Grok 3 Fast Beta\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 5,\n\t\t\t\toutput: 25,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"xai/grok-3-mini\": {\n\t\t\tid: \"xai/grok-3-mini\",\n\t\t\tname: \"Grok 3 Mini Beta\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"xai/grok-3-mini-fast\": {\n\t\t\tid: \"xai/grok-3-mini-fast\",\n\t\t\tname: \"Grok 3 Mini Fast Beta\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.6,\n\t\t\t\toutput: 4,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"xai/grok-4\": {\n\t\t\tid: \"xai/grok-4\",\n\t\t\tname: \"Grok 4\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 256000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"xai/grok-4-fast-non-reasoning\": {\n\t\t\tid: \"xai/grok-4-fast-non-reasoning\",\n\t\t\tname: \"Grok 4 Fast Non-Reasoning\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0.049999999999999996,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 2000000,\n\t\t\tmaxTokens: 256000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"xai/grok-4-fast-reasoning\": {\n\t\t\tid: \"xai/grok-4-fast-reasoning\",\n\t\t\tname: \"Grok 4 Fast Reasoning\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0.049999999999999996,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 2000000,\n\t\t\tmaxTokens: 256000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"xai/grok-4.1-fast-non-reasoning\": {\n\t\t\tid: \"xai/grok-4.1-fast-non-reasoning\",\n\t\t\tname: \"Grok 4.1 Fast Non-Reasoning\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0.049999999999999996,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 2000000,\n\t\t\tmaxTokens: 30000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"xai/grok-4.1-fast-reasoning\": {\n\t\t\tid: \"xai/grok-4.1-fast-reasoning\",\n\t\t\tname: \"Grok 4.1 Fast Reasoning\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0.049999999999999996,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 2000000,\n\t\t\tmaxTokens: 30000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"xai/grok-code-fast-1\": {\n\t\t\tid: \"xai/grok-code-fast-1\",\n\t\t\tname: \"Grok Code Fast 1\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 1.5,\n\t\t\t\tcacheRead: 0.02,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 256000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"xiaomi/mimo-v2-flash\": {\n\t\t\tid: \"xiaomi/mimo-v2-flash\",\n\t\t\tname: \"MiMo V2 Flash\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.09,\n\t\t\t\toutput: 0.29,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 262144,\n\t\t\tmaxTokens: 32000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"zai/glm-4.5\": {\n\t\t\tid: \"zai/glm-4.5\",\n\t\t\tname: \"GLM-4.5\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.6,\n\t\t\t\toutput: 2.2,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"zai/glm-4.5-air\": {\n\t\t\tid: \"zai/glm-4.5-air\",\n\t\t\tname: \"GLM 4.5 Air\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.19999999999999998,\n\t\t\t\toutput: 1.1,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 96000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"zai/glm-4.5v\": {\n\t\t\tid: \"zai/glm-4.5v\",\n\t\t\tname: \"GLM 4.5V\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.6,\n\t\t\t\toutput: 1.7999999999999998,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 65536,\n\t\t\tmaxTokens: 66000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"zai/glm-4.6\": {\n\t\t\tid: \"zai/glm-4.6\",\n\t\t\tname: \"GLM 4.6\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.44999999999999996,\n\t\t\t\toutput: 1.7999999999999998,\n\t\t\t\tcacheRead: 0.11,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 96000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"zai/glm-4.6v\": {\n\t\t\tid: \"zai/glm-4.6v\",\n\t\t\tname: \"GLM-4.6V\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 0.8999999999999999,\n\t\t\t\tcacheRead: 0.049999999999999996,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 24000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"zai/glm-4.6v-flash\": {\n\t\t\tid: \"zai/glm-4.6v-flash\",\n\t\t\tname: \"GLM-4.6V-Flash\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 24000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"zai/glm-4.7\": {\n\t\t\tid: \"zai/glm-4.7\",\n\t\t\tname: \"GLM 4.7\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.43,\n\t\t\t\toutput: 1.75,\n\t\t\t\tcacheRead: 0.08,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 202752,\n\t\t\tmaxTokens: 120000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t\t\"zai/glm-4.7-flashx\": {\n\t\t\tid: \"zai/glm-4.7-flashx\",\n\t\t\tname: \"GLM 4.7 FlashX\",\n\t\t\tapi: \"anthropic-messages\",\n\t\t\tprovider: \"vercel-ai-gateway\",\n\t\t\tbaseUrl: \"https://ai-gateway.vercel.sh\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.06,\n\t\t\t\toutput: 0.39999999999999997,\n\t\t\t\tcacheRead: 0.01,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 200000,\n\t\t\tmaxTokens: 128000,\n\t\t} satisfies Model<\"anthropic-messages\">,\n\t},\n\t\"xai\": {\n\t\t\"grok-2\": {\n\t\t\tid: \"grok-2\",\n\t\t\tname: \"Grok 2\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 2,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-2-1212\": {\n\t\t\tid: \"grok-2-1212\",\n\t\t\tname: \"Grok 2 (1212)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 2,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-2-latest\": {\n\t\t\tid: \"grok-2-latest\",\n\t\t\tname: \"Grok 2 Latest\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 2,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-2-vision\": {\n\t\t\tid: \"grok-2-vision\",\n\t\t\tname: \"Grok 2 Vision\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 2,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 8192,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-2-vision-1212\": {\n\t\t\tid: \"grok-2-vision-1212\",\n\t\t\tname: \"Grok 2 Vision (1212)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 2,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 8192,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-2-vision-latest\": {\n\t\t\tid: \"grok-2-vision-latest\",\n\t\t\tname: \"Grok 2 Vision Latest\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 2,\n\t\t\t\toutput: 10,\n\t\t\t\tcacheRead: 2,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 8192,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-3\": {\n\t\t\tid: \"grok-3\",\n\t\t\tname: \"Grok 3\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.75,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-3-fast\": {\n\t\t\tid: \"grok-3-fast\",\n\t\t\tname: \"Grok 3 Fast\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 5,\n\t\t\t\toutput: 25,\n\t\t\t\tcacheRead: 1.25,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-3-fast-latest\": {\n\t\t\tid: \"grok-3-fast-latest\",\n\t\t\tname: \"Grok 3 Fast Latest\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 5,\n\t\t\t\toutput: 25,\n\t\t\t\tcacheRead: 1.25,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-3-latest\": {\n\t\t\tid: \"grok-3-latest\",\n\t\t\tname: \"Grok 3 Latest\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.75,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-3-mini\": {\n\t\t\tid: \"grok-3-mini\",\n\t\t\tname: \"Grok 3 Mini\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0.075,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-3-mini-fast\": {\n\t\t\tid: \"grok-3-mini-fast\",\n\t\t\tname: \"Grok 3 Mini Fast\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.6,\n\t\t\t\toutput: 4,\n\t\t\t\tcacheRead: 0.15,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-3-mini-fast-latest\": {\n\t\t\tid: \"grok-3-mini-fast-latest\",\n\t\t\tname: \"Grok 3 Mini Fast Latest\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.6,\n\t\t\t\toutput: 4,\n\t\t\t\tcacheRead: 0.15,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-3-mini-latest\": {\n\t\t\tid: \"grok-3-mini-latest\",\n\t\t\tname: \"Grok 3 Mini Latest\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0.075,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 8192,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-4\": {\n\t\t\tid: \"grok-4\",\n\t\t\tname: \"Grok 4\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 3,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 0.75,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 64000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-4-1-fast\": {\n\t\t\tid: \"grok-4-1-fast\",\n\t\t\tname: \"Grok 4.1 Fast\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.2,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0.05,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 2000000,\n\t\t\tmaxTokens: 30000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-4-1-fast-non-reasoning\": {\n\t\t\tid: \"grok-4-1-fast-non-reasoning\",\n\t\t\tname: \"Grok 4.1 Fast (Non-Reasoning)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.2,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0.05,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 2000000,\n\t\t\tmaxTokens: 30000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-4-fast\": {\n\t\t\tid: \"grok-4-fast\",\n\t\t\tname: \"Grok 4 Fast\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.2,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0.05,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 2000000,\n\t\t\tmaxTokens: 30000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-4-fast-non-reasoning\": {\n\t\t\tid: \"grok-4-fast-non-reasoning\",\n\t\t\tname: \"Grok 4 Fast (Non-Reasoning)\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.2,\n\t\t\t\toutput: 0.5,\n\t\t\t\tcacheRead: 0.05,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 2000000,\n\t\t\tmaxTokens: 30000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-beta\": {\n\t\t\tid: \"grok-beta\",\n\t\t\tname: \"Grok Beta\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 5,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 5,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-code-fast-1\": {\n\t\t\tid: \"grok-code-fast-1\",\n\t\t\tname: \"Grok Code Fast 1\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.2,\n\t\t\t\toutput: 1.5,\n\t\t\t\tcacheRead: 0.02,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 256000,\n\t\t\tmaxTokens: 10000,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"grok-vision-beta\": {\n\t\t\tid: \"grok-vision-beta\",\n\t\t\tname: \"Grok Vision Beta\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"xai\",\n\t\t\tbaseUrl: \"https://api.x.ai/v1\",\n\t\t\treasoning: false,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 5,\n\t\t\t\toutput: 15,\n\t\t\t\tcacheRead: 5,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 8192,\n\t\t\tmaxTokens: 4096,\n\t\t} satisfies Model<\"openai-completions\">,\n\t},\n\t\"zai\": {\n\t\t\"glm-4.5\": {\n\t\t\tid: \"glm-4.5\",\n\t\t\tname: \"GLM-4.5\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"zai\",\n\t\t\tbaseUrl: \"https://api.z.ai/api/coding/paas/v4\",\n\t\t\tcompat: {\"supportsDeveloperRole\":false,\"thinkingFormat\":\"zai\"},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.6,\n\t\t\t\toutput: 2.2,\n\t\t\t\tcacheRead: 0.11,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 98304,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"glm-4.5-air\": {\n\t\t\tid: \"glm-4.5-air\",\n\t\t\tname: \"GLM-4.5-Air\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"zai\",\n\t\t\tbaseUrl: \"https://api.z.ai/api/coding/paas/v4\",\n\t\t\tcompat: {\"supportsDeveloperRole\":false,\"thinkingFormat\":\"zai\"},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.2,\n\t\t\t\toutput: 1.1,\n\t\t\t\tcacheRead: 0.03,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 98304,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"glm-4.5-flash\": {\n\t\t\tid: \"glm-4.5-flash\",\n\t\t\tname: \"GLM-4.5-Flash\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"zai\",\n\t\t\tbaseUrl: \"https://api.z.ai/api/coding/paas/v4\",\n\t\t\tcompat: {\"supportsDeveloperRole\":false,\"thinkingFormat\":\"zai\"},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 131072,\n\t\t\tmaxTokens: 98304,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"glm-4.5v\": {\n\t\t\tid: \"glm-4.5v\",\n\t\t\tname: \"GLM-4.5V\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"zai\",\n\t\t\tbaseUrl: \"https://api.z.ai/api/coding/paas/v4\",\n\t\t\tcompat: {\"supportsDeveloperRole\":false,\"thinkingFormat\":\"zai\"},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.6,\n\t\t\t\toutput: 1.8,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 64000,\n\t\t\tmaxTokens: 16384,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"glm-4.6\": {\n\t\t\tid: \"glm-4.6\",\n\t\t\tname: \"GLM-4.6\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"zai\",\n\t\t\tbaseUrl: \"https://api.z.ai/api/coding/paas/v4\",\n\t\t\tcompat: {\"supportsDeveloperRole\":false,\"thinkingFormat\":\"zai\"},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.6,\n\t\t\t\toutput: 2.2,\n\t\t\t\tcacheRead: 0.11,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 204800,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"glm-4.6v\": {\n\t\t\tid: \"glm-4.6v\",\n\t\t\tname: \"GLM-4.6V\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"zai\",\n\t\t\tbaseUrl: \"https://api.z.ai/api/coding/paas/v4\",\n\t\t\tcompat: {\"supportsDeveloperRole\":false,\"thinkingFormat\":\"zai\"},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\", \"image\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.3,\n\t\t\t\toutput: 0.9,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 128000,\n\t\t\tmaxTokens: 32768,\n\t\t} satisfies Model<\"openai-completions\">,\n\t\t\"glm-4.7\": {\n\t\t\tid: \"glm-4.7\",\n\t\t\tname: \"GLM-4.7\",\n\t\t\tapi: \"openai-completions\",\n\t\t\tprovider: \"zai\",\n\t\t\tbaseUrl: \"https://api.z.ai/api/coding/paas/v4\",\n\t\t\tcompat: {\"supportsDeveloperRole\":false,\"thinkingFormat\":\"zai\"},\n\t\t\treasoning: true,\n\t\t\tinput: [\"text\"],\n\t\t\tcost: {\n\t\t\t\tinput: 0.6,\n\t\t\t\toutput: 2.2,\n\t\t\t\tcacheRead: 0.11,\n\t\t\t\tcacheWrite: 0,\n\t\t\t},\n\t\t\tcontextWindow: 204800,\n\t\t\tmaxTokens: 131072,\n\t\t} satisfies Model<\"openai-completions\">,\n\t},\n} as const;\n"]}
node_modules/@mariozechner/pi-ai/dist/models.generated.js:            baseUrl: "https://api.z.ai/api/coding/paas/v4",
node_modules/@mariozechner/pi-ai/dist/models.generated.js:            baseUrl: "https://api.z.ai/api/coding/paas/v4",
node_modules/@mariozechner/pi-ai/dist/models.generated.js:            baseUrl: "https://api.z.ai/api/coding/paas/v4",
node_modules/@mariozechner/pi-ai/dist/models.generated.js:            baseUrl: "https://api.z.ai/api/coding/paas/v4",
node_modules/@mariozechner/pi-ai/dist/models.generated.js:            baseUrl: "https://api.z.ai/api/coding/paas/v4",
node_modules/@mariozechner/pi-ai/dist/models.generated.js:            baseUrl: "https://api.z.ai/api/coding/paas/v4",
node_modules/@mariozechner/pi-ai/dist/models.generated.js:            baseUrl: "https://api.z.ai/api/coding/paas/v4",
node_modules/@mariozechner/pi-ai/dist/providers/openai-completions.js.map:{"version":3,"file":"openai-completions.js","sourceRoot":"","sources":["../../src/providers/openai-completions.ts"],"names":[],"mappings":"AAAA,OAAO,MAAM,MAAM,QAAQ,CAAC;AAU5B,OAAO,EAAE,aAAa,EAAE,MAAM,cAAc,CAAC;AAC7C,OAAO,EAAE,YAAY,EAAE,MAAM,cAAc,CAAC;AAe5C,OAAO,EAAE,2BAA2B,EAAE,MAAM,0BAA0B,CAAC;AACvE,OAAO,EAAE,kBAAkB,EAAE,MAAM,wBAAwB,CAAC;AAC5D,OAAO,EAAE,kBAAkB,EAAE,MAAM,8BAA8B,CAAC;AAClE,OAAO,EAAE,iBAAiB,EAAE,MAAM,yBAAyB,CAAC;AAE5D;;;GAGG;AACH,SAAS,sBAAsB,CAAC,EAAU,EAAU;IACnD,qCAAqC;IACrC,IAAI,UAAU,GAAG,EAAE,CAAC,OAAO,CAAC,eAAe,EAAE,EAAE,CAAC,CAAC;IACjD,wCAAwC;IACxC,IAAI,UAAU,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;QAC3B,4EAA4E;QAC5E,MAAM,OAAO,GAAG,WAAW,CAAC;QAC5B,UAAU,GAAG,UAAU,GAAG,OAAO,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,GAAG,UAAU,CAAC,MAAM,CAAC,CAAC;IACnE,CAAC;SAAM,IAAI,UAAU,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;QAClC,UAAU,GAAG,UAAU,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;IACrC,CAAC;IACD,OAAO,UAAU,CAAC;AAAA,CAClB;AAED;;;;GAIG;AACH,SAAS,cAAc,CAAC,QAAmB,EAAW;IACrD,KAAK,MAAM,GAAG,IAAI,QAAQ,EAAE,CAAC;QAC5B,IAAI,GAAG,CAAC,IAAI,KAAK,YAAY,EAAE,CAAC;YAC/B,OAAO,IAAI,CAAC;QACb,CAAC;QACD,IAAI,GAAG,CAAC,IAAI,KAAK,WAAW,EAAE,CAAC;YAC9B,IAAI,GAAG,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,KAAK,EAAE,EAAE,CAAC,KAAK,CAAC,IAAI,KAAK,UAAU,CAAC,EAAE,CAAC;gBAC5D,OAAO,IAAI,CAAC;YACb,CAAC;QACF,CAAC;IACF,CAAC;IACD,OAAO,KAAK,CAAC;AAAA,CACb;AAOD,MAAM,CAAC,MAAM,uBAAuB,GAAyC,CAC5E,KAAkC,EAClC,OAAgB,EAChB,OAAkC,EACJ,EAAE,CAAC;IACjC,MAAM,MAAM,GAAG,IAAI,2BAA2B,EAAE,CAAC;IAEjD,CAAC,KAAK,IAAI,EAAE,CAAC;QACZ,MAAM,MAAM,GAAqB;YAChC,IAAI,EAAE,WAAW;YACjB,OAAO,EAAE,EAAE;YACX,GAAG,EAAE,KAAK,CAAC,GAAG;YACd,QAAQ,EAAE,KAAK,CAAC,QAAQ;YACxB,KAAK,EAAE,KAAK,CAAC,EAAE;YACf,KAAK,EAAE;gBACN,KAAK,EAAE,CAAC;gBACR,MAAM,EAAE,CAAC;gBACT,SAAS,EAAE,CAAC;gBACZ,UAAU,EAAE,CAAC;gBACb,WAAW,EAAE,CAAC;gBACd,IAAI,EAAE,EAAE,KAAK,EAAE,CAAC,EAAE,MAAM,EAAE,CAAC,EAAE,SAAS,EAAE,CAAC,EAAE,UAAU,EAAE,CAAC,EAAE,KAAK,EAAE,CAAC,EAAE;aACpE;YACD,UAAU,EAAE,MAAM;YAClB,SAAS,EAAE,IAAI,CAAC,GAAG,EAAE;SACrB,CAAC;QAEF,IAAI,CAAC;YACJ,MAAM,MAAM,GAAG,OAAO,EAAE,MAAM,IAAI,YAAY,CAAC,KAAK,CAAC,QAAQ,CAAC,IAAI,EAAE,CAAC;YACrE,MAAM,MAAM,GAAG,YAAY,CAAC,KAAK,EAAE,OAAO,EAAE,MAAM,EAAE,OAAO,EAAE,OAAO,CAAC,CAAC;YACtE,MAAM,MAAM,GAAG,WAAW,CAAC,KAAK,EAAE,OAAO,EAAE,OAAO,CAAC,CAAC;YACpD,OAAO,EAAE,SAAS,EAAE,CAAC,MAAM,CAAC,CAAC;YAC7B,MAAM,YAAY,GAAG,MAAM,MAAM,CAAC,IAAI,CAAC,WAAW,CAAC,MAAM,CAAC,MAAM,EAAE,EAAE,MAAM,EAAE,OAAO,EAAE,MAAM,EAAE,CAAC,CAAC;YAC/F,MAAM,CAAC,IAAI,CAAC,EAAE,IAAI,EAAE,OAAO,EAAE,OAAO,EAAE,MAAM,EAAE,CAAC,CAAC;YAEhD,IAAI,YAAY,GAAiF,IAAI,CAAC;YACtG,MAAM,MAAM,GAAG,MAAM,CAAC,OAAO,CAAC;YAC9B,MAAM,UAAU,GAAG,GAAG,EAAE,CAAC,MAAM,CAAC,MAAM,GAAG,CAAC,CAAC;YAC3C,MAAM,kBAAkB,GAAG,CAAC,KAA2B,EAAE,EAAE,CAAC;gBAC3D,IAAI,KAAK,EAAE,CAAC;oBACX,IAAI,KAAK,CAAC,IAAI,KAAK,MAAM,EAAE,CAAC;wBAC3B,MAAM,CAAC,IAAI,CAAC;4BACX,IAAI,EAAE,UAAU;4BAChB,YAAY,EAAE,UAAU,EAAE;4BAC1B,OAAO,EAAE,KAAK,CAAC,IAAI;4BACnB,OAAO,EAAE,MAAM;yBACf,CAAC,CAAC;oBACJ,CAAC;yBAAM,IAAI,KAAK,CAAC,IAAI,KAAK,UAAU,EAAE,CAAC;wBACtC,MAAM,CAAC,IAAI,CAAC;4BACX,IAAI,EAAE,cAAc;4BACpB,YAAY,EAAE,UAAU,EAAE;4BAC1B,OAAO,EAAE,KAAK,CAAC,QAAQ;4BACvB,OAAO,EAAE,MAAM;yBACf,CAAC,CAAC;oBACJ,CAAC;yBAAM,IAAI,KAAK,CAAC,IAAI,KAAK,UAAU,EAAE,CAAC;wBACtC,KAAK,CAAC,SAAS,GAAG,IAAI,CAAC,KAAK,CAAC,KAAK,CAAC,WAAW,IAAI,IAAI,CAAC,CAAC;wBACxD,OAAO,KAAK,CAAC,WAAW,CAAC;wBACzB,MAAM,CAAC,IAAI,CAAC;4BACX,IAAI,EAAE,cAAc;4BACpB,YAAY,EAAE,UAAU,EAAE;4BAC1B,QAAQ,EAAE,KAAK;4BACf,OAAO,EAAE,MAAM;yBACf,CAAC,CAAC;oBACJ,CAAC;gBACF,CAAC;YAAA,CACD,CAAC;YAEF,IAAI,KAAK,EAAE,MAAM,KAAK,IAAI,YAAY,EAAE,CAAC;gBACxC,IAAI,KAAK,CAAC,KAAK,EAAE,CAAC;oBACjB,MAAM,YAAY,GAAG,KAAK,CAAC,KAAK,CAAC,qBAAqB,EAAE,aAAa,IAAI,CAAC,CAAC;oBAC3E,MAAM,eAAe,GAAG,KAAK,CAAC,KAAK,CAAC,yBAAyB,EAAE,gBAAgB,IAAI,CAAC,CAAC;oBACrF,MAAM,KAAK,GAAG,CAAC,KAAK,CAAC,KAAK,CAAC,aAAa,IAAI,CAAC,CAAC,GAAG,YAAY,CAAC;oBAC9D,MAAM,YAAY,GAAG,CAAC,KAAK,CAAC,KAAK,CAAC,iBAAiB,IAAI,CAAC,CAAC,GAAG,eAAe,CAAC;oBAC5E,MAAM,CAAC,KAAK,GAAG;wBACd,sFAAsF;wBACtF,KAAK;wBACL,MAAM,EAAE,YAAY;wBACpB,SAAS,EAAE,YAAY;wBACvB,UAAU,EAAE,CAAC;wBACb,wEAAwE;wBACxE,qEAAqE;wBACrE,WAAW,EAAE,KAAK,GAAG,YAAY,GAAG,YAAY;wBAChD,IAAI,EAAE;4BACL,KAAK,EAAE,CAAC;4BACR,MAAM,EAAE,CAAC;4BACT,SAAS,EAAE,CAAC;4BACZ,UAAU,EAAE,CAAC;4BACb,KAAK,EAAE,CAAC;yBACR;qBACD,CAAC;oBACF,aAAa,CAAC,KAAK,EAAE,MAAM,CAAC,KAAK,CAAC,CAAC;gBACpC,CAAC;gBAED,MAAM,MAAM,GAAG,KAAK,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;gBAChC,IAAI,CAAC,MAAM;oBAAE,SAAS;gBAEtB,IAAI,MAAM,CAAC,aAAa,EAAE,CAAC;oBAC1B,MAAM,CAAC,UAAU,GAAG,aAAa,CAAC,MAAM,CAAC,aAAa,CAAC,CAAC;gBACzD,CAAC;gBAED,IAAI,MAAM,CAAC,KAAK,EAAE,CAAC;oBAClB,IACC,MAAM,CAAC,KAAK,CAAC,OAAO,KAAK,IAAI;wBAC7B,MAAM,CAAC,KAAK,CAAC,OAAO,KAAK,SAAS;wBAClC,MAAM,CAAC,KAAK,CAAC,OAAO,CAAC,MAAM,GAAG,CAAC,EAC9B,CAAC;wBACF,IAAI,CAAC,YAAY,IAAI,YAAY,CAAC,IAAI,KAAK,MAAM,EAAE,CAAC;4BACnD,kBAAkB,CAAC,YAAY,CAAC,CAAC;4BACjC,YAAY,GAAG,EAAE,IAAI,EAAE,MAAM,EAAE,IAAI,EAAE,EAAE,EAAE,CAAC;4BAC1C,MAAM,CAAC,OAAO,CAAC,IAAI,CAAC,YAAY,CAAC,CAAC;4BAClC,MAAM,CAAC,IAAI,CAAC,EAAE,IAAI,EAAE,YAAY,EAAE,YAAY,EAAE,UAAU,EAAE,EAAE,OAAO,EAAE,MAAM,EAAE,CAAC,CAAC;wBAClF,CAAC;wBAED,IAAI,YAAY,CAAC,IAAI,KAAK,MAAM,EAAE,CAAC;4BAClC,YAAY,CAAC,IAAI,IAAI,MAAM,CAAC,KAAK,CAAC,OAAO,CAAC;4BAC1C,MAAM,CAAC,IAAI,CAAC;gCACX,IAAI,EAAE,YAAY;gCAClB,YAAY,EAAE,UAAU,EAAE;gCAC1B,KAAK,EAAE,MAAM,CAAC,KAAK,CAAC,OAAO;gCAC3B,OAAO,EAAE,MAAM;6BACf,CAAC,CAAC;wBACJ,CAAC;oBACF,CAAC;oBAED,oEAAoE;oBACpE,mDAAmD;oBACnD,+DAA+D;oBAC/D,mFAAmF;oBACnF,MAAM,eAAe,GAAG,CAAC,mBAAmB,EAAE,WAAW,EAAE,gBAAgB,CAAC,CAAC;oBAC7E,IAAI,mBAAmB,GAAkB,IAAI,CAAC;oBAC9C,KAAK,MAAM,KAAK,IAAI,eAAe,EAAE,CAAC;wBACrC,IACE,MAAM,CAAC,KAAa,CAAC,KAAK,CAAC,KAAK,IAAI;4BACpC,MAAM,CAAC,KAAa,CAAC,KAAK,CAAC,KAAK,SAAS;4BACzC,MAAM,CAAC,KAAa,CAAC,KAAK,CAAC,CAAC,MAAM,GAAG,CAAC,EACtC,CAAC;4BACF,IAAI,CAAC,mBAAmB,EAAE,CAAC;gCAC1B,mBAAmB,GAAG,KAAK,CAAC;gCAC5B,MAAM;4BACP,CAAC;wBACF,CAAC;oBACF,CAAC;oBAED,IAAI,mBAAmB,EAAE,CAAC;wBACzB,IAAI,CAAC,YAAY,IAAI,YAAY,CAAC,IAAI,KAAK,UAAU,EAAE,CAAC;4BACvD,kBAAkB,CAAC,YAAY,CAAC,CAAC;4BACjC,YAAY,GAAG;gCACd,IAAI,EAAE,UAAU;gCAChB,QAAQ,EAAE,EAAE;gCACZ,iBAAiB,EAAE,mBAAmB;6BACtC,CAAC;4BACF,MAAM,CAAC,OAAO,CAAC,IAAI,CAAC,YAAY,CAAC,CAAC;4BAClC,MAAM,CAAC,IAAI,CAAC,EAAE,IAAI,EAAE,gBAAgB,EAAE,YAAY,EAAE,UAAU,EAAE,EAAE,OAAO,EAAE,MAAM,EAAE,CAAC,CAAC;wBACtF,CAAC;wBAED,IAAI,YAAY,CAAC,IAAI,KAAK,UAAU,EAAE,CAAC;4BACtC,MAAM,KAAK,GAAI,MAAM,CAAC,KAAa,CAAC,mBAAmB,CAAC,CAAC;4BACzD,YAAY,CAAC,QAAQ,IAAI,KAAK,CAAC;4BAC/B,MAAM,CAAC,IAAI,CAAC;gCACX,IAAI,EAAE,gBAAgB;gCACtB,YAAY,EAAE,UAAU,EAAE;gCAC1B,KAAK;gCACL,OAAO,EAAE,MAAM;6BACf,CAAC,CAAC;wBACJ,CAAC;oBACF,CAAC;oBAED,IAAI,MAAM,EAAE,KAAK,EAAE,UAAU,EAAE,CAAC;wBAC/B,KAAK,MAAM,QAAQ,IAAI,MAAM,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC;4BAChD,IACC,CAAC,YAAY;gCACb,YAAY,CAAC,IAAI,KAAK,UAAU;gCAChC,CAAC,QAAQ,CAAC,EAAE,IAAI,YAAY,CAAC,EAAE,KAAK,QAAQ,CAAC,EAAE,CAAC,EAC/C,CAAC;gCACF,kBAAkB,CAAC,YAAY,CAAC,CAAC;gCACjC,YAAY,GAAG;oCACd,IAAI,EAAE,UAAU;oCAChB,EAAE,EAAE,QAAQ,CAAC,EAAE,IAAI,EAAE;oCACrB,IAAI,EAAE,QAAQ,CAAC,QAAQ,EAAE,IAAI,IAAI,EAAE;oCACnC,SAAS,EAAE,EAAE;oCACb,WAAW,EAAE,EAAE;iCACf,CAAC;gCACF,MAAM,CAAC,OAAO,CAAC,IAAI,CAAC,YAAY,CAAC,CAAC;gCAClC,MAAM,CAAC,IAAI,CAAC,EAAE,IAAI,EAAE,gBAAgB,EAAE,YAAY,EAAE,UAAU,EAAE,EAAE,OAAO,EAAE,MAAM,EAAE,CAAC,CAAC;4BACtF,CAAC;4BAED,IAAI,YAAY,CAAC,IAAI,KAAK,UAAU,EAAE,CAAC;gCACtC,IAAI,QAAQ,CAAC,EAAE;oCAAE,YAAY,CAAC,EAAE,GAAG,QAAQ,CAAC,EAAE,CAAC;gCAC/C,IAAI,QAAQ,CAAC,QAAQ,EAAE,IAAI;oCAAE,YAAY,CAAC,IAAI,GAAG,QAAQ,CAAC,QAAQ,CAAC,IAAI,CAAC;gCACxE,IAAI,KAAK,GAAG,EAAE,CAAC;gCACf,IAAI,QAAQ,CAAC,QAAQ,EAAE,SAAS,EAAE,CAAC;oCAClC,KAAK,GAAG,QAAQ,CAAC,QAAQ,CAAC,SAAS,CAAC;oCACpC,YAAY,CAAC,WAAW,IAAI,QAAQ,CAAC,QAAQ,CAAC,SAAS,CAAC;oCACxD,YAAY,CAAC,SAAS,GAAG,kBAAkB,CAAC,YAAY,CAAC,WAAW,CAAC,CAAC;gCACvE,CAAC;gCACD,MAAM,CAAC,IAAI,CAAC;oCACX,IAAI,EAAE,gBAAgB;oCACtB,YAAY,EAAE,UAAU,EAAE;oCAC1B,KAAK;oCACL,OAAO,EAAE,MAAM;iCACf,CAAC,CAAC;4BACJ,CAAC;wBACF,CAAC;oBACF,CAAC;oBAED,MAAM,gBAAgB,GAAI,MAAM,CAAC,KAAa,CAAC,iBAAiB,CAAC;oBACjE,IAAI,gBAAgB,IAAI,KAAK,CAAC,OAAO,CAAC,gBAAgB,CAAC,EAAE,CAAC;wBACzD,KAAK,MAAM,MAAM,IAAI,gBAAgB,EAAE,CAAC;4BACvC,IAAI,MAAM,CAAC,IAAI,KAAK,qBAAqB,IAAI,MAAM,CAAC,EAAE,IAAI,MAAM,CAAC,IAAI,EAAE,CAAC;gCACvE,MAAM,gBAAgB,GAAG,MAAM,CAAC,OAAO,CAAC,IAAI,CAC3C,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,UAAU,IAAI,CAAC,CAAC,EAAE,KAAK,MAAM,CAAC,EAAE,CAC1B,CAAC;gCAC1B,IAAI,gBAAgB,EAAE,CAAC;oCACtB,gBAAgB,CAAC,gBAAgB,GAAG,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,CAAC;gCAC5D,CAAC;4BACF,CAAC;wBACF,CAAC;oBACF,CAAC;gBACF,CAAC;YACF,CAAC;YAED,kBAAkB,CAAC,YAAY,CAAC,CAAC;YAEjC,IAAI,OAAO,EAAE,MAAM,EAAE,OAAO,EAAE,CAAC;gBAC9B,MAAM,IAAI,KAAK,CAAC,qBAAqB,CAAC,CAAC;YACxC,CAAC;YAED,IAAI,MAAM,CAAC,UAAU,KAAK,SAAS,IAAI,MAAM,CAAC,UAAU,KAAK,OAAO,EAAE,CAAC;gBACtE,MAAM,IAAI,KAAK,CAAC,yBAAyB,CAAC,CAAC;YAC5C,CAAC;YAED,MAAM,CAAC,IAAI,CAAC,EAAE,IAAI,EAAE,MAAM,EAAE,MAAM,EAAE,MAAM,CAAC,UAAU,EAAE,OAAO,EAAE,MAAM,EAAE,CAAC,CAAC;YAC1E,MAAM,CAAC,GAAG,EAAE,CAAC;QACd,CAAC;QAAC,OAAO,KAAK,EAAE,CAAC;YAChB,KAAK,MAAM,KAAK,IAAI,MAAM,CAAC,OAAO;gBAAE,OAAQ,KAAa,CAAC,KAAK,CAAC;YAChE,MAAM,CAAC,UAAU,GAAG,OAAO,EAAE,MAAM,EAAE,OAAO,CAAC,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC,OAAO,CAAC;YACnE,MAAM,CAAC,YAAY,GAAG,KAAK,YAAY,KAAK,CAAC,CAAC,CAAC,KAAK,CAAC,OAAO,CAAC,CAAC,CAAC,IAAI,CAAC,SAAS,CAAC,KAAK,CAAC,CAAC;YACrF,2EAA2E;YAC3E,MAAM,WAAW,GAAI,KAAa,EAAE,KAAK,EAAE,QAAQ,EAAE,GAAG,CAAC;YACzD,IAAI,WAAW;gBAAE,MAAM,CAAC,YAAY,IAAI,KAAK,WAAW,EAAE,CAAC;YAC3D,MAAM,CAAC,IAAI,CAAC,EAAE,IAAI,EAAE,OAAO,EAAE,MAAM,EAAE,MAAM,CAAC,UAAU,EAAE,KAAK,EAAE,MAAM,EAAE,CAAC,CAAC;YACzE,MAAM,CAAC,GAAG,EAAE,CAAC;QACd,CAAC;IAAA,CACD,CAAC,EAAE,CAAC;IAEL,OAAO,MAAM,CAAC;AAAA,CACd,CAAC;AAEF,SAAS,YAAY,CACpB,KAAkC,EAClC,OAAgB,EAChB,MAAe,EACf,cAAuC,EACtC;IACD,IAAI,CAAC,MAAM,EAAE,CAAC;QACb,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,cAAc,EAAE,CAAC;YACjC,MAAM,IAAI,KAAK,CACd,gGAAgG,CAChG,CAAC;QACH,CAAC;QACD,MAAM,GAAG,OAAO,CAAC,GAAG,CAAC,cAAc,CAAC;IACrC,CAAC;IAED,MAAM,OAAO,GAAG,EAAE,GAAG,KAAK,CAAC,OAAO,EAAE,CAAC;IACrC,IAAI,KAAK,CAAC,QAAQ,KAAK,gBAAgB,EAAE,CAAC;QACzC,gFAAgF;QAChF,iFAAiF;QACjF,+CAA+C;QAC/C,MAAM,QAAQ,GAAG,OAAO,CAAC,QAAQ,IAAI,EAAE,CAAC;QACxC,MAAM,WAAW,GAAG,QAAQ,CAAC,QAAQ,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;QAClD,MAAM,WAAW,GAAG,WAAW,CAAC,CAAC,CAAC,WAAW,CAAC,IAAI,KAAK,MAAM,CAAC,CAAC,CAAC,KAAK,CAAC;QACtE,OAAO,CAAC,aAAa,CAAC,GAAG,WAAW,CAAC,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,MAAM,CAAC;QACxD,OAAO,CAAC,eAAe,CAAC,GAAG,oBAAoB,CAAC;QAEhD,mDAAmD;QACnD,MAAM,SAAS,GAAG,QAAQ,CAAC,IAAI,CAAC,CAAC,GAAG,EAAE,EAAE,CAAC;YACxC,IAAI,GAAG,CAAC,IAAI,KAAK,MAAM,IAAI,KAAK,CAAC,OAAO,CAAC,GAAG,CAAC,OAAO,CAAC,EAAE,CAAC;gBACvD,OAAO,GAAG,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,OAAO,CAAC,CAAC;YACpD,CAAC;YACD,IAAI,GAAG,CAAC,IAAI,KAAK,YAAY,IAAI,KAAK,CAAC,OAAO,CAAC,GAAG,CAAC,OAAO,CAAC,EAAE,CAAC;gBAC7D,OAAO,GAAG,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,OAAO,CAAC,CAAC;YACpD,CAAC;YACD,OAAO,KAAK,CAAC;QAAA,CACb,CAAC,CAAC;QACH,IAAI,SAAS,EAAE,CAAC;YACf,OAAO,CAAC,wBAAwB,CAAC,GAAG,MAAM,CAAC;QAC5C,CAAC;IACF,CAAC;IAED,2DAA2D;IAC3D,IAAI,cAAc,EAAE,CAAC;QACpB,MAAM,CAAC,MAAM,CAAC,OAAO,EAAE,cAAc,CAAC,CAAC;IACxC,CAAC;IAED,OAAO,IAAI,MAAM,CAAC;QACjB,MAAM;QACN,OAAO,EAAE,KAAK,CAAC,OAAO;QACtB,uBAAuB,EAAE,IAAI;QAC7B,cAAc,EAAE,OAAO;KACvB,CAAC,CAAC;AAAA,CACH;AAED,SAAS,WAAW,CAAC,KAAkC,EAAE,OAAgB,EAAE,OAAkC,EAAE;IAC9G,MAAM,MAAM,GAAG,SAAS,CAAC,KAAK,CAAC,CAAC;IAChC,MAAM,QAAQ,GAAG,eAAe,CAAC,KAAK,EAAE,OAAO,EAAE,MAAM,CAAC,CAAC;IACzD,uCAAuC,CAAC,KAAK,EAAE,QAAQ,CAAC,CAAC;IAEzD,MAAM,MAAM,GAAgE;QAC3E,KAAK,EAAE,KAAK,CAAC,EAAE;QACf,QAAQ;QACR,MAAM,EAAE,IAAI;KACZ,CAAC;IAEF,IAAI,MAAM,CAAC,wBAAwB,KAAK,KAAK,EAAE,CAAC;QAC9C,MAAc,CAAC,cAAc,GAAG,EAAE,aAAa,EAAE,IAAI,EAAE,CAAC;IAC1D,CAAC;IAED,IAAI,MAAM,CAAC,aAAa,EAAE,CAAC;QAC1B,MAAM,CAAC,KAAK,GAAG,KAAK,CAAC;IACtB,CAAC;IAED,IAAI,OAAO,EAAE,SAAS,EAAE,CAAC;QACxB,IAAI,MAAM,CAAC,cAAc,KAAK,YAAY,EAAE,CAAC;YAC3C,MAAc,CAAC,UAAU,GAAG,OAAO,CAAC,SAAS,CAAC;QAChD,CAAC;aAAM,CAAC;YACP,MAAM,CAAC,qBAAqB,GAAG,OAAO,CAAC,SAAS,CAAC;QAClD,CAAC;IACF,CAAC;IAED,IAAI,OAAO,EAAE,WAAW,KAAK,SAAS,EAAE,CAAC;QACxC,MAAM,CAAC,WAAW,GAAG,OAAO,CAAC,WAAW,CAAC;IAC1C,CAAC;IAED,IAAI,OAAO,CAAC,KAAK,EAAE,CAAC;QACnB,MAAM,CAAC,KAAK,GAAG,YAAY,CAAC,OAAO,CAAC,KAAK,CAAC,CAAC;IAC5C,CAAC;SAAM,IAAI,cAAc,CAAC,OAAO,CAAC,QAAQ,CAAC,EAAE,CAAC;QAC7C,mGAAmG;QACnG,MAAM,CAAC,KAAK,GAAG,EAAE,CAAC;IACnB,CAAC;IAED,IAAI,OAAO,EAAE,UAAU,EAAE,CAAC;QACzB,MAAM,CAAC,WAAW,GAAG,OAAO,CAAC,UAAU,CAAC;IACzC,CAAC;IAED,IAAI,MAAM,CAAC,cAAc,KAAK,KAAK,IAAI,KAAK,CAAC,SAAS,EAAE,CAAC;QACxD,8DAA8D;QAC9D,kEAAkE;QACjE,MAAc,CAAC,QAAQ,GAAG,EAAE,IAAI,EAAE,OAAO,EAAE,eAAe,CAAC,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC,UAAU,EAAE,CAAC;IACxF,CAAC;SAAM,IAAI,OAAO,EAAE,eAAe,IAAI,KAAK,CAAC,SAAS,IAAI,MAAM,CAAC,uBAAuB,EAAE,CAAC;QAC1F,gCAAgC;QAChC,MAAM,CAAC,gBAAgB,GAAG,OAAO,CAAC,eAAe,CAAC;IACnD,CAAC;IAED,OAAO,MAAM,CAAC;AAAA,CACd;AAED,SAAS,uCAAuC,CAC/C,KAAkC,EAClC,QAAsC,EAC/B;IACP,IAAI,KAAK,CAAC,QAAQ,KAAK,YAAY,IAAI,CAAC,KAAK,CAAC,EAAE,CAAC,UAAU,CAAC,YAAY,CAAC;QAAE,OAAO;IAElF,kFAAkF;IAClF,qFAAqF;IACrF,KAAK,IAAI,CAAC,GAAG,QAAQ,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC,IAAI,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC;QAC/C,MAAM,GAAG,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC;QACxB,IAAI,GAAG,CAAC,IAAI,KAAK,MAAM,IAAI,GAAG,CAAC,IAAI,KAAK,WAAW;YAAE,SAAS;QAE9D,MAAM,OAAO,GAAG,GAAG,CAAC,OAAO,CAAC;QAC5B,IAAI,OAAO,OAAO,KAAK,QAAQ,EAAE,CAAC;YACjC,GAAG,CAAC,OAAO,GAAG;gBACb,MAAM,CAAC,MAAM,CAAC,EAAE,IAAI,EAAE,MAAe,EAAE,IAAI,EAAE,OAAO,EAAE,EAAE,EAAE,aAAa,EAAE,EAAE,IAAI,EAAE,WAAW,EAAE,EAAE,CAAC;aACjG,CAAC;YACF,OAAO;QACR,CAAC;QAED,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,OAAO,CAAC;YAAE,SAAS;QAEtC,4CAA4C;QAC5C,KAAK,IAAI,CAAC,GAAG,OAAO,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC,IAAI,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC;YAC9C,MAAM,IAAI,GAAG,OAAO,CAAC,CAAC,CAAC,CAAC;YACxB,IAAI,IAAI,EAAE,IAAI,KAAK,MAAM,EAAE,CAAC;gBAC3B,MAAM,CAAC,MAAM,CAAC,IAAI,EAAE,EAAE,aAAa,EAAE,EAAE,IAAI,EAAE,WAAW,EAAE,EAAE,CAAC,CAAC;gBAC9D,OAAO;YACR,CAAC;QACF,CAAC;IACF,CAAC;AAAA,CACD;AAED,SAAS,eAAe,CACvB,KAAkC,EAClC,OAAgB,EAChB,MAAyC,EACV;IAC/B,MAAM,MAAM,GAAiC,EAAE,CAAC;IAEhD,MAAM,mBAAmB,GAAG,CAAC,EAAU,EAAU,EAAE,CAAC;QACnD,IAAI,MAAM,CAAC,sBAAsB;YAAE,OAAO,sBAAsB,CAAC,EAAE,CAAC,CAAC;QACrE,IAAI,KAAK,CAAC,QAAQ,KAAK,QAAQ;YAAE,OAAO,EAAE,CAAC,MAAM,GAAG,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC;QAC9E,mFAAmF;QACnF,IAAI,KAAK,CAAC,QAAQ,KAAK,gBAAgB,IAAI,KAAK,CAAC,EAAE,CAAC,WAAW,EAAE,CAAC,QAAQ,CAAC,QAAQ,CAAC,EAAE,CAAC;YACtF,OAAO,EAAE,CAAC,OAAO,CAAC,iBAAiB,EAAE,GAAG,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;QACxD,CAAC;QACD,OAAO,EAAE,CAAC;IAAA,CACV,CAAC;IAEF,MAAM,mBAAmB,GAAG,iBAAiB,CAAC,OAAO,CAAC,QAAQ,EAAE,KAAK,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,mBAAmB,CAAC,EAAE,CAAC,CAAC,CAAC;IAExG,IAAI,OAAO,CAAC,YAAY,EAAE,CAAC;QAC1B,MAAM,gBAAgB,GAAG,KAAK,CAAC,SAAS,IAAI,MAAM,CAAC,qBAAqB,CAAC;QACzE,MAAM,IAAI,GAAG,gBAAgB,CAAC,CAAC,CAAC,WAAW,CAAC,CAAC,CAAC,QAAQ,CAAC;QACvD,MAAM,CAAC,IAAI,CAAC,EAAE,IAAI,EAAE,IAAI,EAAE,OAAO,EAAE,kBAAkB,CAAC,OAAO,CAAC,YAAY,CAAC,EAAE,CAAC,CAAC;IAChF,CAAC;IAED,IAAI,QAAQ,GAAkB,IAAI,CAAC;IAEnC,KAAK,MAAM,GAAG,IAAI,mBAAmB,EAAE,CAAC;QACvC,+FAA+F;QAC/F,yDAAyD;QACzD,IAAI,MAAM,CAAC,gCAAgC,IAAI,QAAQ,KAAK,YAAY,IAAI,GAAG,CAAC,IAAI,KAAK,MAAM,EAAE,CAAC;YACjG,MAAM,CAAC,IAAI,CAAC;gBACX,IAAI,EAAE,WAAW;gBACjB,OAAO,EAAE,oCAAoC;aAC7C,CAAC,CAAC;QACJ,CAAC;QAED,IAAI,GAAG,CAAC,IAAI,KAAK,MAAM,EAAE,CAAC;YACzB,IAAI,OAAO,GAAG,CAAC,OAAO,KAAK,QAAQ,EAAE,CAAC;gBACrC,MAAM,CAAC,IAAI,CAAC;oBACX,IAAI,EAAE,MAAM;oBACZ,OAAO,EAAE,kBAAkB,CAAC,GAAG,CAAC,OAAO,CAAC;iBACxC,CAAC,CAAC;YACJ,CAAC;iBAAM,CAAC;gBACP,MAAM,OAAO,GAAgC,GAAG,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,IAAI,EAA6B,EAAE,CAAC;oBACjG,IAAI,IAAI,CAAC,IAAI,KAAK,MAAM,EAAE,CAAC;wBAC1B,OAAO;4BACN,IAAI,EAAE,MAAM;4BACZ,IAAI,EAAE,kBAAkB,CAAC,IAAI,CAAC,IAAI,CAAC;yBACK,CAAC;oBAC3C,CAAC;yBAAM,CAAC;wBACP,OAAO;4BACN,IAAI,EAAE,WAAW;4BACjB,SAAS,EAAE;gCACV,GAAG,EAAE,QAAQ,IAAI,CAAC,QAAQ,WAAW,IAAI,CAAC,IAAI,EAAE;6BAChD;yBACwC,CAAC;oBAC5C,CAAC;gBAAA,CACD,CAAC,CAAC;gBACH,MAAM,eAAe,GAAG,CAAC,KAAK,CAAC,KAAK,CAAC,QAAQ,CAAC,OAAO,CAAC;oBACrD,CAAC,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,WAAW,CAAC;oBAC/C,CAAC,CAAC,OAAO,CAAC;gBACX,IAAI,eAAe,CAAC,MAAM,KAAK,CAAC;oBAAE,SAAS;gBAC3C,MAAM,CAAC,IAAI,CAAC;oBACX,IAAI,EAAE,MAAM;oBACZ,OAAO,EAAE,eAAe;iBACxB,CAAC,CAAC;YACJ,CAAC;QACF,CAAC;aAAM,IAAI,GAAG,CAAC,IAAI,KAAK,WAAW,EAAE,CAAC;YACrC,oFAAoF;YACpF,MAAM,YAAY,GAAwC;gBACzD,IAAI,EAAE,WAAW;gBACjB,OAAO,EAAE,MAAM,CAAC,gCAAgC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,IAAI;aAC5D,CAAC;YAEF,MAAM,UAAU,GAAG,GAAG,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,MAAM,CAAkB,CAAC;YACjF,8DAA8D;YAC9D,MAAM,kBAAkB,GAAG,UAAU,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,EAAE,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;YACxF,IAAI,kBAAkB,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;gBACnC,uEAAuE;gBACvE,2EAA2E;gBAC3E,IAAI,KAAK,CAAC,QAAQ,KAAK,gBAAgB,EAAE,CAAC;oBACzC,YAAY,CAAC,OAAO,GAAG,kBAAkB,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,kBAAkB,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;gBAC3F,CAAC;qBAAM,CAAC;oBACP,YAAY,CAAC,OAAO,GAAG,kBAAkB,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC;wBACpD,OAAO,EAAE,IAAI,EAAE,MAAM,EAAE,IAAI,EAAE,kBAAkB,CAAC,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC;oBAAA,CAC1D,CAAC,CAAC;gBACJ,CAAC;YACF,CAAC;YAED,yBAAyB;YACzB,MAAM,cAAc,GAAG,GAAG,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,UAAU,CAAsB,CAAC;YAC7F,kEAAkE;YAClE,MAAM,sBAAsB,GAAG,cAAc,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,QAAQ,IAAI,CAAC,CAAC,QAAQ,CAAC,IAAI,EAAE,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;YACxG,IAAI,sBAAsB,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;gBACvC,IAAI,MAAM,CAAC,sBAAsB,EAAE,CAAC;oBACnC,gFAAgF;oBAChF,MAAM,YAAY,GAAG,sBAAsB,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;oBAChF,MAAM,WAAW,GAAG,YAAY,CAAC,OAAuD,CAAC;oBACzF,IAAI,WAAW,EAAE,CAAC;wBACjB,WAAW,CAAC,OAAO,CAAC,EAAE,IAAI,EAAE,MAAM,EAAE,IAAI,EAAE,YAAY,EAAE,CAAC,CAAC;oBAC3D,CAAC;yBAAM,CAAC;wBACP,YAAY,CAAC,OAAO,GAAG,CAAC,EAAE,IAAI,EAAE,MAAM,EAAE,IAAI,EAAE,YAAY,EAAE,CAAC,CAAC;oBAC/D,CAAC;gBACF,CAAC;qBAAM,CAAC;oBACP,gGAAgG;oBAChG,MAAM,SAAS,GAAG,sBAAsB,CAAC,CAAC,CAAC,CAAC,iBAAiB,CAAC;oBAC9D,IAAI,SAAS,IAAI,SAAS,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;wBACtC,YAAoB,CAAC,SAAS,CAAC,GAAG,sBAAsB,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;oBAC7F,CAAC;gBACF,CAAC;YACF,CAAC;YAED,MAAM,SAAS,GAAG,GAAG,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,UAAU,CAAe,CAAC;YACjF,IAAI,SAAS,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;gBAC1B,YAAY,CAAC,UAAU,GAAG,SAAS,CAAC,GAAG,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC;oBAChD,EAAE,EAAE,EAAE,CAAC,EAAE;oBACT,IAAI,EAAE,UAAmB;oBACzB,QAAQ,EAAE;wBACT,IAAI,EAAE,EAAE,CAAC,IAAI;wBACb,SAAS,EAAE,IAAI,CAAC,SAAS,CAAC,EAAE,CAAC,SAAS,CAAC;qBACvC;iBACD,CAAC,CAAC,CAAC;gBACJ,MAAM,gBAAgB,GAAG,SAAS;qBAChC,MAAM,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,gBAAgB,CAAC;qBACnC,GAAG,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC;oBACZ,IAAI,CAAC;wBACJ,OAAO,IAAI,CAAC,KAAK,CAAC,EAAE,CAAC,gBAAiB,CAAC,CAAC;oBACzC,CAAC;oBAAC,MAAM,CAAC;wBACR,OAAO,IAAI,CAAC;oBACb,CAAC;gBAAA,CACD,CAAC;qBACD,MAAM,CAAC,OAAO,CAAC,CAAC;gBAClB,IAAI,gBAAgB,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;oBAChC,YAAoB,CAAC,iBAAiB,GAAG,gBAAgB,CAAC;gBAC5D,CAAC;YACF,CAAC;YACD,kEAAkE;YAClE,4EAA4E;YAC5E,8DAA8D;YAC9D,gEAAgE;YAChE,MAAM,OAAO,GAAG,YAAY,CAAC,OAAO,CAAC;YACrC,MAAM,UAAU,GACf,OAAO,KAAK,IAAI;gBAChB,OAAO,KAAK,SAAS;gBACrB,CAAC,OAAO,OAAO,KAAK,QAAQ,CAAC,CAAC,CAAC,OAAO,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC,OAAO,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;YACzE,IAAI,CAAC,UAAU,IAAI,CAAC,YAAY,CAAC,UAAU,EAAE,CAAC;gBAC7C,SAAS;YACV,CAAC;YACD,MAAM,CAAC,IAAI,CAAC,YAAY,CAAC,CAAC;QAC3B,CAAC;aAAM,IAAI,GAAG,CAAC,IAAI,KAAK,YAAY,EAAE,CAAC;YACtC,iCAAiC;YACjC,MAAM,UAAU,GAAG,GAAG,CAAC,OAAO;iBAC5B,MAAM,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,MAAM,CAAC;iBAChC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAE,CAAS,CAAC,IAAI,CAAC;iBAC3B,IAAI,CAAC,IAAI,CAAC,CAAC;YACb,MAAM,SAAS,GAAG,GAAG,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,OAAO,CAAC,CAAC;YAE9D,oEAAoE;YACpE,MAAM,OAAO,GAAG,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC;YACtC,yEAAyE;YACzE,MAAM,aAAa,GAAmC;gBACrD,IAAI,EAAE,MAAM;gBACZ,OAAO,EAAE,kBAAkB,CAAC,OAAO,CAAC,CAAC,CAAC,UAAU,CAAC,CAAC,CAAC,sBAAsB,CAAC;gBAC1E,YAAY,EAAE,GAAG,CAAC,UAAU;aAC5B,CAAC;YACF,IAAI,MAAM,CAAC,sBAAsB,IAAI,GAAG,CAAC,QAAQ,EAAE,CAAC;gBAClD,aAAqB,CAAC,IAAI,GAAG,GAAG,CAAC,QAAQ,CAAC;YAC5C,CAAC;YACD,MAAM,CAAC,IAAI,CAAC,aAAa,CAAC,CAAC;YAE3B,yFAAyF;YACzF,IAAI,SAAS,IAAI,KAAK,CAAC,KAAK,CAAC,QAAQ,CAAC,OAAO,CAAC,EAAE,CAAC;gBAChD,MAAM,aAAa,GAEf,EAAE,CAAC;gBAEP,kBAAkB;gBAClB,aAAa,CAAC,IAAI,CAAC;oBAClB,IAAI,EAAE,MAAM;oBACZ,IAAI,EAAE,qCAAqC;iBAC3C,CAAC,CAAC;gBAEH,aAAa;gBACb,KAAK,MAAM,KAAK,IAAI,GAAG,CAAC,OAAO,EAAE,CAAC;oBACjC,IAAI,KAAK,CAAC,IAAI,KAAK,OAAO,EAAE,CAAC;wBAC5B,aAAa,CAAC,IAAI,CAAC;4BAClB,IAAI,EAAE,WAAW;4BACjB,SAAS,EAAE;gCACV,GAAG,EAAE,QAAS,KAAa,CAAC,QAAQ,WAAY,KAAa,CAAC,IAAI,EAAE;6BACpE;yBACD,CAAC,CAAC;oBACJ,CAAC;gBACF,CAAC;gBAED,MAAM,CAAC,IAAI,CAAC;oBACX,IAAI,EAAE,MAAM;oBACZ,OAAO,EAAE,aAAa;iBACtB,CAAC,CAAC;YACJ,CAAC;QACF,CAAC;QAED,QAAQ,GAAG,GAAG,CAAC,IAAI,CAAC;IACrB,CAAC;IAED,OAAO,MAAM,CAAC;AAAA,CACd;AAED,SAAS,YAAY,CAAC,KAAa,EAAgD;IAClF,OAAO,KAAK,CAAC,GAAG,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC;QAC3B,IAAI,EAAE,UAAU;QAChB,QAAQ,EAAE;YACT,IAAI,EAAE,IAAI,CAAC,IAAI;YACf,WAAW,EAAE,IAAI,CAAC,WAAW;YAC7B,UAAU,EAAE,IAAI,CAAC,UAAiB,EAAE,wCAAwC;YAC5E,MAAM,EAAE,KAAK,EAAE,uEAAuE;SACtF;KACD,CAAC,CAAC,CAAC;AAAA,CACJ;AAED,SAAS,aAAa,CAAC,MAAmD,EAAc;IACvF,IAAI,MAAM,KAAK,IAAI;QAAE,OAAO,MAAM,CAAC;IACnC,QAAQ,MAAM,EAAE,CAAC;QAChB,KAAK,MAAM;YACV,OAAO,MAAM,CAAC;QACf,KAAK,QAAQ;YACZ,OAAO,QAAQ,CAAC;QACjB,KAAK,eAAe,CAAC;QACrB,KAAK,YAAY;YAChB,OAAO,SAAS,CAAC;QAClB,KAAK,gBAAgB;YACpB,OAAO,OAAO,CAAC;QAChB,SAAS,CAAC;YACT,MAAM,WAAW,GAAU,MAAM,CAAC;YAClC,MAAM,IAAI,KAAK,CAAC,0BAA0B,WAAW,EAAE,CAAC,CAAC;QAC1D,CAAC;IACF,CAAC;AAAA,CACD;AAED;;;;GAIG;AACH,SAAS,YAAY,CAAC,KAAkC,EAAqC;IAC5F,MAAM,QAAQ,GAAG,KAAK,CAAC,QAAQ,CAAC;IAChC,MAAM,OAAO,GAAG,KAAK,CAAC,OAAO,CAAC;IAE9B,MAAM,KAAK,GAAG,QAAQ,KAAK,KAAK,IAAI,OAAO,CAAC,QAAQ,CAAC,UAAU,CAAC,CAAC;IAEjE,MAAM,aAAa,GAClB,QAAQ,KAAK,UAAU;QACvB,OAAO,CAAC,QAAQ,CAAC,aAAa,CAAC;QAC/B,QAAQ,KAAK,KAAK;QAClB,OAAO,CAAC,QAAQ,CAAC,UAAU,CAAC;QAC5B,QAAQ,KAAK,SAAS;QACtB,OAAO,CAAC,QAAQ,CAAC,YAAY,CAAC;QAC9B,OAAO,CAAC,QAAQ,CAAC,WAAW,CAAC;QAC7B,KAAK;QACL,QAAQ,KAAK,UAAU;QACvB,OAAO,CAAC,QAAQ,CAAC,aAAa,CAAC,CAAC;IAEjC,MAAM,YAAY,GAAG,QAAQ,KAAK,SAAS,IAAI,OAAO,CAAC,QAAQ,CAAC,YAAY,CAAC,IAAI,OAAO,CAAC,QAAQ,CAAC,WAAW,CAAC,CAAC;IAE/G,MAAM,MAAM,GAAG,QAAQ,KAAK,KAAK,IAAI,OAAO,CAAC,QAAQ,CAAC,UAAU,CAAC,CAAC;IAElE,MAAM,SAAS,GAAG,QAAQ,KAAK,SAAS,IAAI,OAAO,CAAC,QAAQ,CAAC,YAAY,CAAC,CAAC;IAE3E,OAAO;QACN,aAAa,EAAE,CAAC,aAAa;QAC7B,qBAAqB,EAAE,CAAC,aAAa;QACrC,uBAAuB,EAAE,CAAC,MAAM,IAAI,CAAC,KAAK;QAC1C,wBAAwB,EAAE,IAAI;QAC9B,cAAc,EAAE,YAAY,CAAC,CAAC,CAAC,YAAY,CAAC,CAAC,CAAC,uBAAuB;QACrE,sBAAsB,EAAE,SAAS;QACjC,gCAAgC,EAAE,KAAK,EAAE,iDAAiD;QAC1F,sBAAsB,EAAE,SAAS;QACjC,sBAAsB,EAAE,SAAS;QACjC,cAAc,EAAE,KAAK,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,QAAQ;KACxC,CAAC;AAAA,CACF;AAED;;;GAGG;AACH,SAAS,SAAS,CAAC,KAAkC,EAAqC;IACzF,MAAM,QAAQ,GAAG,YAAY,CAAC,KAAK,CAAC,CAAC;IACrC,IAAI,CAAC,KAAK,CAAC,MAAM;QAAE,OAAO,QAAQ,CAAC;IAEnC,OAAO;QACN,aAAa,EAAE,KAAK,CAAC,MAAM,CAAC,aAAa,IAAI,QAAQ,CAAC,aAAa;QACnE,qBAAqB,EAAE,KAAK,CAAC,MAAM,CAAC,qBAAqB,IAAI,QAAQ,CAAC,qBAAqB;QAC3F,uBAAuB,EAAE,KAAK,CAAC,MAAM,CAAC,uBAAuB,IAAI,QAAQ,CAAC,uBAAuB;QACjG,wBAAwB,EAAE,KAAK,CAAC,MAAM,CAAC,wBAAwB,IAAI,QAAQ,CAAC,wBAAwB;QACpG,cAAc,EAAE,KAAK,CAAC,MAAM,CAAC,cAAc,IAAI,QAAQ,CAAC,cAAc;QACtE,sBAAsB,EAAE,KAAK,CAAC,MAAM,CAAC,sBAAsB,IAAI,QAAQ,CAAC,sBAAsB;QAC9F,gCAAgC,EAC/B,KAAK,CAAC,MAAM,CAAC,gCAAgC,IAAI,QAAQ,CAAC,gCAAgC;QAC3F,sBAAsB,EAAE,KAAK,CAAC,MAAM,CAAC,sBAAsB,IAAI,QAAQ,CAAC,sBAAsB;QAC9F,sBAAsB,EAAE,KAAK,CAAC,MAAM,CAAC,sBAAsB,IAAI,QAAQ,CAAC,sBAAsB;QAC9F,cAAc,EAAE,KAAK,CAAC,MAAM,CAAC,cAAc,IAAI,QAAQ,CAAC,cAAc;KACtE,CAAC;AAAA,CACF","sourcesContent":["import OpenAI from \"openai\";\nimport type {\n\tChatCompletionAssistantMessageParam,\n\tChatCompletionChunk,\n\tChatCompletionContentPart,\n\tChatCompletionContentPartImage,\n\tChatCompletionContentPartText,\n\tChatCompletionMessageParam,\n\tChatCompletionToolMessageParam,\n} from \"openai/resources/chat/completions.js\";\nimport { calculateCost } from \"../models.js\";\nimport { getEnvApiKey } from \"../stream.js\";\nimport type {\n\tAssistantMessage,\n\tContext,\n\tMessage,\n\tModel,\n\tOpenAICompletionsCompat,\n\tStopReason,\n\tStreamFunction,\n\tStreamOptions,\n\tTextContent,\n\tThinkingContent,\n\tTool,\n\tToolCall,\n} from \"../types.js\";\nimport { AssistantMessageEventStream } from \"../utils/event-stream.js\";\nimport { parseStreamingJson } from \"../utils/json-parse.js\";\nimport { sanitizeSurrogates } from \"../utils/sanitize-unicode.js\";\nimport { transformMessages } from \"./transform-messages.js\";\n\n/**\n * Normalize tool call ID for Mistral.\n * Mistral requires tool IDs to be exactly 9 alphanumeric characters (a-z, A-Z, 0-9).\n */\nfunction normalizeMistralToolId(id: string): string {\n\t// Remove non-alphanumeric characters\n\tlet normalized = id.replace(/[^a-zA-Z0-9]/g, \"\");\n\t// Mistral requires exactly 9 characters\n\tif (normalized.length < 9) {\n\t\t// Pad with deterministic characters based on original ID to ensure matching\n\t\tconst padding = \"ABCDEFGHI\";\n\t\tnormalized = normalized + padding.slice(0, 9 - normalized.length);\n\t} else if (normalized.length > 9) {\n\t\tnormalized = normalized.slice(0, 9);\n\t}\n\treturn normalized;\n}\n\n/**\n * Check if conversation messages contain tool calls or tool results.\n * This is needed because Anthropic (via proxy) requires the tools param\n * to be present when messages include tool_calls or tool role messages.\n */\nfunction hasToolHistory(messages: Message[]): boolean {\n\tfor (const msg of messages) {\n\t\tif (msg.role === \"toolResult\") {\n\t\t\treturn true;\n\t\t}\n\t\tif (msg.role === \"assistant\") {\n\t\t\tif (msg.content.some((block) => block.type === \"toolCall\")) {\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t}\n\treturn false;\n}\n\nexport interface OpenAICompletionsOptions extends StreamOptions {\n\ttoolChoice?: \"auto\" | \"none\" | \"required\" | { type: \"function\"; function: { name: string } };\n\treasoningEffort?: \"minimal\" | \"low\" | \"medium\" | \"high\" | \"xhigh\";\n}\n\nexport const streamOpenAICompletions: StreamFunction<\"openai-completions\"> = (\n\tmodel: Model<\"openai-completions\">,\n\tcontext: Context,\n\toptions?: OpenAICompletionsOptions,\n): AssistantMessageEventStream => {\n\tconst stream = new AssistantMessageEventStream();\n\n\t(async () => {\n\t\tconst output: AssistantMessage = {\n\t\t\trole: \"assistant\",\n\t\t\tcontent: [],\n\t\t\tapi: model.api,\n\t\t\tprovider: model.provider,\n\t\t\tmodel: model.id,\n\t\t\tusage: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t\ttotalTokens: 0,\n\t\t\t\tcost: { input: 0, output: 0, cacheRead: 0, cacheWrite: 0, total: 0 },\n\t\t\t},\n\t\t\tstopReason: \"stop\",\n\t\t\ttimestamp: Date.now(),\n\t\t};\n\n\t\ttry {\n\t\t\tconst apiKey = options?.apiKey || getEnvApiKey(model.provider) || \"\";\n\t\t\tconst client = createClient(model, context, apiKey, options?.headers);\n\t\t\tconst params = buildParams(model, context, options);\n\t\t\toptions?.onPayload?.(params);\n\t\t\tconst openaiStream = await client.chat.completions.create(params, { signal: options?.signal });\n\t\t\tstream.push({ type: \"start\", partial: output });\n\n\t\t\tlet currentBlock: TextContent | ThinkingContent | (ToolCall & { partialArgs?: string }) | null = null;\n\t\t\tconst blocks = output.content;\n\t\t\tconst blockIndex = () => blocks.length - 1;\n\t\t\tconst finishCurrentBlock = (block?: typeof currentBlock) => {\n\t\t\t\tif (block) {\n\t\t\t\t\tif (block.type === \"text\") {\n\t\t\t\t\t\tstream.push({\n\t\t\t\t\t\t\ttype: \"text_end\",\n\t\t\t\t\t\t\tcontentIndex: blockIndex(),\n\t\t\t\t\t\t\tcontent: block.text,\n\t\t\t\t\t\t\tpartial: output,\n\t\t\t\t\t\t});\n\t\t\t\t\t} else if (block.type === \"thinking\") {\n\t\t\t\t\t\tstream.push({\n\t\t\t\t\t\t\ttype: \"thinking_end\",\n\t\t\t\t\t\t\tcontentIndex: blockIndex(),\n\t\t\t\t\t\t\tcontent: block.thinking,\n\t\t\t\t\t\t\tpartial: output,\n\t\t\t\t\t\t});\n\t\t\t\t\t} else if (block.type === \"toolCall\") {\n\t\t\t\t\t\tblock.arguments = JSON.parse(block.partialArgs || \"{}\");\n\t\t\t\t\t\tdelete block.partialArgs;\n\t\t\t\t\t\tstream.push({\n\t\t\t\t\t\t\ttype: \"toolcall_end\",\n\t\t\t\t\t\t\tcontentIndex: blockIndex(),\n\t\t\t\t\t\t\ttoolCall: block,\n\t\t\t\t\t\t\tpartial: output,\n\t\t\t\t\t\t});\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\n\t\t\tfor await (const chunk of openaiStream) {\n\t\t\t\tif (chunk.usage) {\n\t\t\t\t\tconst cachedTokens = chunk.usage.prompt_tokens_details?.cached_tokens || 0;\n\t\t\t\t\tconst reasoningTokens = chunk.usage.completion_tokens_details?.reasoning_tokens || 0;\n\t\t\t\t\tconst input = (chunk.usage.prompt_tokens || 0) - cachedTokens;\n\t\t\t\t\tconst outputTokens = (chunk.usage.completion_tokens || 0) + reasoningTokens;\n\t\t\t\t\toutput.usage = {\n\t\t\t\t\t\t// OpenAI includes cached tokens in prompt_tokens, so subtract to get non-cached input\n\t\t\t\t\t\tinput,\n\t\t\t\t\t\toutput: outputTokens,\n\t\t\t\t\t\tcacheRead: cachedTokens,\n\t\t\t\t\t\tcacheWrite: 0,\n\t\t\t\t\t\t// Compute totalTokens ourselves since we add reasoning_tokens to output\n\t\t\t\t\t\t// and some providers (e.g., Groq) don't include them in total_tokens\n\t\t\t\t\t\ttotalTokens: input + outputTokens + cachedTokens,\n\t\t\t\t\t\tcost: {\n\t\t\t\t\t\t\tinput: 0,\n\t\t\t\t\t\t\toutput: 0,\n\t\t\t\t\t\t\tcacheRead: 0,\n\t\t\t\t\t\t\tcacheWrite: 0,\n\t\t\t\t\t\t\ttotal: 0,\n\t\t\t\t\t\t},\n\t\t\t\t\t};\n\t\t\t\t\tcalculateCost(model, output.usage);\n\t\t\t\t}\n\n\t\t\t\tconst choice = chunk.choices[0];\n\t\t\t\tif (!choice) continue;\n\n\t\t\t\tif (choice.finish_reason) {\n\t\t\t\t\toutput.stopReason = mapStopReason(choice.finish_reason);\n\t\t\t\t}\n\n\t\t\t\tif (choice.delta) {\n\t\t\t\t\tif (\n\t\t\t\t\t\tchoice.delta.content !== null &&\n\t\t\t\t\t\tchoice.delta.content !== undefined &&\n\t\t\t\t\t\tchoice.delta.content.length > 0\n\t\t\t\t\t) {\n\t\t\t\t\t\tif (!currentBlock || currentBlock.type !== \"text\") {\n\t\t\t\t\t\t\tfinishCurrentBlock(currentBlock);\n\t\t\t\t\t\t\tcurrentBlock = { type: \"text\", text: \"\" };\n\t\t\t\t\t\t\toutput.content.push(currentBlock);\n\t\t\t\t\t\t\tstream.push({ type: \"text_start\", contentIndex: blockIndex(), partial: output });\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (currentBlock.type === \"text\") {\n\t\t\t\t\t\t\tcurrentBlock.text += choice.delta.content;\n\t\t\t\t\t\t\tstream.push({\n\t\t\t\t\t\t\t\ttype: \"text_delta\",\n\t\t\t\t\t\t\t\tcontentIndex: blockIndex(),\n\t\t\t\t\t\t\t\tdelta: choice.delta.content,\n\t\t\t\t\t\t\t\tpartial: output,\n\t\t\t\t\t\t\t});\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t// Some endpoints return reasoning in reasoning_content (llama.cpp),\n\t\t\t\t\t// or reasoning (other openai compatible endpoints)\n\t\t\t\t\t// Use the first non-empty reasoning field to avoid duplication\n\t\t\t\t\t// (e.g., chutes.ai returns both reasoning_content and reasoning with same content)\n\t\t\t\t\tconst reasoningFields = [\"reasoning_content\", \"reasoning\", \"reasoning_text\"];\n\t\t\t\t\tlet foundReasoningField: string | null = null;\n\t\t\t\t\tfor (const field of reasoningFields) {\n\t\t\t\t\t\tif (\n\t\t\t\t\t\t\t(choice.delta as any)[field] !== null &&\n\t\t\t\t\t\t\t(choice.delta as any)[field] !== undefined &&\n\t\t\t\t\t\t\t(choice.delta as any)[field].length > 0\n\t\t\t\t\t\t) {\n\t\t\t\t\t\t\tif (!foundReasoningField) {\n\t\t\t\t\t\t\t\tfoundReasoningField = field;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tif (foundReasoningField) {\n\t\t\t\t\t\tif (!currentBlock || currentBlock.type !== \"thinking\") {\n\t\t\t\t\t\t\tfinishCurrentBlock(currentBlock);\n\t\t\t\t\t\t\tcurrentBlock = {\n\t\t\t\t\t\t\t\ttype: \"thinking\",\n\t\t\t\t\t\t\t\tthinking: \"\",\n\t\t\t\t\t\t\t\tthinkingSignature: foundReasoningField,\n\t\t\t\t\t\t\t};\n\t\t\t\t\t\t\toutput.content.push(currentBlock);\n\t\t\t\t\t\t\tstream.push({ type: \"thinking_start\", contentIndex: blockIndex(), partial: output });\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (currentBlock.type === \"thinking\") {\n\t\t\t\t\t\t\tconst delta = (choice.delta as any)[foundReasoningField];\n\t\t\t\t\t\t\tcurrentBlock.thinking += delta;\n\t\t\t\t\t\t\tstream.push({\n\t\t\t\t\t\t\t\ttype: \"thinking_delta\",\n\t\t\t\t\t\t\t\tcontentIndex: blockIndex(),\n\t\t\t\t\t\t\t\tdelta,\n\t\t\t\t\t\t\t\tpartial: output,\n\t\t\t\t\t\t\t});\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tif (choice?.delta?.tool_calls) {\n\t\t\t\t\t\tfor (const toolCall of choice.delta.tool_calls) {\n\t\t\t\t\t\t\tif (\n\t\t\t\t\t\t\t\t!currentBlock ||\n\t\t\t\t\t\t\t\tcurrentBlock.type !== \"toolCall\" ||\n\t\t\t\t\t\t\t\t(toolCall.id && currentBlock.id !== toolCall.id)\n\t\t\t\t\t\t\t) {\n\t\t\t\t\t\t\t\tfinishCurrentBlock(currentBlock);\n\t\t\t\t\t\t\t\tcurrentBlock = {\n\t\t\t\t\t\t\t\t\ttype: \"toolCall\",\n\t\t\t\t\t\t\t\t\tid: toolCall.id || \"\",\n\t\t\t\t\t\t\t\t\tname: toolCall.function?.name || \"\",\n\t\t\t\t\t\t\t\t\targuments: {},\n\t\t\t\t\t\t\t\t\tpartialArgs: \"\",\n\t\t\t\t\t\t\t\t};\n\t\t\t\t\t\t\t\toutput.content.push(currentBlock);\n\t\t\t\t\t\t\t\tstream.push({ type: \"toolcall_start\", contentIndex: blockIndex(), partial: output });\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tif (currentBlock.type === \"toolCall\") {\n\t\t\t\t\t\t\t\tif (toolCall.id) currentBlock.id = toolCall.id;\n\t\t\t\t\t\t\t\tif (toolCall.function?.name) currentBlock.name = toolCall.function.name;\n\t\t\t\t\t\t\t\tlet delta = \"\";\n\t\t\t\t\t\t\t\tif (toolCall.function?.arguments) {\n\t\t\t\t\t\t\t\t\tdelta = toolCall.function.arguments;\n\t\t\t\t\t\t\t\t\tcurrentBlock.partialArgs += toolCall.function.arguments;\n\t\t\t\t\t\t\t\t\tcurrentBlock.arguments = parseStreamingJson(currentBlock.partialArgs);\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tstream.push({\n\t\t\t\t\t\t\t\t\ttype: \"toolcall_delta\",\n\t\t\t\t\t\t\t\t\tcontentIndex: blockIndex(),\n\t\t\t\t\t\t\t\t\tdelta,\n\t\t\t\t\t\t\t\t\tpartial: output,\n\t\t\t\t\t\t\t\t});\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tconst reasoningDetails = (choice.delta as any).reasoning_details;\n\t\t\t\t\tif (reasoningDetails && Array.isArray(reasoningDetails)) {\n\t\t\t\t\t\tfor (const detail of reasoningDetails) {\n\t\t\t\t\t\t\tif (detail.type === \"reasoning.encrypted\" && detail.id && detail.data) {\n\t\t\t\t\t\t\t\tconst matchingToolCall = output.content.find(\n\t\t\t\t\t\t\t\t\t(b) => b.type === \"toolCall\" && b.id === detail.id,\n\t\t\t\t\t\t\t\t) as ToolCall | undefined;\n\t\t\t\t\t\t\t\tif (matchingToolCall) {\n\t\t\t\t\t\t\t\t\tmatchingToolCall.thoughtSignature = JSON.stringify(detail);\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfinishCurrentBlock(currentBlock);\n\n\t\t\tif (options?.signal?.aborted) {\n\t\t\t\tthrow new Error(\"Request was aborted\");\n\t\t\t}\n\n\t\t\tif (output.stopReason === \"aborted\" || output.stopReason === \"error\") {\n\t\t\t\tthrow new Error(\"An unkown error ocurred\");\n\t\t\t}\n\n\t\t\tstream.push({ type: \"done\", reason: output.stopReason, message: output });\n\t\t\tstream.end();\n\t\t} catch (error) {\n\t\t\tfor (const block of output.content) delete (block as any).index;\n\t\t\toutput.stopReason = options?.signal?.aborted ? \"aborted\" : \"error\";\n\t\t\toutput.errorMessage = error instanceof Error ? error.message : JSON.stringify(error);\n\t\t\t// Some providers via OpenRouter give additional information in this field.\n\t\t\tconst rawMetadata = (error as any)?.error?.metadata?.raw;\n\t\t\tif (rawMetadata) output.errorMessage += `\\n${rawMetadata}`;\n\t\t\tstream.push({ type: \"error\", reason: output.stopReason, error: output });\n\t\t\tstream.end();\n\t\t}\n\t})();\n\n\treturn stream;\n};\n\nfunction createClient(\n\tmodel: Model<\"openai-completions\">,\n\tcontext: Context,\n\tapiKey?: string,\n\toptionsHeaders?: Record<string, string>,\n) {\n\tif (!apiKey) {\n\t\tif (!process.env.OPENAI_API_KEY) {\n\t\t\tthrow new Error(\n\t\t\t\t\"OpenAI API key is required. Set OPENAI_API_KEY environment variable or pass it as an argument.\",\n\t\t\t);\n\t\t}\n\t\tapiKey = process.env.OPENAI_API_KEY;\n\t}\n\n\tconst headers = { ...model.headers };\n\tif (model.provider === \"github-copilot\") {\n\t\t// Copilot expects X-Initiator to indicate whether the request is user-initiated\n\t\t// or agent-initiated (e.g. follow-up after assistant/tool messages). If there is\n\t\t// no prior message, default to user-initiated.\n\t\tconst messages = context.messages || [];\n\t\tconst lastMessage = messages[messages.length - 1];\n\t\tconst isAgentCall = lastMessage ? lastMessage.role !== \"user\" : false;\n\t\theaders[\"X-Initiator\"] = isAgentCall ? \"agent\" : \"user\";\n\t\theaders[\"Openai-Intent\"] = \"conversation-edits\";\n\n\t\t// Copilot requires this header when sending images\n\t\tconst hasImages = messages.some((msg) => {\n\t\t\tif (msg.role === \"user\" && Array.isArray(msg.content)) {\n\t\t\t\treturn msg.content.some((c) => c.type === \"image\");\n\t\t\t}\n\t\t\tif (msg.role === \"toolResult\" && Array.isArray(msg.content)) {\n\t\t\t\treturn msg.content.some((c) => c.type === \"image\");\n\t\t\t}\n\t\t\treturn false;\n\t\t});\n\t\tif (hasImages) {\n\t\t\theaders[\"Copilot-Vision-Request\"] = \"true\";\n\t\t}\n\t}\n\n\t// Merge options headers last so they can override defaults\n\tif (optionsHeaders) {\n\t\tObject.assign(headers, optionsHeaders);\n\t}\n\n\treturn new OpenAI({\n\t\tapiKey,\n\t\tbaseURL: model.baseUrl,\n\t\tdangerouslyAllowBrowser: true,\n\t\tdefaultHeaders: headers,\n\t});\n}\n\nfunction buildParams(model: Model<\"openai-completions\">, context: Context, options?: OpenAICompletionsOptions) {\n\tconst compat = getCompat(model);\n\tconst messages = convertMessages(model, context, compat);\n\tmaybeAddOpenRouterAnthropicCacheControl(model, messages);\n\n\tconst params: OpenAI.Chat.Completions.ChatCompletionCreateParamsStreaming = {\n\t\tmodel: model.id,\n\t\tmessages,\n\t\tstream: true,\n\t};\n\n\tif (compat.supportsUsageInStreaming !== false) {\n\t\t(params as any).stream_options = { include_usage: true };\n\t}\n\n\tif (compat.supportsStore) {\n\t\tparams.store = false;\n\t}\n\n\tif (options?.maxTokens) {\n\t\tif (compat.maxTokensField === \"max_tokens\") {\n\t\t\t(params as any).max_tokens = options.maxTokens;\n\t\t} else {\n\t\t\tparams.max_completion_tokens = options.maxTokens;\n\t\t}\n\t}\n\n\tif (options?.temperature !== undefined) {\n\t\tparams.temperature = options.temperature;\n\t}\n\n\tif (context.tools) {\n\t\tparams.tools = convertTools(context.tools);\n\t} else if (hasToolHistory(context.messages)) {\n\t\t// Anthropic (via LiteLLM/proxy) requires tools param when conversation has tool_calls/tool_results\n\t\tparams.tools = [];\n\t}\n\n\tif (options?.toolChoice) {\n\t\tparams.tool_choice = options.toolChoice;\n\t}\n\n\tif (compat.thinkingFormat === \"zai\" && model.reasoning) {\n\t\t// Z.ai uses binary thinking: { type: \"enabled\" | \"disabled\" }\n\t\t// Must explicitly disable since z.ai defaults to thinking enabled\n\t\t(params as any).thinking = { type: options?.reasoningEffort ? \"enabled\" : \"disabled\" };\n\t} else if (options?.reasoningEffort && model.reasoning && compat.supportsReasoningEffort) {\n\t\t// OpenAI-style reasoning_effort\n\t\tparams.reasoning_effort = options.reasoningEffort;\n\t}\n\n\treturn params;\n}\n\nfunction maybeAddOpenRouterAnthropicCacheControl(\n\tmodel: Model<\"openai-completions\">,\n\tmessages: ChatCompletionMessageParam[],\n): void {\n\tif (model.provider !== \"openrouter\" || !model.id.startsWith(\"anthropic/\")) return;\n\n\t// Anthropic-style caching requires cache_control on a text part. Add a breakpoint\n\t// on the last user/assistant message (walking backwards until we find text content).\n\tfor (let i = messages.length - 1; i >= 0; i--) {\n\t\tconst msg = messages[i];\n\t\tif (msg.role !== \"user\" && msg.role !== \"assistant\") continue;\n\n\t\tconst content = msg.content;\n\t\tif (typeof content === \"string\") {\n\t\t\tmsg.content = [\n\t\t\t\tObject.assign({ type: \"text\" as const, text: content }, { cache_control: { type: \"ephemeral\" } }),\n\t\t\t];\n\t\t\treturn;\n\t\t}\n\n\t\tif (!Array.isArray(content)) continue;\n\n\t\t// Find last text part and add cache_control\n\t\tfor (let j = content.length - 1; j >= 0; j--) {\n\t\t\tconst part = content[j];\n\t\t\tif (part?.type === \"text\") {\n\t\t\t\tObject.assign(part, { cache_control: { type: \"ephemeral\" } });\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunction convertMessages(\n\tmodel: Model<\"openai-completions\">,\n\tcontext: Context,\n\tcompat: Required<OpenAICompletionsCompat>,\n): ChatCompletionMessageParam[] {\n\tconst params: ChatCompletionMessageParam[] = [];\n\n\tconst normalizeToolCallId = (id: string): string => {\n\t\tif (compat.requiresMistralToolIds) return normalizeMistralToolId(id);\n\t\tif (model.provider === \"openai\") return id.length > 40 ? id.slice(0, 40) : id;\n\t\t// Copilot Claude models route to Claude backend which requires Anthropic ID format\n\t\tif (model.provider === \"github-copilot\" && model.id.toLowerCase().includes(\"claude\")) {\n\t\t\treturn id.replace(/[^a-zA-Z0-9_-]/g, \"_\").slice(0, 64);\n\t\t}\n\t\treturn id;\n\t};\n\n\tconst transformedMessages = transformMessages(context.messages, model, (id) => normalizeToolCallId(id));\n\n\tif (context.systemPrompt) {\n\t\tconst useDeveloperRole = model.reasoning && compat.supportsDeveloperRole;\n\t\tconst role = useDeveloperRole ? \"developer\" : \"system\";\n\t\tparams.push({ role: role, content: sanitizeSurrogates(context.systemPrompt) });\n\t}\n\n\tlet lastRole: string | null = null;\n\n\tfor (const msg of transformedMessages) {\n\t\t// Some providers (e.g. Mistral/Devstral) don't allow user messages directly after tool results\n\t\t// Insert a synthetic assistant message to bridge the gap\n\t\tif (compat.requiresAssistantAfterToolResult && lastRole === \"toolResult\" && msg.role === \"user\") {\n\t\t\tparams.push({\n\t\t\t\trole: \"assistant\",\n\t\t\t\tcontent: \"I have processed the tool results.\",\n\t\t\t});\n\t\t}\n\n\t\tif (msg.role === \"user\") {\n\t\t\tif (typeof msg.content === \"string\") {\n\t\t\t\tparams.push({\n\t\t\t\t\trole: \"user\",\n\t\t\t\t\tcontent: sanitizeSurrogates(msg.content),\n\t\t\t\t});\n\t\t\t} else {\n\t\t\t\tconst content: ChatCompletionContentPart[] = msg.content.map((item): ChatCompletionContentPart => {\n\t\t\t\t\tif (item.type === \"text\") {\n\t\t\t\t\t\treturn {\n\t\t\t\t\t\t\ttype: \"text\",\n\t\t\t\t\t\t\ttext: sanitizeSurrogates(item.text),\n\t\t\t\t\t\t} satisfies ChatCompletionContentPartText;\n\t\t\t\t\t} else {\n\t\t\t\t\t\treturn {\n\t\t\t\t\t\t\ttype: \"image_url\",\n\t\t\t\t\t\t\timage_url: {\n\t\t\t\t\t\t\t\turl: `data:${item.mimeType};base64,${item.data}`,\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t} satisfies ChatCompletionContentPartImage;\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t\t\tconst filteredContent = !model.input.includes(\"image\")\n\t\t\t\t\t? content.filter((c) => c.type !== \"image_url\")\n\t\t\t\t\t: content;\n\t\t\t\tif (filteredContent.length === 0) continue;\n\t\t\t\tparams.push({\n\t\t\t\t\trole: \"user\",\n\t\t\t\t\tcontent: filteredContent,\n\t\t\t\t});\n\t\t\t}\n\t\t} else if (msg.role === \"assistant\") {\n\t\t\t// Some providers (e.g. Mistral) don't accept null content, use empty string instead\n\t\t\tconst assistantMsg: ChatCompletionAssistantMessageParam = {\n\t\t\t\trole: \"assistant\",\n\t\t\t\tcontent: compat.requiresAssistantAfterToolResult ? \"\" : null,\n\t\t\t};\n\n\t\t\tconst textBlocks = msg.content.filter((b) => b.type === \"text\") as TextContent[];\n\t\t\t// Filter out empty text blocks to avoid API validation errors\n\t\t\tconst nonEmptyTextBlocks = textBlocks.filter((b) => b.text && b.text.trim().length > 0);\n\t\t\tif (nonEmptyTextBlocks.length > 0) {\n\t\t\t\t// GitHub Copilot requires assistant content as a string, not an array.\n\t\t\t\t// Sending as array causes Claude models to re-answer all previous prompts.\n\t\t\t\tif (model.provider === \"github-copilot\") {\n\t\t\t\t\tassistantMsg.content = nonEmptyTextBlocks.map((b) => sanitizeSurrogates(b.text)).join(\"\");\n\t\t\t\t} else {\n\t\t\t\t\tassistantMsg.content = nonEmptyTextBlocks.map((b) => {\n\t\t\t\t\t\treturn { type: \"text\", text: sanitizeSurrogates(b.text) };\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Handle thinking blocks\n\t\t\tconst thinkingBlocks = msg.content.filter((b) => b.type === \"thinking\") as ThinkingContent[];\n\t\t\t// Filter out empty thinking blocks to avoid API validation errors\n\t\t\tconst nonEmptyThinkingBlocks = thinkingBlocks.filter((b) => b.thinking && b.thinking.trim().length > 0);\n\t\t\tif (nonEmptyThinkingBlocks.length > 0) {\n\t\t\t\tif (compat.requiresThinkingAsText) {\n\t\t\t\t\t// Convert thinking blocks to plain text (no tags to avoid model mimicking them)\n\t\t\t\t\tconst thinkingText = nonEmptyThinkingBlocks.map((b) => b.thinking).join(\"\\n\\n\");\n\t\t\t\t\tconst textContent = assistantMsg.content as Array<{ type: \"text\"; text: string }> | null;\n\t\t\t\t\tif (textContent) {\n\t\t\t\t\t\ttextContent.unshift({ type: \"text\", text: thinkingText });\n\t\t\t\t\t} else {\n\t\t\t\t\t\tassistantMsg.content = [{ type: \"text\", text: thinkingText }];\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// Use the signature from the first thinking block if available (for llama.cpp server + gpt-oss)\n\t\t\t\t\tconst signature = nonEmptyThinkingBlocks[0].thinkingSignature;\n\t\t\t\t\tif (signature && signature.length > 0) {\n\t\t\t\t\t\t(assistantMsg as any)[signature] = nonEmptyThinkingBlocks.map((b) => b.thinking).join(\"\\n\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tconst toolCalls = msg.content.filter((b) => b.type === \"toolCall\") as ToolCall[];\n\t\t\tif (toolCalls.length > 0) {\n\t\t\t\tassistantMsg.tool_calls = toolCalls.map((tc) => ({\n\t\t\t\t\tid: tc.id,\n\t\t\t\t\ttype: \"function\" as const,\n\t\t\t\t\tfunction: {\n\t\t\t\t\t\tname: tc.name,\n\t\t\t\t\t\targuments: JSON.stringify(tc.arguments),\n\t\t\t\t\t},\n\t\t\t\t}));\n\t\t\t\tconst reasoningDetails = toolCalls\n\t\t\t\t\t.filter((tc) => tc.thoughtSignature)\n\t\t\t\t\t.map((tc) => {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\treturn JSON.parse(tc.thoughtSignature!);\n\t\t\t\t\t\t} catch {\n\t\t\t\t\t\t\treturn null;\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\t\t\t\t\t.filter(Boolean);\n\t\t\t\tif (reasoningDetails.length > 0) {\n\t\t\t\t\t(assistantMsg as any).reasoning_details = reasoningDetails;\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Skip assistant messages that have no content and no tool calls.\n\t\t\t// Mistral explicitly requires \"either content or tool_calls, but not none\".\n\t\t\t// Other providers also don't accept empty assistant messages.\n\t\t\t// This handles aborted assistant responses that got no content.\n\t\t\tconst content = assistantMsg.content;\n\t\t\tconst hasContent =\n\t\t\t\tcontent !== null &&\n\t\t\t\tcontent !== undefined &&\n\t\t\t\t(typeof content === \"string\" ? content.length > 0 : content.length > 0);\n\t\t\tif (!hasContent && !assistantMsg.tool_calls) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tparams.push(assistantMsg);\n\t\t} else if (msg.role === \"toolResult\") {\n\t\t\t// Extract text and image content\n\t\t\tconst textResult = msg.content\n\t\t\t\t.filter((c) => c.type === \"text\")\n\t\t\t\t.map((c) => (c as any).text)\n\t\t\t\t.join(\"\\n\");\n\t\t\tconst hasImages = msg.content.some((c) => c.type === \"image\");\n\n\t\t\t// Always send tool result with text (or placeholder if only images)\n\t\t\tconst hasText = textResult.length > 0;\n\t\t\t// Some providers (e.g. Mistral) require the 'name' field in tool results\n\t\t\tconst toolResultMsg: ChatCompletionToolMessageParam = {\n\t\t\t\trole: \"tool\",\n\t\t\t\tcontent: sanitizeSurrogates(hasText ? textResult : \"(see attached image)\"),\n\t\t\t\ttool_call_id: msg.toolCallId,\n\t\t\t};\n\t\t\tif (compat.requiresToolResultName && msg.toolName) {\n\t\t\t\t(toolResultMsg as any).name = msg.toolName;\n\t\t\t}\n\t\t\tparams.push(toolResultMsg);\n\n\t\t\t// If there are images and model supports them, send a follow-up user message with images\n\t\t\tif (hasImages && model.input.includes(\"image\")) {\n\t\t\t\tconst contentBlocks: Array<\n\t\t\t\t\t{ type: \"text\"; text: string } | { type: \"image_url\"; image_url: { url: string } }\n\t\t\t\t> = [];\n\n\t\t\t\t// Add text prefix\n\t\t\t\tcontentBlocks.push({\n\t\t\t\t\ttype: \"text\",\n\t\t\t\t\ttext: \"Attached image(s) from tool result:\",\n\t\t\t\t});\n\n\t\t\t\t// Add images\n\t\t\t\tfor (const block of msg.content) {\n\t\t\t\t\tif (block.type === \"image\") {\n\t\t\t\t\t\tcontentBlocks.push({\n\t\t\t\t\t\t\ttype: \"image_url\",\n\t\t\t\t\t\t\timage_url: {\n\t\t\t\t\t\t\t\turl: `data:${(block as any).mimeType};base64,${(block as any).data}`,\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t});\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tparams.push({\n\t\t\t\t\trole: \"user\",\n\t\t\t\t\tcontent: contentBlocks,\n\t\t\t\t});\n\t\t\t}\n\t\t}\n\n\t\tlastRole = msg.role;\n\t}\n\n\treturn params;\n}\n\nfunction convertTools(tools: Tool[]): OpenAI.Chat.Completions.ChatCompletionTool[] {\n\treturn tools.map((tool) => ({\n\t\ttype: \"function\",\n\t\tfunction: {\n\t\t\tname: tool.name,\n\t\t\tdescription: tool.description,\n\t\t\tparameters: tool.parameters as any, // TypeBox already generates JSON Schema\n\t\t\tstrict: false, // Disable strict mode to allow optional parameters without null unions\n\t\t},\n\t}));\n}\n\nfunction mapStopReason(reason: ChatCompletionChunk.Choice[\"finish_reason\"]): StopReason {\n\tif (reason === null) return \"stop\";\n\tswitch (reason) {\n\t\tcase \"stop\":\n\t\t\treturn \"stop\";\n\t\tcase \"length\":\n\t\t\treturn \"length\";\n\t\tcase \"function_call\":\n\t\tcase \"tool_calls\":\n\t\t\treturn \"toolUse\";\n\t\tcase \"content_filter\":\n\t\t\treturn \"error\";\n\t\tdefault: {\n\t\t\tconst _exhaustive: never = reason;\n\t\t\tthrow new Error(`Unhandled stop reason: ${_exhaustive}`);\n\t\t}\n\t}\n}\n\n/**\n * Detect compatibility settings from provider and baseUrl for known providers.\n * Provider takes precedence over URL-based detection since it's explicitly configured.\n * Returns a fully resolved OpenAICompletionsCompat object with all fields set.\n */\nfunction detectCompat(model: Model<\"openai-completions\">): Required<OpenAICompletionsCompat> {\n\tconst provider = model.provider;\n\tconst baseUrl = model.baseUrl;\n\n\tconst isZai = provider === \"zai\" || baseUrl.includes(\"api.z.ai\");\n\n\tconst isNonStandard =\n\t\tprovider === \"cerebras\" ||\n\t\tbaseUrl.includes(\"cerebras.ai\") ||\n\t\tprovider === \"xai\" ||\n\t\tbaseUrl.includes(\"api.x.ai\") ||\n\t\tprovider === \"mistral\" ||\n\t\tbaseUrl.includes(\"mistral.ai\") ||\n\t\tbaseUrl.includes(\"chutes.ai\") ||\n\t\tisZai ||\n\t\tprovider === \"opencode\" ||\n\t\tbaseUrl.includes(\"opencode.ai\");\n\n\tconst useMaxTokens = provider === \"mistral\" || baseUrl.includes(\"mistral.ai\") || baseUrl.includes(\"chutes.ai\");\n\n\tconst isGrok = provider === \"xai\" || baseUrl.includes(\"api.x.ai\");\n\n\tconst isMistral = provider === \"mistral\" || baseUrl.includes(\"mistral.ai\");\n\n\treturn {\n\t\tsupportsStore: !isNonStandard,\n\t\tsupportsDeveloperRole: !isNonStandard,\n\t\tsupportsReasoningEffort: !isGrok && !isZai,\n\t\tsupportsUsageInStreaming: true,\n\t\tmaxTokensField: useMaxTokens ? \"max_tokens\" : \"max_completion_tokens\",\n\t\trequiresToolResultName: isMistral,\n\t\trequiresAssistantAfterToolResult: false, // Mistral no longer requires this as of Dec 2024\n\t\trequiresThinkingAsText: isMistral,\n\t\trequiresMistralToolIds: isMistral,\n\t\tthinkingFormat: isZai ? \"zai\" : \"openai\",\n\t};\n}\n\n/**\n * Get resolved compatibility settings for a model.\n * Uses explicit model.compat if provided, otherwise auto-detects from provider/URL.\n */\nfunction getCompat(model: Model<\"openai-completions\">): Required<OpenAICompletionsCompat> {\n\tconst detected = detectCompat(model);\n\tif (!model.compat) return detected;\n\n\treturn {\n\t\tsupportsStore: model.compat.supportsStore ?? detected.supportsStore,\n\t\tsupportsDeveloperRole: model.compat.supportsDeveloperRole ?? detected.supportsDeveloperRole,\n\t\tsupportsReasoningEffort: model.compat.supportsReasoningEffort ?? detected.supportsReasoningEffort,\n\t\tsupportsUsageInStreaming: model.compat.supportsUsageInStreaming ?? detected.supportsUsageInStreaming,\n\t\tmaxTokensField: model.compat.maxTokensField ?? detected.maxTokensField,\n\t\trequiresToolResultName: model.compat.requiresToolResultName ?? detected.requiresToolResultName,\n\t\trequiresAssistantAfterToolResult:\n\t\t\tmodel.compat.requiresAssistantAfterToolResult ?? detected.requiresAssistantAfterToolResult,\n\t\trequiresThinkingAsText: model.compat.requiresThinkingAsText ?? detected.requiresThinkingAsText,\n\t\trequiresMistralToolIds: model.compat.requiresMistralToolIds ?? detected.requiresMistralToolIds,\n\t\tthinkingFormat: model.compat.thinkingFormat ?? detected.thinkingFormat,\n\t};\n}\n"]}
node_modules/@mariozechner/pi-ai/dist/providers/openai-completions.d.ts.map:{"version":3,"file":"openai-completions.d.ts","sourceRoot":"","sources":["../../src/providers/openai-completions.ts"],"names":[],"mappings":"AAYA,OAAO,KAAK,EAOX,cAAc,EACd,aAAa,EAKb,MAAM,aAAa,CAAC;AA2CrB,MAAM,WAAW,wBAAyB,SAAQ,aAAa;IAC9D,UAAU,CAAC,EAAE,MAAM,GAAG,MAAM,GAAG,UAAU,GAAG;QAAE,IAAI,EAAE,UAAU,CAAC;QAAC,QAAQ,EAAE;YAAE,IAAI,EAAE,MAAM,CAAA;SAAE,CAAA;KAAE,CAAC;IAC7F,eAAe,CAAC,EAAE,SAAS,GAAG,KAAK,GAAG,QAAQ,GAAG,MAAM,GAAG,OAAO,CAAC;CAClE;AAED,eAAO,MAAM,uBAAuB,EAAE,cAAc,CAAC,oBAAoB,CAqPxE,CAAC","sourcesContent":["import OpenAI from \"openai\";\nimport type {\n\tChatCompletionAssistantMessageParam,\n\tChatCompletionChunk,\n\tChatCompletionContentPart,\n\tChatCompletionContentPartImage,\n\tChatCompletionContentPartText,\n\tChatCompletionMessageParam,\n\tChatCompletionToolMessageParam,\n} from \"openai/resources/chat/completions.js\";\nimport { calculateCost } from \"../models.js\";\nimport { getEnvApiKey } from \"../stream.js\";\nimport type {\n\tAssistantMessage,\n\tContext,\n\tMessage,\n\tModel,\n\tOpenAICompletionsCompat,\n\tStopReason,\n\tStreamFunction,\n\tStreamOptions,\n\tTextContent,\n\tThinkingContent,\n\tTool,\n\tToolCall,\n} from \"../types.js\";\nimport { AssistantMessageEventStream } from \"../utils/event-stream.js\";\nimport { parseStreamingJson } from \"../utils/json-parse.js\";\nimport { sanitizeSurrogates } from \"../utils/sanitize-unicode.js\";\nimport { transformMessages } from \"./transform-messages.js\";\n\n/**\n * Normalize tool call ID for Mistral.\n * Mistral requires tool IDs to be exactly 9 alphanumeric characters (a-z, A-Z, 0-9).\n */\nfunction normalizeMistralToolId(id: string): string {\n\t// Remove non-alphanumeric characters\n\tlet normalized = id.replace(/[^a-zA-Z0-9]/g, \"\");\n\t// Mistral requires exactly 9 characters\n\tif (normalized.length < 9) {\n\t\t// Pad with deterministic characters based on original ID to ensure matching\n\t\tconst padding = \"ABCDEFGHI\";\n\t\tnormalized = normalized + padding.slice(0, 9 - normalized.length);\n\t} else if (normalized.length > 9) {\n\t\tnormalized = normalized.slice(0, 9);\n\t}\n\treturn normalized;\n}\n\n/**\n * Check if conversation messages contain tool calls or tool results.\n * This is needed because Anthropic (via proxy) requires the tools param\n * to be present when messages include tool_calls or tool role messages.\n */\nfunction hasToolHistory(messages: Message[]): boolean {\n\tfor (const msg of messages) {\n\t\tif (msg.role === \"toolResult\") {\n\t\t\treturn true;\n\t\t}\n\t\tif (msg.role === \"assistant\") {\n\t\t\tif (msg.content.some((block) => block.type === \"toolCall\")) {\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t}\n\treturn false;\n}\n\nexport interface OpenAICompletionsOptions extends StreamOptions {\n\ttoolChoice?: \"auto\" | \"none\" | \"required\" | { type: \"function\"; function: { name: string } };\n\treasoningEffort?: \"minimal\" | \"low\" | \"medium\" | \"high\" | \"xhigh\";\n}\n\nexport const streamOpenAICompletions: StreamFunction<\"openai-completions\"> = (\n\tmodel: Model<\"openai-completions\">,\n\tcontext: Context,\n\toptions?: OpenAICompletionsOptions,\n): AssistantMessageEventStream => {\n\tconst stream = new AssistantMessageEventStream();\n\n\t(async () => {\n\t\tconst output: AssistantMessage = {\n\t\t\trole: \"assistant\",\n\t\t\tcontent: [],\n\t\t\tapi: model.api,\n\t\t\tprovider: model.provider,\n\t\t\tmodel: model.id,\n\t\t\tusage: {\n\t\t\t\tinput: 0,\n\t\t\t\toutput: 0,\n\t\t\t\tcacheRead: 0,\n\t\t\t\tcacheWrite: 0,\n\t\t\t\ttotalTokens: 0,\n\t\t\t\tcost: { input: 0, output: 0, cacheRead: 0, cacheWrite: 0, total: 0 },\n\t\t\t},\n\t\t\tstopReason: \"stop\",\n\t\t\ttimestamp: Date.now(),\n\t\t};\n\n\t\ttry {\n\t\t\tconst apiKey = options?.apiKey || getEnvApiKey(model.provider) || \"\";\n\t\t\tconst client = createClient(model, context, apiKey, options?.headers);\n\t\t\tconst params = buildParams(model, context, options);\n\t\t\toptions?.onPayload?.(params);\n\t\t\tconst openaiStream = await client.chat.completions.create(params, { signal: options?.signal });\n\t\t\tstream.push({ type: \"start\", partial: output });\n\n\t\t\tlet currentBlock: TextContent | ThinkingContent | (ToolCall & { partialArgs?: string }) | null = null;\n\t\t\tconst blocks = output.content;\n\t\t\tconst blockIndex = () => blocks.length - 1;\n\t\t\tconst finishCurrentBlock = (block?: typeof currentBlock) => {\n\t\t\t\tif (block) {\n\t\t\t\t\tif (block.type === \"text\") {\n\t\t\t\t\t\tstream.push({\n\t\t\t\t\t\t\ttype: \"text_end\",\n\t\t\t\t\t\t\tcontentIndex: blockIndex(),\n\t\t\t\t\t\t\tcontent: block.text,\n\t\t\t\t\t\t\tpartial: output,\n\t\t\t\t\t\t});\n\t\t\t\t\t} else if (block.type === \"thinking\") {\n\t\t\t\t\t\tstream.push({\n\t\t\t\t\t\t\ttype: \"thinking_end\",\n\t\t\t\t\t\t\tcontentIndex: blockIndex(),\n\t\t\t\t\t\t\tcontent: block.thinking,\n\t\t\t\t\t\t\tpartial: output,\n\t\t\t\t\t\t});\n\t\t\t\t\t} else if (block.type === \"toolCall\") {\n\t\t\t\t\t\tblock.arguments = JSON.parse(block.partialArgs || \"{}\");\n\t\t\t\t\t\tdelete block.partialArgs;\n\t\t\t\t\t\tstream.push({\n\t\t\t\t\t\t\ttype: \"toolcall_end\",\n\t\t\t\t\t\t\tcontentIndex: blockIndex(),\n\t\t\t\t\t\t\ttoolCall: block,\n\t\t\t\t\t\t\tpartial: output,\n\t\t\t\t\t\t});\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\n\t\t\tfor await (const chunk of openaiStream) {\n\t\t\t\tif (chunk.usage) {\n\t\t\t\t\tconst cachedTokens = chunk.usage.prompt_tokens_details?.cached_tokens || 0;\n\t\t\t\t\tconst reasoningTokens = chunk.usage.completion_tokens_details?.reasoning_tokens || 0;\n\t\t\t\t\tconst input = (chunk.usage.prompt_tokens || 0) - cachedTokens;\n\t\t\t\t\tconst outputTokens = (chunk.usage.completion_tokens || 0) + reasoningTokens;\n\t\t\t\t\toutput.usage = {\n\t\t\t\t\t\t// OpenAI includes cached tokens in prompt_tokens, so subtract to get non-cached input\n\t\t\t\t\t\tinput,\n\t\t\t\t\t\toutput: outputTokens,\n\t\t\t\t\t\tcacheRead: cachedTokens,\n\t\t\t\t\t\tcacheWrite: 0,\n\t\t\t\t\t\t// Compute totalTokens ourselves since we add reasoning_tokens to output\n\t\t\t\t\t\t// and some providers (e.g., Groq) don't include them in total_tokens\n\t\t\t\t\t\ttotalTokens: input + outputTokens + cachedTokens,\n\t\t\t\t\t\tcost: {\n\t\t\t\t\t\t\tinput: 0,\n\t\t\t\t\t\t\toutput: 0,\n\t\t\t\t\t\t\tcacheRead: 0,\n\t\t\t\t\t\t\tcacheWrite: 0,\n\t\t\t\t\t\t\ttotal: 0,\n\t\t\t\t\t\t},\n\t\t\t\t\t};\n\t\t\t\t\tcalculateCost(model, output.usage);\n\t\t\t\t}\n\n\t\t\t\tconst choice = chunk.choices[0];\n\t\t\t\tif (!choice) continue;\n\n\t\t\t\tif (choice.finish_reason) {\n\t\t\t\t\toutput.stopReason = mapStopReason(choice.finish_reason);\n\t\t\t\t}\n\n\t\t\t\tif (choice.delta) {\n\t\t\t\t\tif (\n\t\t\t\t\t\tchoice.delta.content !== null &&\n\t\t\t\t\t\tchoice.delta.content !== undefined &&\n\t\t\t\t\t\tchoice.delta.content.length > 0\n\t\t\t\t\t) {\n\t\t\t\t\t\tif (!currentBlock || currentBlock.type !== \"text\") {\n\t\t\t\t\t\t\tfinishCurrentBlock(currentBlock);\n\t\t\t\t\t\t\tcurrentBlock = { type: \"text\", text: \"\" };\n\t\t\t\t\t\t\toutput.content.push(currentBlock);\n\t\t\t\t\t\t\tstream.push({ type: \"text_start\", contentIndex: blockIndex(), partial: output });\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (currentBlock.type === \"text\") {\n\t\t\t\t\t\t\tcurrentBlock.text += choice.delta.content;\n\t\t\t\t\t\t\tstream.push({\n\t\t\t\t\t\t\t\ttype: \"text_delta\",\n\t\t\t\t\t\t\t\tcontentIndex: blockIndex(),\n\t\t\t\t\t\t\t\tdelta: choice.delta.content,\n\t\t\t\t\t\t\t\tpartial: output,\n\t\t\t\t\t\t\t});\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t// Some endpoints return reasoning in reasoning_content (llama.cpp),\n\t\t\t\t\t// or reasoning (other openai compatible endpoints)\n\t\t\t\t\t// Use the first non-empty reasoning field to avoid duplication\n\t\t\t\t\t// (e.g., chutes.ai returns both reasoning_content and reasoning with same content)\n\t\t\t\t\tconst reasoningFields = [\"reasoning_content\", \"reasoning\", \"reasoning_text\"];\n\t\t\t\t\tlet foundReasoningField: string | null = null;\n\t\t\t\t\tfor (const field of reasoningFields) {\n\t\t\t\t\t\tif (\n\t\t\t\t\t\t\t(choice.delta as any)[field] !== null &&\n\t\t\t\t\t\t\t(choice.delta as any)[field] !== undefined &&\n\t\t\t\t\t\t\t(choice.delta as any)[field].length > 0\n\t\t\t\t\t\t) {\n\t\t\t\t\t\t\tif (!foundReasoningField) {\n\t\t\t\t\t\t\t\tfoundReasoningField = field;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tif (foundReasoningField) {\n\t\t\t\t\t\tif (!currentBlock || currentBlock.type !== \"thinking\") {\n\t\t\t\t\t\t\tfinishCurrentBlock(currentBlock);\n\t\t\t\t\t\t\tcurrentBlock = {\n\t\t\t\t\t\t\t\ttype: \"thinking\",\n\t\t\t\t\t\t\t\tthinking: \"\",\n\t\t\t\t\t\t\t\tthinkingSignature: foundReasoningField,\n\t\t\t\t\t\t\t};\n\t\t\t\t\t\t\toutput.content.push(currentBlock);\n\t\t\t\t\t\t\tstream.push({ type: \"thinking_start\", contentIndex: blockIndex(), partial: output });\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (currentBlock.type === \"thinking\") {\n\t\t\t\t\t\t\tconst delta = (choice.delta as any)[foundReasoningField];\n\t\t\t\t\t\t\tcurrentBlock.thinking += delta;\n\t\t\t\t\t\t\tstream.push({\n\t\t\t\t\t\t\t\ttype: \"thinking_delta\",\n\t\t\t\t\t\t\t\tcontentIndex: blockIndex(),\n\t\t\t\t\t\t\t\tdelta,\n\t\t\t\t\t\t\t\tpartial: output,\n\t\t\t\t\t\t\t});\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tif (choice?.delta?.tool_calls) {\n\t\t\t\t\t\tfor (const toolCall of choice.delta.tool_calls) {\n\t\t\t\t\t\t\tif (\n\t\t\t\t\t\t\t\t!currentBlock ||\n\t\t\t\t\t\t\t\tcurrentBlock.type !== \"toolCall\" ||\n\t\t\t\t\t\t\t\t(toolCall.id && currentBlock.id !== toolCall.id)\n\t\t\t\t\t\t\t) {\n\t\t\t\t\t\t\t\tfinishCurrentBlock(currentBlock);\n\t\t\t\t\t\t\t\tcurrentBlock = {\n\t\t\t\t\t\t\t\t\ttype: \"toolCall\",\n\t\t\t\t\t\t\t\t\tid: toolCall.id || \"\",\n\t\t\t\t\t\t\t\t\tname: toolCall.function?.name || \"\",\n\t\t\t\t\t\t\t\t\targuments: {},\n\t\t\t\t\t\t\t\t\tpartialArgs: \"\",\n\t\t\t\t\t\t\t\t};\n\t\t\t\t\t\t\t\toutput.content.push(currentBlock);\n\t\t\t\t\t\t\t\tstream.push({ type: \"toolcall_start\", contentIndex: blockIndex(), partial: output });\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tif (currentBlock.type === \"toolCall\") {\n\t\t\t\t\t\t\t\tif (toolCall.id) currentBlock.id = toolCall.id;\n\t\t\t\t\t\t\t\tif (toolCall.function?.name) currentBlock.name = toolCall.function.name;\n\t\t\t\t\t\t\t\tlet delta = \"\";\n\t\t\t\t\t\t\t\tif (toolCall.function?.arguments) {\n\t\t\t\t\t\t\t\t\tdelta = toolCall.function.arguments;\n\t\t\t\t\t\t\t\t\tcurrentBlock.partialArgs += toolCall.function.arguments;\n\t\t\t\t\t\t\t\t\tcurrentBlock.arguments = parseStreamingJson(currentBlock.partialArgs);\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tstream.push({\n\t\t\t\t\t\t\t\t\ttype: \"toolcall_delta\",\n\t\t\t\t\t\t\t\t\tcontentIndex: blockIndex(),\n\t\t\t\t\t\t\t\t\tdelta,\n\t\t\t\t\t\t\t\t\tpartial: output,\n\t\t\t\t\t\t\t\t});\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tconst reasoningDetails = (choice.delta as any).reasoning_details;\n\t\t\t\t\tif (reasoningDetails && Array.isArray(reasoningDetails)) {\n\t\t\t\t\t\tfor (const detail of reasoningDetails) {\n\t\t\t\t\t\t\tif (detail.type === \"reasoning.encrypted\" && detail.id && detail.data) {\n\t\t\t\t\t\t\t\tconst matchingToolCall = output.content.find(\n\t\t\t\t\t\t\t\t\t(b) => b.type === \"toolCall\" && b.id === detail.id,\n\t\t\t\t\t\t\t\t) as ToolCall | undefined;\n\t\t\t\t\t\t\t\tif (matchingToolCall) {\n\t\t\t\t\t\t\t\t\tmatchingToolCall.thoughtSignature = JSON.stringify(detail);\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfinishCurrentBlock(currentBlock);\n\n\t\t\tif (options?.signal?.aborted) {\n\t\t\t\tthrow new Error(\"Request was aborted\");\n\t\t\t}\n\n\t\t\tif (output.stopReason === \"aborted\" || output.stopReason === \"error\") {\n\t\t\t\tthrow new Error(\"An unkown error ocurred\");\n\t\t\t}\n\n\t\t\tstream.push({ type: \"done\", reason: output.stopReason, message: output });\n\t\t\tstream.end();\n\t\t} catch (error) {\n\t\t\tfor (const block of output.content) delete (block as any).index;\n\t\t\toutput.stopReason = options?.signal?.aborted ? \"aborted\" : \"error\";\n\t\t\toutput.errorMessage = error instanceof Error ? error.message : JSON.stringify(error);\n\t\t\t// Some providers via OpenRouter give additional information in this field.\n\t\t\tconst rawMetadata = (error as any)?.error?.metadata?.raw;\n\t\t\tif (rawMetadata) output.errorMessage += `\\n${rawMetadata}`;\n\t\t\tstream.push({ type: \"error\", reason: output.stopReason, error: output });\n\t\t\tstream.end();\n\t\t}\n\t})();\n\n\treturn stream;\n};\n\nfunction createClient(\n\tmodel: Model<\"openai-completions\">,\n\tcontext: Context,\n\tapiKey?: string,\n\toptionsHeaders?: Record<string, string>,\n) {\n\tif (!apiKey) {\n\t\tif (!process.env.OPENAI_API_KEY) {\n\t\t\tthrow new Error(\n\t\t\t\t\"OpenAI API key is required. Set OPENAI_API_KEY environment variable or pass it as an argument.\",\n\t\t\t);\n\t\t}\n\t\tapiKey = process.env.OPENAI_API_KEY;\n\t}\n\n\tconst headers = { ...model.headers };\n\tif (model.provider === \"github-copilot\") {\n\t\t// Copilot expects X-Initiator to indicate whether the request is user-initiated\n\t\t// or agent-initiated (e.g. follow-up after assistant/tool messages). If there is\n\t\t// no prior message, default to user-initiated.\n\t\tconst messages = context.messages || [];\n\t\tconst lastMessage = messages[messages.length - 1];\n\t\tconst isAgentCall = lastMessage ? lastMessage.role !== \"user\" : false;\n\t\theaders[\"X-Initiator\"] = isAgentCall ? \"agent\" : \"user\";\n\t\theaders[\"Openai-Intent\"] = \"conversation-edits\";\n\n\t\t// Copilot requires this header when sending images\n\t\tconst hasImages = messages.some((msg) => {\n\t\t\tif (msg.role === \"user\" && Array.isArray(msg.content)) {\n\t\t\t\treturn msg.content.some((c) => c.type === \"image\");\n\t\t\t}\n\t\t\tif (msg.role === \"toolResult\" && Array.isArray(msg.content)) {\n\t\t\t\treturn msg.content.some((c) => c.type === \"image\");\n\t\t\t}\n\t\t\treturn false;\n\t\t});\n\t\tif (hasImages) {\n\t\t\theaders[\"Copilot-Vision-Request\"] = \"true\";\n\t\t}\n\t}\n\n\t// Merge options headers last so they can override defaults\n\tif (optionsHeaders) {\n\t\tObject.assign(headers, optionsHeaders);\n\t}\n\n\treturn new OpenAI({\n\t\tapiKey,\n\t\tbaseURL: model.baseUrl,\n\t\tdangerouslyAllowBrowser: true,\n\t\tdefaultHeaders: headers,\n\t});\n}\n\nfunction buildParams(model: Model<\"openai-completions\">, context: Context, options?: OpenAICompletionsOptions) {\n\tconst compat = getCompat(model);\n\tconst messages = convertMessages(model, context, compat);\n\tmaybeAddOpenRouterAnthropicCacheControl(model, messages);\n\n\tconst params: OpenAI.Chat.Completions.ChatCompletionCreateParamsStreaming = {\n\t\tmodel: model.id,\n\t\tmessages,\n\t\tstream: true,\n\t};\n\n\tif (compat.supportsUsageInStreaming !== false) {\n\t\t(params as any).stream_options = { include_usage: true };\n\t}\n\n\tif (compat.supportsStore) {\n\t\tparams.store = false;\n\t}\n\n\tif (options?.maxTokens) {\n\t\tif (compat.maxTokensField === \"max_tokens\") {\n\t\t\t(params as any).max_tokens = options.maxTokens;\n\t\t} else {\n\t\t\tparams.max_completion_tokens = options.maxTokens;\n\t\t}\n\t}\n\n\tif (options?.temperature !== undefined) {\n\t\tparams.temperature = options.temperature;\n\t}\n\n\tif (context.tools) {\n\t\tparams.tools = convertTools(context.tools);\n\t} else if (hasToolHistory(context.messages)) {\n\t\t// Anthropic (via LiteLLM/proxy) requires tools param when conversation has tool_calls/tool_results\n\t\tparams.tools = [];\n\t}\n\n\tif (options?.toolChoice) {\n\t\tparams.tool_choice = options.toolChoice;\n\t}\n\n\tif (compat.thinkingFormat === \"zai\" && model.reasoning) {\n\t\t// Z.ai uses binary thinking: { type: \"enabled\" | \"disabled\" }\n\t\t// Must explicitly disable since z.ai defaults to thinking enabled\n\t\t(params as any).thinking = { type: options?.reasoningEffort ? \"enabled\" : \"disabled\" };\n\t} else if (options?.reasoningEffort && model.reasoning && compat.supportsReasoningEffort) {\n\t\t// OpenAI-style reasoning_effort\n\t\tparams.reasoning_effort = options.reasoningEffort;\n\t}\n\n\treturn params;\n}\n\nfunction maybeAddOpenRouterAnthropicCacheControl(\n\tmodel: Model<\"openai-completions\">,\n\tmessages: ChatCompletionMessageParam[],\n): void {\n\tif (model.provider !== \"openrouter\" || !model.id.startsWith(\"anthropic/\")) return;\n\n\t// Anthropic-style caching requires cache_control on a text part. Add a breakpoint\n\t// on the last user/assistant message (walking backwards until we find text content).\n\tfor (let i = messages.length - 1; i >= 0; i--) {\n\t\tconst msg = messages[i];\n\t\tif (msg.role !== \"user\" && msg.role !== \"assistant\") continue;\n\n\t\tconst content = msg.content;\n\t\tif (typeof content === \"string\") {\n\t\t\tmsg.content = [\n\t\t\t\tObject.assign({ type: \"text\" as const, text: content }, { cache_control: { type: \"ephemeral\" } }),\n\t\t\t];\n\t\t\treturn;\n\t\t}\n\n\t\tif (!Array.isArray(content)) continue;\n\n\t\t// Find last text part and add cache_control\n\t\tfor (let j = content.length - 1; j >= 0; j--) {\n\t\t\tconst part = content[j];\n\t\t\tif (part?.type === \"text\") {\n\t\t\t\tObject.assign(part, { cache_control: { type: \"ephemeral\" } });\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunction convertMessages(\n\tmodel: Model<\"openai-completions\">,\n\tcontext: Context,\n\tcompat: Required<OpenAICompletionsCompat>,\n): ChatCompletionMessageParam[] {\n\tconst params: ChatCompletionMessageParam[] = [];\n\n\tconst normalizeToolCallId = (id: string): string => {\n\t\tif (compat.requiresMistralToolIds) return normalizeMistralToolId(id);\n\t\tif (model.provider === \"openai\") return id.length > 40 ? id.slice(0, 40) : id;\n\t\t// Copilot Claude models route to Claude backend which requires Anthropic ID format\n\t\tif (model.provider === \"github-copilot\" && model.id.toLowerCase().includes(\"claude\")) {\n\t\t\treturn id.replace(/[^a-zA-Z0-9_-]/g, \"_\").slice(0, 64);\n\t\t}\n\t\treturn id;\n\t};\n\n\tconst transformedMessages = transformMessages(context.messages, model, (id) => normalizeToolCallId(id));\n\n\tif (context.systemPrompt) {\n\t\tconst useDeveloperRole = model.reasoning && compat.supportsDeveloperRole;\n\t\tconst role = useDeveloperRole ? \"developer\" : \"system\";\n\t\tparams.push({ role: role, content: sanitizeSurrogates(context.systemPrompt) });\n\t}\n\n\tlet lastRole: string | null = null;\n\n\tfor (const msg of transformedMessages) {\n\t\t// Some providers (e.g. Mistral/Devstral) don't allow user messages directly after tool results\n\t\t// Insert a synthetic assistant message to bridge the gap\n\t\tif (compat.requiresAssistantAfterToolResult && lastRole === \"toolResult\" && msg.role === \"user\") {\n\t\t\tparams.push({\n\t\t\t\trole: \"assistant\",\n\t\t\t\tcontent: \"I have processed the tool results.\",\n\t\t\t});\n\t\t}\n\n\t\tif (msg.role === \"user\") {\n\t\t\tif (typeof msg.content === \"string\") {\n\t\t\t\tparams.push({\n\t\t\t\t\trole: \"user\",\n\t\t\t\t\tcontent: sanitizeSurrogates(msg.content),\n\t\t\t\t});\n\t\t\t} else {\n\t\t\t\tconst content: ChatCompletionContentPart[] = msg.content.map((item): ChatCompletionContentPart => {\n\t\t\t\t\tif (item.type === \"text\") {\n\t\t\t\t\t\treturn {\n\t\t\t\t\t\t\ttype: \"text\",\n\t\t\t\t\t\t\ttext: sanitizeSurrogates(item.text),\n\t\t\t\t\t\t} satisfies ChatCompletionContentPartText;\n\t\t\t\t\t} else {\n\t\t\t\t\t\treturn {\n\t\t\t\t\t\t\ttype: \"image_url\",\n\t\t\t\t\t\t\timage_url: {\n\t\t\t\t\t\t\t\turl: `data:${item.mimeType};base64,${item.data}`,\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t} satisfies ChatCompletionContentPartImage;\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t\t\tconst filteredContent = !model.input.includes(\"image\")\n\t\t\t\t\t? content.filter((c) => c.type !== \"image_url\")\n\t\t\t\t\t: content;\n\t\t\t\tif (filteredContent.length === 0) continue;\n\t\t\t\tparams.push({\n\t\t\t\t\trole: \"user\",\n\t\t\t\t\tcontent: filteredContent,\n\t\t\t\t});\n\t\t\t}\n\t\t} else if (msg.role === \"assistant\") {\n\t\t\t// Some providers (e.g. Mistral) don't accept null content, use empty string instead\n\t\t\tconst assistantMsg: ChatCompletionAssistantMessageParam = {\n\t\t\t\trole: \"assistant\",\n\t\t\t\tcontent: compat.requiresAssistantAfterToolResult ? \"\" : null,\n\t\t\t};\n\n\t\t\tconst textBlocks = msg.content.filter((b) => b.type === \"text\") as TextContent[];\n\t\t\t// Filter out empty text blocks to avoid API validation errors\n\t\t\tconst nonEmptyTextBlocks = textBlocks.filter((b) => b.text && b.text.trim().length > 0);\n\t\t\tif (nonEmptyTextBlocks.length > 0) {\n\t\t\t\t// GitHub Copilot requires assistant content as a string, not an array.\n\t\t\t\t// Sending as array causes Claude models to re-answer all previous prompts.\n\t\t\t\tif (model.provider === \"github-copilot\") {\n\t\t\t\t\tassistantMsg.content = nonEmptyTextBlocks.map((b) => sanitizeSurrogates(b.text)).join(\"\");\n\t\t\t\t} else {\n\t\t\t\t\tassistantMsg.content = nonEmptyTextBlocks.map((b) => {\n\t\t\t\t\t\treturn { type: \"text\", text: sanitizeSurrogates(b.text) };\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Handle thinking blocks\n\t\t\tconst thinkingBlocks = msg.content.filter((b) => b.type === \"thinking\") as ThinkingContent[];\n\t\t\t// Filter out empty thinking blocks to avoid API validation errors\n\t\t\tconst nonEmptyThinkingBlocks = thinkingBlocks.filter((b) => b.thinking && b.thinking.trim().length > 0);\n\t\t\tif (nonEmptyThinkingBlocks.length > 0) {\n\t\t\t\tif (compat.requiresThinkingAsText) {\n\t\t\t\t\t// Convert thinking blocks to plain text (no tags to avoid model mimicking them)\n\t\t\t\t\tconst thinkingText = nonEmptyThinkingBlocks.map((b) => b.thinking).join(\"\\n\\n\");\n\t\t\t\t\tconst textContent = assistantMsg.content as Array<{ type: \"text\"; text: string }> | null;\n\t\t\t\t\tif (textContent) {\n\t\t\t\t\t\ttextContent.unshift({ type: \"text\", text: thinkingText });\n\t\t\t\t\t} else {\n\t\t\t\t\t\tassistantMsg.content = [{ type: \"text\", text: thinkingText }];\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// Use the signature from the first thinking block if available (for llama.cpp server + gpt-oss)\n\t\t\t\t\tconst signature = nonEmptyThinkingBlocks[0].thinkingSignature;\n\t\t\t\t\tif (signature && signature.length > 0) {\n\t\t\t\t\t\t(assistantMsg as any)[signature] = nonEmptyThinkingBlocks.map((b) => b.thinking).join(\"\\n\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tconst toolCalls = msg.content.filter((b) => b.type === \"toolCall\") as ToolCall[];\n\t\t\tif (toolCalls.length > 0) {\n\t\t\t\tassistantMsg.tool_calls = toolCalls.map((tc) => ({\n\t\t\t\t\tid: tc.id,\n\t\t\t\t\ttype: \"function\" as const,\n\t\t\t\t\tfunction: {\n\t\t\t\t\t\tname: tc.name,\n\t\t\t\t\t\targuments: JSON.stringify(tc.arguments),\n\t\t\t\t\t},\n\t\t\t\t}));\n\t\t\t\tconst reasoningDetails = toolCalls\n\t\t\t\t\t.filter((tc) => tc.thoughtSignature)\n\t\t\t\t\t.map((tc) => {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\treturn JSON.parse(tc.thoughtSignature!);\n\t\t\t\t\t\t} catch {\n\t\t\t\t\t\t\treturn null;\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\t\t\t\t\t.filter(Boolean);\n\t\t\t\tif (reasoningDetails.length > 0) {\n\t\t\t\t\t(assistantMsg as any).reasoning_details = reasoningDetails;\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Skip assistant messages that have no content and no tool calls.\n\t\t\t// Mistral explicitly requires \"either content or tool_calls, but not none\".\n\t\t\t// Other providers also don't accept empty assistant messages.\n\t\t\t// This handles aborted assistant responses that got no content.\n\t\t\tconst content = assistantMsg.content;\n\t\t\tconst hasContent =\n\t\t\t\tcontent !== null &&\n\t\t\t\tcontent !== undefined &&\n\t\t\t\t(typeof content === \"string\" ? content.length > 0 : content.length > 0);\n\t\t\tif (!hasContent && !assistantMsg.tool_calls) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tparams.push(assistantMsg);\n\t\t} else if (msg.role === \"toolResult\") {\n\t\t\t// Extract text and image content\n\t\t\tconst textResult = msg.content\n\t\t\t\t.filter((c) => c.type === \"text\")\n\t\t\t\t.map((c) => (c as any).text)\n\t\t\t\t.join(\"\\n\");\n\t\t\tconst hasImages = msg.content.some((c) => c.type === \"image\");\n\n\t\t\t// Always send tool result with text (or placeholder if only images)\n\t\t\tconst hasText = textResult.length > 0;\n\t\t\t// Some providers (e.g. Mistral) require the 'name' field in tool results\n\t\t\tconst toolResultMsg: ChatCompletionToolMessageParam = {\n\t\t\t\trole: \"tool\",\n\t\t\t\tcontent: sanitizeSurrogates(hasText ? textResult : \"(see attached image)\"),\n\t\t\t\ttool_call_id: msg.toolCallId,\n\t\t\t};\n\t\t\tif (compat.requiresToolResultName && msg.toolName) {\n\t\t\t\t(toolResultMsg as any).name = msg.toolName;\n\t\t\t}\n\t\t\tparams.push(toolResultMsg);\n\n\t\t\t// If there are images and model supports them, send a follow-up user message with images\n\t\t\tif (hasImages && model.input.includes(\"image\")) {\n\t\t\t\tconst contentBlocks: Array<\n\t\t\t\t\t{ type: \"text\"; text: string } | { type: \"image_url\"; image_url: { url: string } }\n\t\t\t\t> = [];\n\n\t\t\t\t// Add text prefix\n\t\t\t\tcontentBlocks.push({\n\t\t\t\t\ttype: \"text\",\n\t\t\t\t\ttext: \"Attached image(s) from tool result:\",\n\t\t\t\t});\n\n\t\t\t\t// Add images\n\t\t\t\tfor (const block of msg.content) {\n\t\t\t\t\tif (block.type === \"image\") {\n\t\t\t\t\t\tcontentBlocks.push({\n\t\t\t\t\t\t\ttype: \"image_url\",\n\t\t\t\t\t\t\timage_url: {\n\t\t\t\t\t\t\t\turl: `data:${(block as any).mimeType};base64,${(block as any).data}`,\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t});\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tparams.push({\n\t\t\t\t\trole: \"user\",\n\t\t\t\t\tcontent: contentBlocks,\n\t\t\t\t});\n\t\t\t}\n\t\t}\n\n\t\tlastRole = msg.role;\n\t}\n\n\treturn params;\n}\n\nfunction convertTools(tools: Tool[]): OpenAI.Chat.Completions.ChatCompletionTool[] {\n\treturn tools.map((tool) => ({\n\t\ttype: \"function\",\n\t\tfunction: {\n\t\t\tname: tool.name,\n\t\t\tdescription: tool.description,\n\t\t\tparameters: tool.parameters as any, // TypeBox already generates JSON Schema\n\t\t\tstrict: false, // Disable strict mode to allow optional parameters without null unions\n\t\t},\n\t}));\n}\n\nfunction mapStopReason(reason: ChatCompletionChunk.Choice[\"finish_reason\"]): StopReason {\n\tif (reason === null) return \"stop\";\n\tswitch (reason) {\n\t\tcase \"stop\":\n\t\t\treturn \"stop\";\n\t\tcase \"length\":\n\t\t\treturn \"length\";\n\t\tcase \"function_call\":\n\t\tcase \"tool_calls\":\n\t\t\treturn \"toolUse\";\n\t\tcase \"content_filter\":\n\t\t\treturn \"error\";\n\t\tdefault: {\n\t\t\tconst _exhaustive: never = reason;\n\t\t\tthrow new Error(`Unhandled stop reason: ${_exhaustive}`);\n\t\t}\n\t}\n}\n\n/**\n * Detect compatibility settings from provider and baseUrl for known providers.\n * Provider takes precedence over URL-based detection since it's explicitly configured.\n * Returns a fully resolved OpenAICompletionsCompat object with all fields set.\n */\nfunction detectCompat(model: Model<\"openai-completions\">): Required<OpenAICompletionsCompat> {\n\tconst provider = model.provider;\n\tconst baseUrl = model.baseUrl;\n\n\tconst isZai = provider === \"zai\" || baseUrl.includes(\"api.z.ai\");\n\n\tconst isNonStandard =\n\t\tprovider === \"cerebras\" ||\n\t\tbaseUrl.includes(\"cerebras.ai\") ||\n\t\tprovider === \"xai\" ||\n\t\tbaseUrl.includes(\"api.x.ai\") ||\n\t\tprovider === \"mistral\" ||\n\t\tbaseUrl.includes(\"mistral.ai\") ||\n\t\tbaseUrl.includes(\"chutes.ai\") ||\n\t\tisZai ||\n\t\tprovider === \"opencode\" ||\n\t\tbaseUrl.includes(\"opencode.ai\");\n\n\tconst useMaxTokens = provider === \"mistral\" || baseUrl.includes(\"mistral.ai\") || baseUrl.includes(\"chutes.ai\");\n\n\tconst isGrok = provider === \"xai\" || baseUrl.includes(\"api.x.ai\");\n\n\tconst isMistral = provider === \"mistral\" || baseUrl.includes(\"mistral.ai\");\n\n\treturn {\n\t\tsupportsStore: !isNonStandard,\n\t\tsupportsDeveloperRole: !isNonStandard,\n\t\tsupportsReasoningEffort: !isGrok && !isZai,\n\t\tsupportsUsageInStreaming: true,\n\t\tmaxTokensField: useMaxTokens ? \"max_tokens\" : \"max_completion_tokens\",\n\t\trequiresToolResultName: isMistral,\n\t\trequiresAssistantAfterToolResult: false, // Mistral no longer requires this as of Dec 2024\n\t\trequiresThinkingAsText: isMistral,\n\t\trequiresMistralToolIds: isMistral,\n\t\tthinkingFormat: isZai ? \"zai\" : \"openai\",\n\t};\n}\n\n/**\n * Get resolved compatibility settings for a model.\n * Uses explicit model.compat if provided, otherwise auto-detects from provider/URL.\n */\nfunction getCompat(model: Model<\"openai-completions\">): Required<OpenAICompletionsCompat> {\n\tconst detected = detectCompat(model);\n\tif (!model.compat) return detected;\n\n\treturn {\n\t\tsupportsStore: model.compat.supportsStore ?? detected.supportsStore,\n\t\tsupportsDeveloperRole: model.compat.supportsDeveloperRole ?? detected.supportsDeveloperRole,\n\t\tsupportsReasoningEffort: model.compat.supportsReasoningEffort ?? detected.supportsReasoningEffort,\n\t\tsupportsUsageInStreaming: model.compat.supportsUsageInStreaming ?? detected.supportsUsageInStreaming,\n\t\tmaxTokensField: model.compat.maxTokensField ?? detected.maxTokensField,\n\t\trequiresToolResultName: model.compat.requiresToolResultName ?? detected.requiresToolResultName,\n\t\trequiresAssistantAfterToolResult:\n\t\t\tmodel.compat.requiresAssistantAfterToolResult ?? detected.requiresAssistantAfterToolResult,\n\t\trequiresThinkingAsText: model.compat.requiresThinkingAsText ?? detected.requiresThinkingAsText,\n\t\trequiresMistralToolIds: model.compat.requiresMistralToolIds ?? detected.requiresMistralToolIds,\n\t\tthinkingFormat: model.compat.thinkingFormat ?? detected.thinkingFormat,\n\t};\n}\n"]}
